idea,domain,paper0_abstract,paper0_title,paper0_url,paper1_abstract,paper1_title,paper1_url,paper2_abstract,paper2_title,paper2_url,paper3_abstract,paper3_title,paper3_url,paper4_abstract,paper4_title,paper4_url,paper5_abstract,paper5_title,paper5_url,paper6_abstract,paper6_title,paper6_url,paper7_abstract,paper7_title,paper7_url,paper8_abstract,paper8_title,paper8_url,paper9_abstract,paper9_title,paper9_url,class
"Develop a literature review tool that uses **personalized visual augmentations** to highlight and suggest relevant but obscure citations specifically to **assist with difficult queries**. By analyzing the user's reading history and citation patterns, the tool can identify and emphasize lesser-known works that are highly pertinent to the researcher's current focus. This would make the discovery process less cumbersome for challenging topics. **Large-scale experiments** would be conducted to assess the tool's effectiveness in improving the comprehensiveness and efficiency of literature reviews.","['Computer Science', 'Human-Computer Interaction', 'Information Retrieval', 'Digital Libraries', 'Personalized Systems']","When reading a scholarly article, inline citations help researchers contextualize the current article and discover relevant prior work. However, it can be challenging to prioritize and make sense of the hundreds of citations encountered during literature reviews. This paper introduces CiteSee, a paper reading tool that leverages a user’s publishing, reading, and saving activities to provide personalized visual augmentations and context around citations. First, CiteSee connects the current paper to familiar contexts by surfacing known citations a user had cited or opened. Second, CiteSee helps users prioritize their exploration by highlighting relevant but unknown citations based on saving and reading history. We conducted a lab study that suggests CiteSee is significantly more effective for paper discovery than three baselines. A field deployment study shows CiteSee helps participants keep track of their explorations and leads to better situational awareness and increased paper discovery via inline citation when conducting real-world literature reviews.",CiteSee: Augmenting Citations in Scientific Papers with Persistent and Personalized Historical Context,https://www.semanticscholar.org/paper/81f7eb73883a559e39a5ab7754c77371488c4c7e,"Citations allow quickly identifying related research. If multiple publications are selected as seeds, specific suggestions for related literature can be made based on the number of incoming and outgoing citation links to this selection. Interactively adding recommended publications to the selection refines the next suggestion and incrementally builds a relevant collection of publications. Following this approach, the paper presents a search and foraging approach, PUREsuggest, which combines citation-based suggestions with augmented visualizations of the citation network. The focus and novelty of the approach is, first, the transparency of how the rankings are explained visually and, second, that the process can be steered through user-defined keywords, which reflect topics of interests. The system can be used to build new literature collections, to update and assess existing ones, as well as to use the collected literature for identifying relevant experts in the field. We evaluated the recommendation approach through simulated sessions and performed a user study investigating search strategies and usage patterns supported by the interface.",PUREsuggest: Citation-based Literature Search and Visual Exploration with Keyword-controlled Rankings,https://www.semanticscholar.org/paper/813fccf0b9f37dfc7bb6df0690502d72468b76c7,"Citations and their accompanying information such as citation context is a very important consideration when searching for relevant information in literature. With literature continually expanding, obtaining this information in an efficient and effective way, using a standard search tool becomes a very laborious task. In addition, scientific articles found in the literature employ rich interrelationships e.g., citation and their accompanying information making it very difficult for standard search applications to present these interrelationships to the searcher in an efficient manner. In order to alleviate this intensive task, we have designed a tool VisNavi (Visualization and Navigation) that supports scientific researchers in their literature review, in a form of a visualized star-center approach that, for the current treated paper places a citing author at its center and the richly embedded interrelationships are spun around it. The star design enables researchers to gain a clearer insight by interactively exploring these rich interrelationships along with their accompanying information. We designed a human judgment experiment to obtain a human rating on the functionality of the tool. We then cast the human rating as a reference point to improve the tool's design.","Supporting Literature Review by Searching, Visualizing and Navigating Related Papers",https://www.semanticscholar.org/paper/b961214ababf77e17e6c59e53b03a732e2ff3dd9,"Scholars who want to research a scientific topic must take time to read, extract meaning, and identify connections across many papers. As scientific literature grows, this becomes increasingly challenging. Meanwhile, authors summarize prior research in papers’ related work sections, though this is scoped to support a single paper. A formative study found that while reading multiple related work paragraphs helps overview a topic, it is hard to navigate overlapping and diverging references and research foci. In this work, we design a system, Relatedly, that scaffolds exploring and reading multiple related work paragraphs on a topic, with features including dynamic re-ranking and highlighting to spotlight unexplored dissimilar information, auto-generated descriptive paragraph headings, and low-lighting of redundant information. From a within-subjects user study (n=15), we found that scholars generate more coherent, insightful, and comprehensive topic outlines using Relatedly compared to a baseline paper list.",Relatedly: Scaffolding Literature Reviews with Existing Related Work Sections,https://www.semanticscholar.org/paper/c388626d1a342339078aaab7acc280efbc4f77fc,"Reviewing the literature to understand relevant threads of past work is a critical part of research and vehicle for learning. However, as the scientific literature grows the challenges for users to find and make sense of the many different threads of research grow as well. Previous work has helped scholars to find and group papers with citation information or textual similarity using standalone tools or overview visualizations. Instead, in this work we explore a tool integrated into users’ reading process that helps them with leveraging authors’ existing summarization of threads, typically in introduction or related work sections, in order to situate their own work’s contributions. To explore this we developed a prototype that supports efficient extraction and organization of threads along with supporting evidence as scientists read research articles. The system then recommends further relevant articles based on user-created threads. We evaluate the system in a lab study and find that it helps scientists to follow and curate research threads without breaking out of their flow of reading, collect relevant papers and clips, and discover interesting new articles to further grow threads.",Threddy: An Interactive System for Personalized Thread-based Exploration and Organization of Scientific Literature,https://www.semanticscholar.org/paper/fa1e4d2cc397affeb8816bc98f0d1ef38a5ee8fb,"In order to help scholars understand and follow a research topic, significant research has been devoted to creating systems that help scholars discover relevant papers and authors. Recent approaches have shown the usefulness of highlighting relevant authors while scholars engage in paper discovery. However, these systems do not capture and utilize users’ evolving knowledge of authors. We reflect on the design space and introduce ComLittee, a literature discovery system that supports author-centric exploration. In contrast to paper-centric interaction in prior systems, ComLittee’s author-centric interaction supports curating research threads from individual authors, finding new authors and papers using combined signals from a paper recommender and the curated authors’ authorship graphs, and understanding them in the context of those signals. In a within-subjects experiment that compares to a paper-centric discovery system with author-highlighting, we demonstrate how ComLittee improves author and paper discovery.",ComLittee: Literature Discovery with Personal Elected Author Committees,https://www.semanticscholar.org/paper/7f95d982f8ed3189d84577f1fdf07f93c99423f2,"With the rapid growth of scholarly archives, researchers subscribe to “paper alert’’ systems that periodically provide them with recommendations of recently published papers that are similar to previously collected papers. However, researchers sometimes struggle to make sense of nuanced connections between recommended papers and their own research context, as existing systems only present paper titles and abstracts. To help researchers spot these connections, we present PaperWeaver, an enriched paper alerts system that provides contextualized text descriptions of recommended papers based on user-collected papers. PaperWeaver employs a computational method based on Large Language Models (LLMs) to infer users’ research interests from their collected papers, extract context-specific aspects of papers, and compare recommended and collected papers on these aspects. Our user study (N=15) showed that participants using PaperWeaver were able to better understand the relevance of recommended papers and triage them more confidently when compared to a baseline that presented the related work sections from recommended papers.",PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers,https://www.semanticscholar.org/paper/2cbdad900dda764efa362793765ec12cb5ae66f5,"When starting a new research activity, it is essential to study related work. Traditional search engines and dedicated social networks are generally used to search for relevant literature. Current technologies rely on keyword based searches which, however, do not provide the support of a wider context. Cite-as-you-write aims to simplify and shorten this exploratory task: given a verbose description of the problem to be investigated, the system automatically recommends related papers/citations.",Cite-as-you-write,https://www.semanticscholar.org/paper/48296ce509c66d8f306aff53d886eb5cd7ff3cb6,"The literature review is a key component of academic research, which allows researchers to build upon each other's work. While modern search engines enable fast access to publications, there is a lack of support for filtering out the vast majority of papers that are irrelevant to the current research focus. We present PaperQuest, a visualization tool that supports efficient reading decisions, by only displaying the information useful at a given step of the review. We propose an algorithm to find and sort papers that are likely to be relevant to users, based on the papers they have already expressed interest in and the number of citations. The current implementation uses papers from the CHI, UIST, and VIS conferences, and citation counts from Google Scholar, but is easily extensible to other domains of the literature.",PaperQuest: A Visualization Tool to Support Literature Review,https://www.semanticscholar.org/paper/ee39f99c99ca2d372c039b8d84620e2537abff04,"Reviewing literatures for a certain research field is always important for academics. One could use Google‐like information seeking tools, but oftentimes he/she would end up obtaining too many possibly related papers, as well as the papers in the associated citation network. During such a process, a user may easily get lost after following a few links for searching or cross‐referencing. It is also difficult for the user to identify relevant/important papers from the resulting huge collection of papers. Our work, called PaperVis, endeavors to provide a user‐friendly interface to help users quickly grasp the intrinsic complex citation‐reference structures among a specific group of papers. We modify the existing Radial Space Filling (RSF) and Bullseye View techniques to arrange involved papers as a node‐link graph that better depicts the relationships among them while saving the screen space at the same time. PaperVis applies visual cues to present node attributes and their transitions among interactions, and it categorizes papers into semantically meaningful hierarchies to facilitate ensuing literature exploration. We conduct experiments on the InfoVis 2004 Contest Dataset to demonstrate the effectiveness of PaperVis.",PaperVis: Literature Review Made Easy,https://www.semanticscholar.org/paper/31e2f7c599cd5cd621f13932186db352be4d8eea,not novel
"Develop a system that uses a **<b style=""background-color:#FFFACD;"">faceted author representation</b>** of digital learning resource (DLR) creators based on their educational materials and inferred teaching personas. This system would aim **<b style=""background-color:#F0FFF0;"">to support ubiquitous learning</b>** by helping learners discover novel educators and materials that offer innovative perspectives. **<b style=""background-color:#E6E6FA;"">Usability testing of learning resources</b>** would be conducted to ensure the system enhances the learning experience by balancing relevance and novelty, thus boosting the accessibility and discoverability of diverse educational content.","['Computer Science', 'Human-Computer Interaction', 'Information Retrieval', 'Recommender Systems', 'Educational Technology']","Isolated silos of scientific research and the growing challenge of information overload limit awareness across the literature and hinder innovation. Algorithmic curation and recommendation, which often prioritize relevance, can further reinforce these informational “filter bubbles.” In response, we describe Bridger, a system for facilitating discovery of scholars and their work. We construct a faceted representation of authors with information gleaned from their papers and inferred author personas, and use it to develop an approach that locates commonalities and contrasts between scientists to balance relevance and novelty. In studies with computer science researchers, this approach helps users discover authors considered useful for generating novel research directions. We also demonstrate an approach for displaying information about authors, boosting the ability to understand the work of new, unfamiliar scholars. Our analysis reveals that Bridger connects authors who have different citation profiles and publish in different venues, raising the prospect of bridging diverse scientific communities.",Bursting Scientific Filter Bubbles: Boosting Innovation via Novel Author Discovery,https://www.semanticscholar.org/paper/bea1c05b0584a4463cbfbcf4917d6afacaef0bde,"Scientific silos can hinder innovation. These information “filter bubbles” and the growing challenge of information overload limit awareness across the literature, making it difficult to keep track of even narrow areas of interest, let alone discover new ones. Al-gorithmic curation and recommendation, which often prioritize relevance, can further reinforce these bubbles. In response, we describe Bridger, a system for facilitating discovery of scholars and their work, to explore design tradeoffs among relevant and novel recommendations. We construct a faceted representation of authors using information extracted from their papers and inferred personas. We explore approaches both for recommending new content and for displaying it in a manner that helps researchers to understand the work of authors who they are unfamiliar with. In studies with computer science researchers, our approach substantially improves users’ abilities to do so. We develop an approach that locates commonalities and contrasts between scientists—retrieving partially similar authors, rather than aiming for strict similarity. We find this approach helps users discover authors useful for generating novel research ideas of relevance to their work, at a higher rate than a state-of-art neural model. Our analysis reveals that Bridger connects authors who have different citation profiles, publish in different venues, and are more distant in social co-authorship networks, raising the prospect of bridging diverse communities and facilitating discovery.",Bridger: Toward Bursting Scientific Filter Bubbles and Boosting Innovation via Novel Author Discovery,https://www.semanticscholar.org/paper/4d02f1a1b99a21665ef94bcdec733a3f8f4056b4,"In the age of information abundance, discovering personalized content is paramount to engaging users on a variety of topics. The engine goes beyond traditional approaches by not only dynamically adjusting recommendations to the users' historical preferences, but also actively encouraging exploration of interdisciplinary areas by referencing them with other genre or domains. The recommendation engine uses a multi-layer strategy that includes collaborative and content-based filtering, hybrid models and advanced diversity metrices. It transcends the general boundaries of user preferences and fosters randomness and novelty through cross-domain recommendations. The system adapts to changing user interests by suggesting content in various formats such as articles, podcasts, videos and simulations. Community integration plays a key role, leveraging collaborative filtering to discover community-curated lists and hidden gems across various categories. Real-time trend research allows users to stay informed and engaged with the latest developments, maintaining a dynamic and up-to-date content ecosystem. Ethical considerations, transparency and user-friendly interfaces are essential elements to address bias and provide explanations for recommendations. Through continuous user feedback and iterative improvements, the novel algorithmic recommendation engine for diverse content discovery aims to redefine content discovery and foster a sense of exploration, curiosity and learning across a wide range of knowledge domains.",Novel Algorithmic Recommendation Engine for Diverse Content Discovery,https://www.semanticscholar.org/paper/60e54892d78a056b957508feab0072cca4b5fac8,"In order to help scholars understand and follow a research topic, significant research has been devoted to creating systems that help scholars discover relevant papers and authors. Recent approaches have shown the usefulness of highlighting relevant authors while scholars engage in paper discovery. However, these systems do not capture and utilize users’ evolving knowledge of authors. We reflect on the design space and introduce ComLittee, a literature discovery system that supports author-centric exploration. In contrast to paper-centric interaction in prior systems, ComLittee’s author-centric interaction supports curating research threads from individual authors, finding new authors and papers using combined signals from a paper recommender and the curated authors’ authorship graphs, and understanding them in the context of those signals. In a within-subjects experiment that compares to a paper-centric discovery system with author-highlighting, we demonstrate how ComLittee improves author and paper discovery.",ComLittee: Literature Discovery with Personal Elected Author Committees,https://www.semanticscholar.org/paper/7f95d982f8ed3189d84577f1fdf07f93c99423f2,"Open user models provide affordance for a transparent user control over recommendations based on shared symbolic representation within the system. Users must build their user profile by adding these symbols and tuning their importance to get meaningful recommendations. Since the link between these symbols and the reference explanation is often unavailable, it can be difficult for users to understand them. These symbols are often referred to as concepts, tags, areas, topics, labels, features, or keyphrases. This study showcases an information exploration system that helps students identify potential faculty members to collaborate with. The system works by matching user and faculty profiles that contain keywords or phrases representing topics/areas of interest. Students must develop their understanding of research topics while building their profiles, which can become challenging as they add more keywords. To support students in controlling the recommendation, we introduce post hoc explanations with three levels of detail: no explanations, individual explanation for topics, and explanation of the relationships between topics. This study explores how explanation is associated with the user context / tasks and the exploration process. Our observation suggests that expertise in the field is linked to exploring fewer novel topics and seeking fewer explanations but engaging more with explanations of relationships. In addition, we found that the engagement with faculty information is moderately correlated with the use of more advanced explanations.",Explanations in Open User Models for Personalized Information Exploration,https://www.semanticscholar.org/paper/ff4e3f399b8db5615b067f39a416f2055f9b285e,"AMiner is the second generation of the ArnetMiner system. We focus on developing author-centric analytic and mining tools for gaining a deep understanding of the large and heterogeneous networks formed by authors, papers, venues, and knowledge concepts. One fundamental goal is how to extract and integrate semantics from different sources. We have developed algorithms to automatically extract researchers' profiles from the Web and re- solve the name ambiguity problem, and connect different professional networks. We also developed methodologies to incorporate knowledge from the Wikipedia and other sources into the system to bridge the gap between network science and the web mining research. In this talk, I will focus on answering two fundamental questions for author-centric network analysis: who is who? and who are similar to each other? The system has been in operation since 2006 and has collected more than 100,000,000 author profiles, 100,000,000 publication papers, and 7,800,000 knowledge concepts. It has been widely used for collaboration recommendation, similarity analysis, and community evolution.",AMiner: Mining Deep Knowledge from Big Scholar Data,https://www.semanticscholar.org/paper/169edf0ad0b5e3ab31df4481a9069cbfdb774423,"Entity search is an emerging IR and NLP task that involves the retrieval of entities of a specific type in response to a query. We address the similar researcher search"" or the ""researcher recommendation"" problem, an instance of similar entity search"" for the academic domain. In response to a researcher name' query, the goal of a researcher recommender system is to output the list of researchers that have similar expertise as that of the queried researcher. We propose models for computing similarity between researchers based on expertise profiles extracted from their publications and academic homepages. We provide results of our models for the recommendation task on two publicly-available datasets. To the best of our knowledge, we are the first to address content-based researcher recommendation in an academic setting and demonstrate it for Computer Science via our system, ScholarSearch.",Similar researcher search in academic environments,https://www.semanticscholar.org/paper/abb67d59b7569cfae6596568e552d9e9691d8842,"Expanding a set of known experts with new ones that share similar expertise is a problem that emerges in various real-life applications. We demonstrate VeTo-web, an open source, publicly available tool that deals with this problem in the context of searching for academic experts. VeTo-web exploits analysis techniques for scholarly knowledge graphs to identify scholars that share similar research activities with a given expert group and offers a Web-based user interface to assist its users in expanding a set of academic experts with additional scholars with similar expertise.",VeTo-web: A Recommendation Tool for the Expansion of Sets of Scholars,https://www.semanticscholar.org/paper/1551d4c248f9081a83050c2af20e703c9af44940,"The ever-increasing pace of scientific publication necessitates methods for quickly identifying relevant papers. While neural recommenders trained on user interests can help, they still result in long, monotonous lists of suggested papers. To improve the discovery experience we introduce multiple new methods for augmenting recommendations with textual relevance messages that highlight knowledge-graph connections between recommended papers and a user’s publication and interaction history. We explore associations mediated by author entities and those using citations alone. In a large-scale, real-world study, we show how our approach significantly increases engagement—and future engagement when mediated by authors—without introducing bias towards highly-cited authors. To expand message coverage for users with less publication or interaction history, we develop a novel method that highlights connections with proxy authors of interest to users and evaluate it in a controlled lab study. Finally, we synthesize design implications for future graph-based messages.",From Who You Know to What You Read: Augmenting Scientific Recommendations with Implicit Social Networks,https://www.semanticscholar.org/paper/cbf0c0593479304ccfa9dd5bcd26d851c1ff72d7,"Interdisciplinary studies often require researchers to explore literature in diverse branches of knowledge. Yet, navigating through the highly scattered knowledge from unfamiliar disciplines poses a significant challenge. In this paper, we introduce DiscipLink, a novel interactive system that facilitates collaboration between researchers and large language models (LLMs) in interdisciplinary information seeking (IIS). Based on users' topics of interest, DiscipLink initiates exploratory questions from the perspectives of possible relevant fields of study, and users can further tailor these questions. DiscipLink then supports users in searching and screening papers under selected questions by automatically expanding queries with disciplinary-specific terminologies, extracting themes from retrieved papers, and highlighting the connections between papers and questions. Our evaluation, comprising a within-subject comparative experiment and an open-ended exploratory study, reveals that DiscipLink can effectively support researchers in breaking down disciplinary boundaries and integrating scattered knowledge in diverse fields. The findings underscore the potential of LLM-powered tools in fostering information-seeking practices and bolstering interdisciplinary research.",DiscipLink: Unfolding Interdisciplinary Information Seeking Process via Human-AI Co-Exploration,https://www.semanticscholar.org/paper/ed1471ddca3ae6842367d299badc8fef14eca44a,novel
"Develop a tool that integrates **<b style=""background-color:#FFFACD;"">personalized visual augmentations</b>** to **<b style=""background-color:#F0FFF0;"">enhance citation data retrieval</b>** during literature reviews. The tool would automatically scrape and display citation data relevant to the user's research interests and historical activities. Conduct a **<b style=""background-color:#E6E6FA;"">feasibility study</b>** to determine the effectiveness of this approach in improving researchers' ability to quickly and accurately gather citation information related to their work.","['Computer Science', 'Human-Computer Interaction', 'Information Retrieval', 'Digital Libraries']","When reading a scholarly article, inline citations help researchers contextualize the current article and discover relevant prior work. However, it can be challenging to prioritize and make sense of the hundreds of citations encountered during literature reviews. This paper introduces CiteSee, a paper reading tool that leverages a user’s publishing, reading, and saving activities to provide personalized visual augmentations and context around citations. First, CiteSee connects the current paper to familiar contexts by surfacing known citations a user had cited or opened. Second, CiteSee helps users prioritize their exploration by highlighting relevant but unknown citations based on saving and reading history. We conducted a lab study that suggests CiteSee is significantly more effective for paper discovery than three baselines. A field deployment study shows CiteSee helps participants keep track of their explorations and leads to better situational awareness and increased paper discovery via inline citation when conducting real-world literature reviews.",CiteSee: Augmenting Citations in Scientific Papers with Persistent and Personalized Historical Context,https://www.semanticscholar.org/paper/81f7eb73883a559e39a5ab7754c77371488c4c7e,"The literature review is a key component of academic research, which allows researchers to build upon each other's work. While modern search engines enable fast access to publications, there is a lack of support for filtering out the vast majority of papers that are irrelevant to the current research focus. We present PaperQuest, a visualization tool that supports efficient reading decisions, by only displaying the information useful at a given step of the review. We propose an algorithm to find and sort papers that are likely to be relevant to users, based on the papers they have already expressed interest in and the number of citations. The current implementation uses papers from the CHI, UIST, and VIS conferences, and citation counts from Google Scholar, but is easily extensible to other domains of the literature.",PaperQuest: A Visualization Tool to Support Literature Review,https://www.semanticscholar.org/paper/ee39f99c99ca2d372c039b8d84620e2537abff04,"Citations allow quickly identifying related research. If multiple publications are selected as seeds, specific suggestions for related literature can be made based on the number of incoming and outgoing citation links to this selection. Interactively adding recommended publications to the selection refines the next suggestion and incrementally builds a relevant collection of publications. Following this approach, the paper presents a search and foraging approach, PUREsuggest, which combines citation-based suggestions with augmented visualizations of the citation network. The focus and novelty of the approach is, first, the transparency of how the rankings are explained visually and, second, that the process can be steered through user-defined keywords, which reflect topics of interests. The system can be used to build new literature collections, to update and assess existing ones, as well as to use the collected literature for identifying relevant experts in the field. We evaluated the recommendation approach through simulated sessions and performed a user study investigating search strategies and usage patterns supported by the interface.",PUREsuggest: Citation-based Literature Search and Visual Exploration with Keyword-controlled Rankings,https://www.semanticscholar.org/paper/813fccf0b9f37dfc7bb6df0690502d72468b76c7,"Citations and their accompanying information such as citation context is a very important consideration when searching for relevant information in literature. With literature continually expanding, obtaining this information in an efficient and effective way, using a standard search tool becomes a very laborious task. In addition, scientific articles found in the literature employ rich interrelationships e.g., citation and their accompanying information making it very difficult for standard search applications to present these interrelationships to the searcher in an efficient manner. In order to alleviate this intensive task, we have designed a tool VisNavi (Visualization and Navigation) that supports scientific researchers in their literature review, in a form of a visualized star-center approach that, for the current treated paper places a citing author at its center and the richly embedded interrelationships are spun around it. The star design enables researchers to gain a clearer insight by interactively exploring these rich interrelationships along with their accompanying information. We designed a human judgment experiment to obtain a human rating on the functionality of the tool. We then cast the human rating as a reference point to improve the tool's design.","Supporting Literature Review by Searching, Visualizing and Navigating Related Papers",https://www.semanticscholar.org/paper/b961214ababf77e17e6c59e53b03a732e2ff3dd9,"When reading a scholarly paper, scientists oftentimes wish to understand how follow-on work has built on or engages with what they are reading. While a paper itself can only discuss prior work, some scientific search engines can provide a list of all subsequent citing papers; unfortunately, they are undifferentiated and disconnected from the contents of the original reference paper. In this work, we introduce a novel paper reading experience that integrates relevant information about follow-on work directly into a paper, allowing readers to learn about newer papers and see how a paper is discussed by its citing papers in the context of the reference paper. We built a tool, called CiteRead, that implements the following three contributions: 1) automated techniques for selecting important citing papers, building on results from a formative study we conducted, 2) an automated process for localizing commentary provided by citing papers to a place in the reference paper, and 3) an interactive experience that allows readers to seamlessly alternate between the reference paper and information from citing papers (e.g., citation sentences), placed in the margins. Based on a user study with 12 scientists, we found that in comparison to having just a list of citing papers and their citation sentences, the use of CiteRead while reading allows for better comprehension and retention of information about follow-on work.",CiteRead: Integrating Localized Citation Contexts into Scientific Paper Reading,https://www.semanticscholar.org/paper/277360f074e809135d4b49cf7fd2f572c0db6658,"Reviewing literatures for a certain research field is always important for academics. One could use Google‐like information seeking tools, but oftentimes he/she would end up obtaining too many possibly related papers, as well as the papers in the associated citation network. During such a process, a user may easily get lost after following a few links for searching or cross‐referencing. It is also difficult for the user to identify relevant/important papers from the resulting huge collection of papers. Our work, called PaperVis, endeavors to provide a user‐friendly interface to help users quickly grasp the intrinsic complex citation‐reference structures among a specific group of papers. We modify the existing Radial Space Filling (RSF) and Bullseye View techniques to arrange involved papers as a node‐link graph that better depicts the relationships among them while saving the screen space at the same time. PaperVis applies visual cues to present node attributes and their transitions among interactions, and it categorizes papers into semantically meaningful hierarchies to facilitate ensuing literature exploration. We conduct experiments on the InfoVis 2004 Contest Dataset to demonstrate the effectiveness of PaperVis.",PaperVis: Literature Review Made Easy,https://www.semanticscholar.org/paper/31e2f7c599cd5cd621f13932186db352be4d8eea,"Reviewing the literature to understand relevant threads of past work is a critical part of research and vehicle for learning. However, as the scientific literature grows the challenges for users to find and make sense of the many different threads of research grow as well. Previous work has helped scholars to find and group papers with citation information or textual similarity using standalone tools or overview visualizations. Instead, in this work we explore a tool integrated into users’ reading process that helps them with leveraging authors’ existing summarization of threads, typically in introduction or related work sections, in order to situate their own work’s contributions. To explore this we developed a prototype that supports efficient extraction and organization of threads along with supporting evidence as scientists read research articles. The system then recommends further relevant articles based on user-created threads. We evaluate the system in a lab study and find that it helps scientists to follow and curate research threads without breaking out of their flow of reading, collect relevant papers and clips, and discover interesting new articles to further grow threads.",Threddy: An Interactive System for Personalized Thread-based Exploration and Organization of Scientific Literature,https://www.semanticscholar.org/paper/fa1e4d2cc397affeb8816bc98f0d1ef38a5ee8fb,"Researchers spend lots of time for reading scientific papers as they need to stay updated with recent trends. However, navigating citations, which are indispensable elements of research papers, can act as a barrier for junior researchers as they do not have enough background knowledge and experience. We conduct a formative user study to identify challenges in navigating cited papers. We then prototype QuickRef, an interactive reader that provides additional information about cited papers on the side panel. A preliminary user study documents the usability of QuickRef. Further, we present practical design implications for citation navigation support.",QuickRef: Should I Read Cited Papers for Understanding This Paper?,https://www.semanticscholar.org/paper/02ff1b212cf28ee9af968180765d1eb2b2885d9d,"Reading academic paper is a daily task for researchers and graduate students. However, reading effectively can be challenging, particularly for novices in scientific research. For example, when readers are reading the related work section that cites a fair number of references in limited page space, they often need to flip back and forth between the text and the references and may also frequently search elsewhere for more information about the references. This increases the difficulty of understanding a paper. In this paper, we propose a narrative visualization system that helps the reading of academic papers. As a first step, we adopt narrative visualization to present literature review as interactive slides. Specifically, we propose a narrative structure with three levels of granularities that the reader can drill down or roll up freely. The logic flow of a slideshow can be organized based on the paper's presentation or citations. We demonstrate the effectiveness of our system through several case studies and user studies. The results show that the system allows users to quickly track and glance related work, making paper reading more effective and enjoyable.",A Guided Tour of Literature Review: Facilitating Academic Paper Reading with Narrative Visualization,https://www.semanticscholar.org/paper/0182f730c7e6cd0f55d2f5fe2d9d9645484fcfd2,"Research paper recommender systems are widely used by academics to discover and explore the most relevant publications on a topic. While existing recommendation interfaces present researchers with a ranked list of publications based on a global relevance score, they fail to visualize the full range of non-textual features uniquely present in academic publications: citations, figures, charts, or images, and mathematical formulae or expressions. Especially for STEM literature, examining such non-textual features efficiently can provide utility to researchers interested in answering specialized research questions or information needs. If research paper search and recommender systems are to consider the similarity of such features as one facet of a content-based similarity assessment for academic literature, new methods for visualizing these non-textual features are needed. In this paper, we review the state-of-the-art in visualizing feature-based similarity in documents. We subsequently propose a set of user-customizable visualization approaches tailored to STEM literature and the research paper recommendation context. Results from a study with 10 expert users show that the interactive visualization interface we propose for the exploration of non-textual features in publications can effectively address specialized information retrieval tasks, which cannot be addressed by existing research paper search or recommendation interfaces.",Visualizing Feature-based Similarity for Research Paper Recommendation,https://www.semanticscholar.org/paper/adbb281eb78653988a02c2558d7a0c67edd84a3a,not novel
"Conduct a **secondary analysis of systematic review data** from diverse analogical reasoning benchmarks to **evaluate analogical reasoning in language models**. This approach will aggregate and compare results across different tasks, such as AnaloBench, KiVA, and mathematical problem synthesis benchmarks, to identify common strengths and weaknesses in LLMs. The findings will be synthesized into comprehensive performance metrics and **compared** across models of varying scales using a **performance comparison** framework. This will establish a more nuanced understanding of how different models handle analogical reasoning across multiple domains and conditions.","['Computer Science', 'Artificial Intelligence', 'Natural Language Processing', 'Benchmarking', 'Analogical Reasoning']","Humans regularly engage in analogical thinking, relating personal experiences to current situations ($X$ is analogous to $Y$ because of $Z$). Analogical thinking allows humans to solve problems in creative ways, grasp difficult concepts, and articulate ideas more effectively. Can language models (LMs) do the same? To answer this question, we propose ANALOBENCH, a benchmark to determine analogical reasoning ability in LMs. Our benchmarking approach focuses on aspects of this ability that are common among humans: (i) recalling related experiences from a large amount of information, and (ii) applying analogical reasoning to complex and lengthy scenarios. We test a broad collection of proprietary models (e.g., GPT family, Claude V2) and open source models such as LLaMA2. As in prior results, scaling up LMs results in some performance boosts. Surprisingly, scale offers minimal gains when, (i) analogies involve lengthy scenarios, or (ii) recalling relevant scenarios from a large pool of information, a process analogous to finding a needle in a haystack. We hope these observations encourage further research in this field.",AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies,https://www.semanticscholar.org/paper/5c24dd41f46fd7107997b0a46e1207e0fed63b34,"Analogical reasoning is a fundamental cognitive ability of humans. However, current language models (LMs) still struggle to achieve human-like performance in analogical reasoning tasks due to a lack of resources for model training. In this work, we address this gap by proposing ANALOGYKB, a million-scale analogy knowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB identifies two types of analogies from the KGs: 1) analogies of the same relations, which can be directly extracted from the KGs, and 2) analogies of analogous relations, which are identified with a selection and filtering pipeline enabled by large language models (LLMs), followed by minor human efforts for data quality control. Evaluations on a series of datasets of two analogical reasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB successfully enables both smaller LMs and LLMs to gain better analogical reasoning capabilities.",ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base,https://www.semanticscholar.org/paper/bad287184c6739fd6f476f89cb83e09415982d9f,"Analogical reasoning is a fundamental capacity of human cognition that allows us to reason abstractly about novel situations by relating them to past experiences. While it is thought to be essential for robust reasoning in AI systems, conventional approaches require significant training and/or hard-coding of domain knowledge to be applied to benchmark tasks. Inspired by cognitive science research that has found connections between human language and analogy-making, we explore the use of intuitive language-based abstractions to support analogy in AI systems. Specifically, we apply large pre-trained language models (PLMs) to visual Raven’s Progressive Matrices (RPM), a common relational reasoning test. By simply encoding the perceptual features of the problem into language form, we find that PLMs exhibit a striking capacity for zero-shot relational reasoning, exceeding human performance and nearing supervised vision-based methods. We explore different encodings that vary the level of abstraction over task features, finding that higher-level abstractions further strengthen PLMs’ analogical reasoning. Our detailed analysis reveals insights on the role of model complexity, in-context learning, and prior knowledge in solving RPM tasks.",In-Context Analogical Reasoning with Pre-Trained Language Models,https://www.semanticscholar.org/paper/0366177b44ed13d86b9d704a3a82ea3750e5abed,"Large language models (LLMs) have performed well on several reasoning benchmarks, including ones that test analogical reasoning abilities. However, it has been debated whether they are actually performing humanlike abstract reasoning or instead employing less general processes that rely on similarity to what has been seen in their training data. Here we investigate the generality of analogy-making abilities previously claimed for LLMs (Webb, Holyoak,&Lu, 2023). We take one set of analogy problems used to evaluate LLMs and create a set of""counterfactual""variants-versions that test the same abstract reasoning abilities but that are likely dissimilar from any pre-training data. We test humans and three GPT models on both the original and counterfactual problems, and show that, while the performance of humans remains high for all the problems, the GPT models' performance declines sharply on the counterfactual set. This work provides evidence that, despite previously reported successes of LLMs on analogical reasoning, these models lack the robustness and generality of human analogy-making.",Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models,https://www.semanticscholar.org/paper/b69ae70abf1e8519a565c40bed07ab499576a8f6,"Cross-domain analogical reasoning is a core creative ability that can be challenging for humans. Recent work has shown some proofs-of-concept of Large language Models’ (LLMs) ability to generate cross-domain analogies. However, the reliability and potential usefulness of this capacity for augmenting human creative work has received little systematic exploration. In this paper, we systematically explore LLMs capacity to augment cross-domain analogical reasoning. Across three studies, we found: 1) LLM-generated cross-domain analogies were frequently judged as helpful in the context of a problem reformulation task (median 4 out of 5 helpfulness rating), and frequently (∼ 80% of cases) led to observable changes in problem formulations, and 2) there was an upper bound of ∼ 25% of outputs being rated as potentially harmful, with a majority due to potentially upsetting content, rather than biased or toxic content. These results demonstrate the potential utility — and risks — of LLMs for augmenting cross-domain analogical creativity.",Fluid Transformers and Creative Analogies: Exploring Large Language Models’ Capacity for Augmenting Cross-Domain Analogical Creativity,https://www.semanticscholar.org/paper/d0cbc7cdf5c9a2fd9426f255acb5ee29f3351640,"The vital role of analogical reasoning in human cognition allows us to grasp novel concepts by linking them with familiar ones through shared relational structures. Despite the attention previous research has given to word analogies, this work suggests that Large Language Models (LLMs) often overlook the structures that underpin these analogies, raising questions about the efficacy of word analogies as a measure of analogical reasoning skills akin to human cognition. In response to this, our paper introduces a task of analogical structure abduction, grounded in cognitive psychology, designed to abduce structures that form an analogy between two systems. In support of this task, we establish a benchmark called SCAR, containing 400 scientific analogies from 13 distinct fields, tailored for evaluating analogical reasoning with structure abduction. The empirical evidence underlines the continued challenges faced by LLMs, including ChatGPT and GPT-4, in mastering this task, signifying the need for future exploration to enhance their abilities.",Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction,https://www.semanticscholar.org/paper/c10ce97f538adecfc0bc15e6ca39dd5d5f002bc1,"The Abstraction Reasoning Corpus (ARC) is a visual analogical reasoning test designed for humans and machines (Chollet, 2019). We compared human and large language model (LLM) performance on a new child-friendly set of ARC items. Results show that both children and adults outperform most LLMs on these tasks. Error analysis revealed a similar""fallback""solution strategy in LLMs and young children, where part of the analogy is simply copied. In addition, we found two other error types, one based on seemingly grasping key concepts (e.g., Inside-Outside) and the other based on simple combinations of analogy input matrices. On the whole,""concept""errors were more common in humans, and""matrix""errors were more common in LLMs. This study sheds new light on LLM reasoning ability and the extent to which we can use error analyses and comparisons with human development to understand how LLMs solve visual analogies.",Do Large Language Models Solve ARC Visual Analogies Like People Do?,https://www.semanticscholar.org/paper/26e48be4bdb56554c3d71c83076512fd23fef575,"Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, analogical prompting, designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench.",Large Language Models as Analogical Reasoners,https://www.semanticscholar.org/paper/d4bf36cbc5855ea87235d7a64f406717ac6aa3c9,"Analogical reasoning is considered core to human learning and cognition. Recent studies have compared the analogical reasoning abilities of human subjects and Large Language Models (LLMs) on abstract symbol manipulation tasks, such as letter string analogies. However, these studies largely neglect analogical reasoning over semantically meaningful symbols, such as natural language words. This ability to draw analogies that link language to non-linguistic domains, which we term semantic structure-mapping, is thought to play a crucial role in language acquisition and broader cognitive development. We test human subjects and LLMs on analogical reasoning tasks that require the transfer of semantic structure and content from one domain to another. Advanced LLMs match human performance across many task variations. However, humans and LLMs respond differently to certain task variations and semantic distractors. Overall, our data suggest that LLMs are approaching human-level performance on these important cognitive tasks, but are not yet entirely human like.",Semantic Structure-Mapping in LLM and Human Analogical Reasoning,https://www.semanticscholar.org/paper/138133b86efa826b3d67f924cd5ba861bacf9add,"Analogical reasoning is a unique ability of humans to address unfamiliar challenges by transferring strategies from relevant past experiences. One key finding in psychology is that compared with irrelevant past experiences, recalling relevant ones can help humans better handle new tasks. Coincidentally, the NLP community has also recently found that self-generating relevant examples in the context can help large language models (LLMs) better solve a given problem than hand-crafted prompts. However, it is yet not clear whether relevance is the key factor eliciting such capability, i.e., can LLMs benefit more from self-generated relevant examples than irrelevant ones? In this work, we systematically explore whether LLMs can truly perform analogical reasoning on a diverse set of reasoning tasks. With extensive experiments and analysis, we show that self-generated random examples can surprisingly achieve comparable or even better performance, e.g., 4% performance boost on GSM8K with random biological examples. We find that the accuracy of self-generated examples is the key factor and subsequently design two improved methods with significantly reduced inference costs. Overall, we aim to advance a deeper understanding of LLM analogical reasoning and hope this work stimulates further research in the design of self-generated contexts.",Relevant or Random: Can LLMs Truly Perform Analogical Reasoning?,https://www.semanticscholar.org/paper/b2ee9afe5142bd47163c4261a544e77a622023e0,not novel
"investigate the role of gamified crowdsourcing applications in enhancing collaborative problem-solving skills in science education, a study could implement a gamified platform where students collectively solve scientific challenges and measure their engagement and learning outcomes.","['Education', 'Science Education', 'Gamification', 'Educational Technology', 'Collaborative Learning']","The implementation of gamification in education has attracted many researchers to increase engagement and achieve learning more effectively. Implementing technology in science curricula has seen a massive influx over the past years to stop the decline in students’ motivation towards science learning and promote scientific thinking. This study’s objective is to present the empirical findings of the state-of-the-art literature on the use of gamification in science education. Therefore, we performed a systematic literature review of 24 empirical research papers published in various electronic databases and the web search engine for scholarly literature and academic resources, Google Scholar, between 2012 and 2020. This review reveals the latest emerging trends of gamification in science education while revealing the literature gap, challenges, impediments, and extending the possibilities for future research directions. It examines the conflicting findings of other studies and provides a framework and insight for future researchers regarding content areas, educational levels, theoretical models, outcomes, methodologies, game elements, and assessment tools.",Gamification in Science Education. A Systematic Review of the Literature,https://www.semanticscholar.org/paper/e1ee593d9a7939a328dcfd48a4aab6f5f3a751ee,"In this research, the impact of gamification applications in science education on the science learning motivation of students has been determined and the opinions of students and parents on applications have been discussed. A total of 16 students and their parents partic-ipated in the study. The research was conducted on 4th grade primary students and em-ployed a mixed method consisting of both qualitative and quantitative elements. The moti-vation of students for learning science was obtained through quantitative data, whereas the opinions of students and parents on the usage of gamification applications were gathered through qualitative data. In the study, it was found that applications in science education created a positive impact on the learning motivation of students for science. Additionally, the research results also displayed that students and parents have positive opinions on the usage of gamification in science education.",Use of Gamification Applications in Science Education,https://www.semanticscholar.org/paper/75c44d61c1e0b45d5af2082a37a8b4c971412084,"Massively multiplayer online games provide rich contexts for fostering scientific and mathematical thinking and reasoning. We are leveraging these affordances by developing an MMOG, The Radix Endeavor, which integrates STEM practices as core game mechanics. In this poster, we will describe how we are designing the game-play experiences around activities in biology and math content linked to the common core and next generation standards. We hope to generate discussion around (1) designing collaborative problem solving activities (2) using log data to assess players at the individual and group level. Massively multiplayer online games (MMOGs) provide rich contexts for fostering scientific and mathematical thinking and reasoning. Due to being massive and persistent, the open-ended game play encourages a sustained investment in “systems-based reasoning, model-based reasoning, [and] evaluative epistemology in which knowledge is treated as an open-ended process of evaluation and argument” (Steinkuehler & Duncan, 2008). This reasoning is often done collaboratively (both synchronously and asynchronously) with peers. We are leveraging these affordances by developing an MMOG, The Radix Endeavor, which integrates science, technology, engineering and math (STEM) practices as core game mechanics. In this poster, we will describe how we are designing the game-play experiences around activities in biology and math content linked to the common core and next generation standards. We hope to generate discussion around the types of strategies for (1) designing collaborative problem solving activities (2) using log data to assess players at the individual and group level. Theoretical Framework Our work is guided by theories of situated and collaborative learning. From the situative perspective, learning is seen as enculturation supported by social interaction (Brown, Collins, Duguid, 1989; Lave & Wenger, 1991). Engagement and participation in activities are dependent on interaction with other people (Greeno, 1998). Many terms have been used to describe collaborative learning, such as cooperative learning (Slavin, 1996, Johnson & Johnson 1999), group processes (Webb and Palinscar, 1996), collective cognitive responsibility (Scardamalia 2002), groupwork (Cohen, 1994), and collaboration (Roschelle, 1992). We define collaborative learning as a group of students with distributed expertise sharing cognitive responsibility for a specific task or goal. The emphasis in collaborative learning is on the learning and cognitive advancement of the group. MMOGs, more than any other type of game, rely on collaboration. Not only is this a necessary so-called, “21st century skill;” it is one that is particularly relevant to science as it is practiced by working scientists. The MMOG: The Radix Endeavor The Radix Endeavor is a Massively Multiplayer Online Game (MMOG) being developed by The Education Arcade at the Massachusetts Institute of Technology, designed to improve learning and interest in STEM in high school students. The content specifically focuses on statistics, algebra, probability, geometry, ecology, evolution, genetics, and human body systems. Players take on the role of mathematicians and scientists and embark on quests that encourage them to explore and interact with the virtual world through math and science. Players become embedded in a narrative in the world where they encounter a villain who does not believe in the practices of science. Players have to reason about science issues applicable to game characters' everyday lives, refute the unscientific claims of the villain, and make choices based on what they consider to be valid evidence.",The Radix Endeavor: Designing a Massively Multiplayer Online Game around Collaborative Problem Solving in STEM,https://www.semanticscholar.org/paper/02087f1159ec82ec923ca64e083033eb9c7e73a2,"One way of engaging a learning community in the creation and evolution of its own e-learning resources is through crowdsourcing . Crowdsourcing refers to a problem-solving approach that taps into the knowledge and creativity of groups rather than individuals (Estelles & Gonzales 2012). The ‘ crowd’ could be community members, students or educators. They become the ‘ source’ of information and inspiration by being asked for contributions to solve a problem. Contributions can be large or small and in any number of forms including suggestions, photographs, artwork, quotes etc. Digital platforms such as social media allow micro-contributions of dozens or hundreds of participants. Qualitative analysis of these data can lead to pedagogical innovations or adaptations. For e-learning design, input can also inform style, characters/avatars, subject matter, emphasis, and use of community vocabulary (slang) which may aid an e-learning tool to better articulate with its intended audience. This workshop will show in practice how ideas can be crowdsourced digitally and analysed to generate solutions. The presenters will demonstrate a range of crowdsourced projects from their own areas of expertise, including STEM education , sexual health education and an HKU Common Core Course (Humanities division). The second part of the demonstration is a hands-on exercise in which participants solve real problems in small groups by crowdsourcing material using a digital platform. This workshop is particularly aligned with the sub-theme of technology and social change , as it increases ownership and engagement by students or the community through their own contributions. The presenters include a local NGO worker who would add an extra dimension to the academic perspectives of teacher-and-student, showing the potential of digital crowdsourcing to be a societal change-maker. Intended audience University teachers who would like to incorporate student-driven content and creativity into their digital teaching programmes. Academics and technology innovators seeking to solve problems using widespread input rather than narrow expert opinion . Estelles-Arolas, E., & Gonzalez-Ladron-De-Guevara, F. (2012). Towards an Integrated Crowdsourcing Definition. Journal of Information science, 38 (2), 189-200.",Crowdsourcing for digital learning,https://www.semanticscholar.org/paper/944f63e226d176b1f33e4ae8a65b76e9d595ef08,Abstract,The Effectiveness Of An Educational Environment Based Gamification Strategy Through An Educational Platform In The Acquisition Of Scientific Concepts And The Attitudes Towards Science,https://www.semanticscholar.org/paper/9f3671c537b38d5b68e1b124388e08bc71314d9a,"This paper explores how wiki may be used to support secondary education students’ collaborative creativity processes and how such interaction can promote critical and creativity thinking. A science case-based project in which 81 secondary students participated was designed, implemented and evaluated. Students worked in the science wiki project during two weeks. We scaffold students to be collaboratively engaged in purposeful critical and creative discourse in order to solve collectively science challenges and construct meaning about topics related to environmental challenges. Through the analyses of students’ contributions in the wiki we have characterized collaborative creativity processes in science inquiry that includes performance (processes to develop a novel way of approaching and understanding the problem) and collaboration (peer collaboration, dialogue). The significance of the paper relays on the operationalization of the collaborative creativity processes in the wiki within four overarching learning to learn together skills, which are: distributed leadership, mutual engagement, peer evaluation and group reflection. Our findings showed that the wiki environment afforded the development of an effective and creative online collaborative learning community. In student’s wiki contributions, the four learning to learn together skills took place. However, not all the groups displayed the four learning together skills during their collaboration in the wiki and there were differences among groups in relation to the presence and proportion of these skills. We discuss the contribution of these four learning to learn together skills for the collaborative creativity processes and the relation of the presence of the above mentioned skills with the level of creativity showed in the collaborative writing product students produced in the wiki project. Besides, the paper discusses a series of issues that instructors should consider when wikis are incorporated into teaching and learning for creativity. We claim that embedded scaffolds to help students to argue and reason creatively in their contributions in the wiki environment are needed.",Collaborative Creativity Processes in a Wiki: A Study in Secondary Education.,https://www.semanticscholar.org/paper/48c79106ea1b3882c43fe06c47f02cc280a8300e,"Problem-based learning (PBL) is a widely recommended method in science, technology, engineering, and mathematics (STEM) education through which students develop their scientific knowledge by collaboratively solving real-world problems. PBL benefits from both the activation of creative thinking and from socially shared regulation of learning (SSRL)-a group-level phenomenon whereby students collectively share common perceptions of their collaborative learning process and co-construction of knowledge. The current study examines the influence of three types of support (question prompts designed to promote SSRL, creative thinking, or a combination of both) on the participation of individuals in SSRL processes and on their knowledge acquisition, using a sample of 104 seventh-graders in accelerated science classes. Individuals' participation through the different stages of SSRL (forethought, performance, and reflection) was assessed using video recordings, and their scientific knowledge was measured through pre-and post-intervention knowledge tests. While all groups improved their scientific knowledge, individuals receiving only SSRL support improved their participation in most stages of SSRL compared with those receiving creativity or combined support, and a control group which received no support. The findings strengthen the case for SSRL-directed question prompts as a means to enhance student engagement in problem-solving tasks.",Prompting Socially Shared Regulation of Learning and Creativity in Solving STEM Problems,https://www.semanticscholar.org/paper/f4b32cfbfa8761854ee44b0609fcdfc1255dcf4e,"Gamification whereby game mechanics are integrated into a non-game experience, such as a learning experience has a positive effect on engagement. STEM education is not currently taking full advantage of the possibilities offered by the implementation of gamification into science lessons. More worryingly, Europe is facing a shortage of scientists in the future, as students are disengaging from STEM subjects, finding them difficult and irrelevant. Science teaching has not caught up with the millennials, who by nature are digital natives, having grown up in an age of technological advancements and video games. While there are many ways to include gamification into education, the implementations made must be genuinely beneficial, and not only changes for the sake of changes. Here, we present the results of a gamification survey, carried out as part of the NEWTON H2020 Project, and discuss the most useful aspects of gamification mechanisms.",Gamification Elements in STEM Subjects Lessons Learned from NEWTON Project,https://www.semanticscholar.org/paper/22f80f26ac277e6a545402e3ccc66ee55189f614,"Today's Millennial students have changing preferences for education and work environments that negatively affect their enrollment and retention rates into university computer science programs. To better suit these preferences, and to improve CS educational techniques, teaching methods and tools outside of the traditional lecture sessions and textbooks must be explored and implemented. Currently, both serious games and collaborative classroom work, including pair programming, are the focus of studies meant to do just this. The proposed work deals with both serious games and student collaboration research, positing that educational games with collaborative elements (multiplayer games) will take advantage of the benefits offered by each of these areas, resulting in an educational game that demonstrates increased learning gains and student engagement above that of individual learning game experiences. Collaborative educational games and software also have the potential to solve many of the problems that collaborative work may pose to course instructors in terms of helping to regulate and evaluate student performance.",Games for CS education: computer-supported collaborative learning and multiplayer games,https://www.semanticscholar.org/paper/0c52ad48cbf69a1ad5baf07e431f74e28fe4175b,"In recent years, gamification, the use of game elements in non-game contexts, has drawn the attention of educators due to the possibility of making learning more motivating and engaging; this led to an increase of research in the field. Despite the availability of literature reviews about gamification and its effects, no work to this date has focused exclusively on Higher Education (HE). Next, worldwide there is an increasing demand for skilled Science, Technology, Engineering and Mathematics (STEM) professionals that meet the challenges related to scientific and technological innovations of the 21st Century. This lead to the need of strengthening STEM Higher Education. This brings us to the purpose of this work: presenting a systematic literature review of empirical studies about gamification STEM related Higher Education. This review study started from a systematic mapping design of 'Web of Science' articles, with following inclusion criteria: empirical gamification studies set up in HE, published between 2000 and 2016; focusing on undergraduate or graduate students; in the STEM knowledge field, and set up in authentic settings. An initial search resulted in 562 potentially relevant articles. After applying all selection criteria, only 18 studies could be retained. 12 additional articles were included by analyzing references from earlier literature reviews, resulting in 30 studies to be included. Analysis results show how a combination of game elements (e.g. leaderboards, badges, points and other combinations) positively affects students' performance, attendance, goal orientation and attitude towards mostly computer science related subjects. The analysis results also point at a lack of studies in certain STEM areas, a lack of studies that identify the particular game element associated with the positive differential impact on student performance; a lack of validated psychometric measurements, and lack of focus on student variables that could/should be taken into account as mediating/moderating variables clarifying the impact of gamification in the HE focus on STEM learning and teaching.",Gamification in higher education and stem : a systematic review of literature,https://www.semanticscholar.org/paper/075819ba6abdb7d60eb94d4f70d9695fe37fdc7b,not novel
"Develop a system that leverages **faceted author representation** to identify research papers that can significantly **improve model accuracy and interpretability**. By evaluating **user studies with researchers**, this system would help identify key publications and scholars whose work can be integrated into machine learning models to enhance their performance and interpretability. This fusion can lead to the discovery of novel approaches and methodologies, bridging different scientific communities and promoting interdisciplinary innovation.","['Computer Science', 'Information Retrieval', 'Recommender Systems', 'Human-Computer Interaction', 'Scientific Innovation', 'Interdisciplinary Research']","Isolated silos of scientific research and the growing challenge of information overload limit awareness across the literature and hinder innovation. Algorithmic curation and recommendation, which often prioritize relevance, can further reinforce these informational “filter bubbles.” In response, we describe Bridger, a system for facilitating discovery of scholars and their work. We construct a faceted representation of authors with information gleaned from their papers and inferred author personas, and use it to develop an approach that locates commonalities and contrasts between scientists to balance relevance and novelty. In studies with computer science researchers, this approach helps users discover authors considered useful for generating novel research directions. We also demonstrate an approach for displaying information about authors, boosting the ability to understand the work of new, unfamiliar scholars. Our analysis reveals that Bridger connects authors who have different citation profiles and publish in different venues, raising the prospect of bridging diverse scientific communities.",Bursting Scientific Filter Bubbles: Boosting Innovation via Novel Author Discovery,https://www.semanticscholar.org/paper/bea1c05b0584a4463cbfbcf4917d6afacaef0bde,"Scientific silos can hinder innovation. These information “filter bubbles” and the growing challenge of information overload limit awareness across the literature, making it difficult to keep track of even narrow areas of interest, let alone discover new ones. Al-gorithmic curation and recommendation, which often prioritize relevance, can further reinforce these bubbles. In response, we describe Bridger, a system for facilitating discovery of scholars and their work, to explore design tradeoffs among relevant and novel recommendations. We construct a faceted representation of authors using information extracted from their papers and inferred personas. We explore approaches both for recommending new content and for displaying it in a manner that helps researchers to understand the work of authors who they are unfamiliar with. In studies with computer science researchers, our approach substantially improves users’ abilities to do so. We develop an approach that locates commonalities and contrasts between scientists—retrieving partially similar authors, rather than aiming for strict similarity. We find this approach helps users discover authors useful for generating novel research ideas of relevance to their work, at a higher rate than a state-of-art neural model. Our analysis reveals that Bridger connects authors who have different citation profiles, publish in different venues, and are more distant in social co-authorship networks, raising the prospect of bridging diverse communities and facilitating discovery.",Bridger: Toward Bursting Scientific Filter Bubbles and Boosting Innovation via Novel Author Discovery,https://www.semanticscholar.org/paper/4d02f1a1b99a21665ef94bcdec733a3f8f4056b4,"In order to help scholars understand and follow a research topic, significant research has been devoted to creating systems that help scholars discover relevant papers and authors. Recent approaches have shown the usefulness of highlighting relevant authors while scholars engage in paper discovery. However, these systems do not capture and utilize users’ evolving knowledge of authors. We reflect on the design space and introduce ComLittee, a literature discovery system that supports author-centric exploration. In contrast to paper-centric interaction in prior systems, ComLittee’s author-centric interaction supports curating research threads from individual authors, finding new authors and papers using combined signals from a paper recommender and the curated authors’ authorship graphs, and understanding them in the context of those signals. In a within-subjects experiment that compares to a paper-centric discovery system with author-highlighting, we demonstrate how ComLittee improves author and paper discovery.",ComLittee: Literature Discovery with Personal Elected Author Committees,https://www.semanticscholar.org/paper/7f95d982f8ed3189d84577f1fdf07f93c99423f2,"AMiner is the second generation of the ArnetMiner system. We focus on developing author-centric analytic and mining tools for gaining a deep understanding of the large and heterogeneous networks formed by authors, papers, venues, and knowledge concepts. One fundamental goal is how to extract and integrate semantics from different sources. We have developed algorithms to automatically extract researchers' profiles from the Web and re- solve the name ambiguity problem, and connect different professional networks. We also developed methodologies to incorporate knowledge from the Wikipedia and other sources into the system to bridge the gap between network science and the web mining research. In this talk, I will focus on answering two fundamental questions for author-centric network analysis: who is who? and who are similar to each other? The system has been in operation since 2006 and has collected more than 100,000,000 author profiles, 100,000,000 publication papers, and 7,800,000 knowledge concepts. It has been widely used for collaboration recommendation, similarity analysis, and community evolution.",AMiner: Mining Deep Knowledge from Big Scholar Data,https://www.semanticscholar.org/paper/169edf0ad0b5e3ab31df4481a9069cbfdb774423,"Rexplore leverages novel solutions in data mining, semantic technologies and visual analytics, and provides an innovative environment for exploring and making sense of scholarly data. Rexplore allows users: 1) to detect and make sense of important trends in research; 2) to identify a variety of interesting relations between researchers, beyond the standard co-authorship relations provided by most other systems; 3) to perform fine-grained expert search with respect to detailed multi-dimensional parameters; 4) to detect and characterize the dynamics of interesting communities of researchers, identified on the basis of shared research interests and scientific trajectories; 5) to analyse research performance at different levels of abstraction, including individual researchers, organizations, countries, and research communities.",Online The Open University ’ s repository of research publications and other research outputs Understanding research dynamics,https://www.semanticscholar.org/paper/cfe3906ad90ef852a29c60aa948f0b261f8da416,"Finding potential collaborators has become a challenge due to the growing number of scientists in organizations such as universities, research institutes, or companies. Collaboration Recommendation Systems (CRSs) have been developed to help researchers identify possible collaboration partners, but they often rely on citation graphs or paper abstracts which may not be readily available in organizational databases or online sources. However, scientific publication titles provide consistent bibliometric data that can provide insights into research areas. TOMOSCO is a topic modelling framework that uses transformer-based methods to extract research area information from small amounts of text, such as publication titles or brief project descriptions. TOMOSCO can classify, cluster, and match research topics across different disciplines, uncovering relationships among scientists and suggesting potential interdisciplinary collaborations. In experiments, TOMOSCO was able to identify existing collaborations with over 90% accuracy based solely on publication titles and propose new collaborations based on previously unseen publications and project descriptions.",Exploiting Topic Modelling for the Identification of Untapped Scientific Collaborations,https://www.semanticscholar.org/paper/ed12a2ed110c00c70a57b78fc476fda98b1231ca,"Interdisciplinary studies often require researchers to explore literature in diverse branches of knowledge. Yet, navigating through the highly scattered knowledge from unfamiliar disciplines poses a significant challenge. In this paper, we introduce DiscipLink, a novel interactive system that facilitates collaboration between researchers and large language models (LLMs) in interdisciplinary information seeking (IIS). Based on users' topics of interest, DiscipLink initiates exploratory questions from the perspectives of possible relevant fields of study, and users can further tailor these questions. DiscipLink then supports users in searching and screening papers under selected questions by automatically expanding queries with disciplinary-specific terminologies, extracting themes from retrieved papers, and highlighting the connections between papers and questions. Our evaluation, comprising a within-subject comparative experiment and an open-ended exploratory study, reveals that DiscipLink can effectively support researchers in breaking down disciplinary boundaries and integrating scattered knowledge in diverse fields. The findings underscore the potential of LLM-powered tools in fostering information-seeking practices and bolstering interdisciplinary research.",DiscipLink: Unfolding Interdisciplinary Information Seeking Process via Human-AI Co-Exploration,https://www.semanticscholar.org/paper/ed1471ddca3ae6842367d299badc8fef14eca44a,"In the extensive recommender systems literature, novelty and diversity have been identified as key properties of useful recommendations. However, these properties have received limited attention in the specific sub-field of research paper recommender systems. In this work, we argue for the importance of offering novel and diverse research paper recommendations to scientists. This approach aims to reduce siloed reading, break down filter bubbles, and promote interdisciplinary research. We propose a novel framework for evaluating the novelty and diversity of research paper recommendations that leverages methods from network analysis and natural language processing. Using this framework, we show that the choice of representational method within a larger research paper recommendation system can have a measurable impact on the nature of downstream recommendations, specifically on their novelty and diversity. We introduce a novel paper embedding method, which we demonstrate offers more innovative and diverse recommendations without sacrificing precision, compared to other state-of-the-art baselines.",The Role of Document Embedding in Research Paper Recommender Systems: To Breakdown or to Bolster Disciplinary Borders?,https://www.semanticscholar.org/paper/3ff3765268e076e14cc3a092f35506c3cb300833,"The ever-increasing pace of scientific publication necessitates methods for quickly identifying relevant papers. While neural recommenders trained on user interests can help, they still result in long, monotonous lists of suggested papers. To improve the discovery experience we introduce multiple new methods for augmenting recommendations with textual relevance messages that highlight knowledge-graph connections between recommended papers and a user’s publication and interaction history. We explore associations mediated by author entities and those using citations alone. In a large-scale, real-world study, we show how our approach significantly increases engagement—and future engagement when mediated by authors—without introducing bias towards highly-cited authors. To expand message coverage for users with less publication or interaction history, we develop a novel method that highlights connections with proxy authors of interest to users and evaluate it in a controlled lab study. Finally, we synthesize design implications for future graph-based messages.",From Who You Know to What You Read: Augmenting Scientific Recommendations with Implicit Social Networks,https://www.semanticscholar.org/paper/cbf0c0593479304ccfa9dd5bcd26d851c1ff72d7,"Serendipity occurs when one finds an interesting discovery while searching for something else. While search engines seek to report work relevant to a targeted query, recommendation engines are particularly well-suited for serendipitous recommendations as such processes do not need to fulfill a targeted query. Junior researchers can use such an engine to broaden their horizon and learn new areas, while senior researchers can discover interdisciplinary frontiers to apply integrative research. We adapt a state-of-the-art scholarly paper recommendation system's user profile construction to make use of information drawn from 1) dissimilar users and 2) co-authors to specifically target serendipitous recommendation.",Serendipitous recommendation for scholarly papers considering relations among researchers,https://www.semanticscholar.org/paper/a7030fe66a8652ac91f1279b4b73d8b413dbdf22,novel
"Create a paper recommendation system that uses **representation alignment** to **improve paper recommendation relevance**. By aligning the representations of recommended papers with those of the user's previously collected papers, the system ensures that new recommendations are contextually relevant and easily understandable. This approach helps bridge the gap between new and existing research, making it easier for scholars to identify valuable papers. The system's effectiveness can be validated through a **performance comparison across datasets**, demonstrating its capability to deliver more contextually aligned and relevant recommendations.","['Computer Science', 'Information Retrieval', 'Recommender Systems', 'Large Language Models', 'Scholarly Communication']","With the rapid growth of scholarly archives, researchers subscribe to “paper alert’’ systems that periodically provide them with recommendations of recently published papers that are similar to previously collected papers. However, researchers sometimes struggle to make sense of nuanced connections between recommended papers and their own research context, as existing systems only present paper titles and abstracts. To help researchers spot these connections, we present PaperWeaver, an enriched paper alerts system that provides contextualized text descriptions of recommended papers based on user-collected papers. PaperWeaver employs a computational method based on Large Language Models (LLMs) to infer users’ research interests from their collected papers, extract context-specific aspects of papers, and compare recommended and collected papers on these aspects. Our user study (N=15) showed that participants using PaperWeaver were able to better understand the relevance of recommended papers and triage them more confidently when compared to a baseline that presented the related work sections from recommended papers.",PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers,https://www.semanticscholar.org/paper/2cbdad900dda764efa362793765ec12cb5ae66f5,"The inception of a research agenda typically commences with the creation of a comprehensive research proposal. The efficacy of the proposal often hinges on its ability to connect with the existing scientific literature that supports its ideas. To effectively assess the relevance of existing articles to a research proposal, it is imperative to categorize these articles into high-level thematic groups, referred to as topics, that align with the proposal. This paper introduces a novel task of aligning scientific articles, relevant to a proposal, with researcher-provided proposal topics. Additionally, we construct a dataset to serve as a benchmark for this task. We establish human and Large Language Model (LLM) baselines and propose a novel three-stage approach to address this challenge. We synthesize and use pseudo-labels that map proposal topics to text spans from cited articles to train Language Models (LMs) for two purposes: (i) as a retriever, to extract relevant text spans from cited articles for each topic, and (ii) as a classifier, to categorize the articles into the proposal topics. Our retriever-classifier pipeline, which employs very small open-source LMs fine-tuned with our constructed dataset, achieves results comparable to a vanilla paid LLM-based classifier, demonstrating its efficacy. However, a notable gap of 23.57 F1 score between our approach and the human baseline highlights the complexity of this task and emphasizes the need for further research.",Beyond Retrieval: Topic-based Alignment of Scientific Papers to Research Proposal,https://www.semanticscholar.org/paper/b79596072a6fe64840057375fad7f2f1b6cdb185,"In the extensive recommender systems literature, novelty and diversity have been identified as key properties of useful recommendations. However, these properties have received limited attention in the specific sub-field of research paper recommender systems. In this work, we argue for the importance of offering novel and diverse research paper recommendations to scientists. This approach aims to reduce siloed reading, break down filter bubbles, and promote interdisciplinary research. We propose a novel framework for evaluating the novelty and diversity of research paper recommendations that leverages methods from network analysis and natural language processing. Using this framework, we show that the choice of representational method within a larger research paper recommendation system can have a measurable impact on the nature of downstream recommendations, specifically on their novelty and diversity. We introduce a novel paper embedding method, which we demonstrate offers more innovative and diverse recommendations without sacrificing precision, compared to other state-of-the-art baselines.",The Role of Document Embedding in Research Paper Recommender Systems: To Breakdown or to Bolster Disciplinary Borders?,https://www.semanticscholar.org/paper/3ff3765268e076e14cc3a092f35506c3cb300833,"Scientific writing involves retrieving, summarizing, and citing relevant papers, which can be time-consuming processes. Although in many workflows these processes are serially linked, there are opportunities for natural language processing (NLP) to provide end-to-end assistive tools. We propose SciLit, a pipeline that automatically recommends relevant papers, extracts highlights, and suggests a reference sentence as a citation of a paper, taking into consideration the user-provided context and keywords. SciLit efficiently recommends papers from large databases of hundreds of millions of papers using a two-stage pre-fetching and re-ranking literature search system that flexibly deals with addition and removal of a paper database. We provide a convenient user interface that displays the recommended papers as extractive summaries and that offers abstractively-generated citing sentences which are aligned with the provided context and which mention the chosen keyword(s). Our assistive tool for literature discovery and scientific writing is available at https://scilit.vercel.app","SciLit: A Platform for Joint Scientific Literature Discovery, Summarization and Citation Generation",https://www.semanticscholar.org/paper/2c17c9bfed8984188172ef94fdf6c7d78b053a89,"Reviewing the literature to understand relevant threads of past work is a critical part of research and vehicle for learning. However, as the scientific literature grows the challenges for users to find and make sense of the many different threads of research grow as well. Previous work has helped scholars to find and group papers with citation information or textual similarity using standalone tools or overview visualizations. Instead, in this work we explore a tool integrated into users’ reading process that helps them with leveraging authors’ existing summarization of threads, typically in introduction or related work sections, in order to situate their own work’s contributions. To explore this we developed a prototype that supports efficient extraction and organization of threads along with supporting evidence as scientists read research articles. The system then recommends further relevant articles based on user-created threads. We evaluate the system in a lab study and find that it helps scientists to follow and curate research threads without breaking out of their flow of reading, collect relevant papers and clips, and discover interesting new articles to further grow threads.",Threddy: An Interactive System for Personalized Thread-based Exploration and Organization of Scientific Literature,https://www.semanticscholar.org/paper/fa1e4d2cc397affeb8816bc98f0d1ef38a5ee8fb,"In the process of literature review writing, researchers need to search and read several papers in order to find those which are relevant to their research. This paper proposes an assisted literature review prototype (STELLAR – Semantic Topics Ecosystem Learning-based Literature Assistant Review) based on (1) text and data mining models that learn from researchers’ annotated data and semantic enriched metadata, (2) machine learning models (MLM) and (3) a semantic metadata ecosystem (SMESE) (i) to discover papers and recommend relevant of them for a specific topic using ranking algorithm and (ii) to identify papers according to researchers’ selections parameters and his annotations. Notice that SMESE is our prototype that semantically harvests papers from different sources. Specifically, STELLAR allows to: Identify the relevant papers from SMESE thanks to the computation of a new ranking index (called, DTb Index) based on paper's semantic and contextual metadata such as discipline, topic, venue, authors in order to define the Literature Corpus of a specific topic or area of research. Define the Literature Corpus Radius making use of value of the similarity between each paper and a specific research area, topic, title and description (called LCR Index). Assist the researcher in refining the list of papers relevant for the literature review. To narrow down the search for relevant papers, many views and relationships of the list of candidate papers are made available. Using various types of datasets and a simulation prototypes, the STELLAR performance was evaluated and compared to two existing approaches.",TEXT AND DATA MINING & MACHINE LEARNING MODELS TO BUILD AN ASSISTED LITERATURE REVIEW WITH RELEVANT PAPERS,https://www.semanticscholar.org/paper/5b1b1e06d4189a507ec507d6a4860b7923f6ef7e,"The number of research papers available today is growing at a staggering rate, generating a huge amount of information that people cannot keep up with. According to a tendency indicated by the United States’ National Science Foundation, more than 10 million new papers will be published in the next 20 years. Because most of these papers will be available on the Web, this research focus on exploring issues on recommending research papers to users, in order to directly lead users to papers of their interest. Recommender systems are used to recommend items to users among a huge stream of available items, according to users’ interests. This research focuses on the two most prevalent techniques to date, namely Content-Based Filtering and Collaborative Filtering. The first explores the text of the paper itself, recommending items similar in content to the ones the user has rated in the past. The second explores the citation web existing among papers. As these two techniques have complementary advantages, we explored hybrid approaches to recommending research papers. We created standalone and hybrid versions of algorithms and evaluated them through both offline experiments on a database of 102,295 papers, and an online experiment with 110 users. Our results show that the two techniques can be successfully combined to recommend papers. The coverage is also increased at the level of 100% in the hybrid algorithms. In addition, we found that different algorithms are more suitable for recommending different kinds of papers. Finally, we verified that users’ research experience influences the way users perceive recommendations. In parallel, we found that there are no significant differences in recommending papers for users from different countries. However, our results showed that users’ interacting with a research paper Recommender Systems are much happier when the interface is presented in the user’s native language, regardless the language that the papers are written. Therefore, an interface should be tailored to the user’s mother language.",Combining collaborative and content-based filtering to recommend research papers,https://www.semanticscholar.org/paper/c73307417197a4657f09ca72a245e8012b516ac1,"Finding relevant scholarly papers is an important task for researchers. Such a literature search involves identifying drawbacks in existing works and proposing new approaches that address them. However, the growing number of scientific published papers results in information overload even for simple searches, such that researchers have difficulty in finding papers relevant to their interests. Recommendation systems can help address this problem to find relevant papers efficiently. In this article, we summarize our work on scholarly paper recommendation from both relevance and serendipitous perspectives. Experimental results on a publicly-available scholarly paper recommendation dataset show that our proposed approaches provides promising recommendations for researchers, outperforming the state-of-the-art with statistical significance.","""Towards higher relevance and serendipity in scholarly paper recommendation"" by Kazunari Sugiyama and Min-Yen Kan with Martin Vesely as coordinator",https://www.semanticscholar.org/paper/522a1e52c19b14be345026fef724d7b159f0a023,"We argue that Content-based filtering (CBF) and Graph-based methods (GB) complement one another in Academic Search recommendations. The scientific literature can be viewed as a conversation between authors and the audience. CBF uses abstracts to infer authors' positions, and GB uses citations to infer responses from the audience. In this paper, we describe nine differences between CBF and GB, as well as synergistic opportunities for hybrid combinations. Two embeddings will be used to illustrate these opportunities: (1) Specter, a CBF method based on BERT-like deepnet encodings of abstracts, and (2) ProNE, a GB method based on spectral clustering of more than 200M papers and 2B citations from Semantic Scholar.",Academic Article Recommendation Using Multiple Perspectives,https://www.semanticscholar.org/paper/aa4fd280be1571876fc59e853ec78d91f34ce5f6,"We present the design and methodology for the large scale hybrid paper recommender system used by Microsoft Academic. The system provides recommendations for approximately 160 million English research papers and patents. Our approach handles incomplete citation information while also alleviating the cold-start problem that often affects other recommender systems. We use the Microsoft Academic Graph (MAG), titles, and available abstracts of research papers to build a recommendation list for all documents, thereby combining co-citation and content based approaches. Tuning system parameters also allows for blending and prioritization of each approach which, in turn, allows us to balance paper novelty versus authority in recommendation results. We evaluate the generated recommendations via a user study of 40 participants, with over 2400 recommendation pairs graded and discuss the quality of the results using P@10 and nDCG scores. We see that there is a strong correlation between participant scores and the similarity rankings produced by our system but that additional focus needs to be put towards improving recommender precision, particularly for content based recommendations. The results of the user survey and associated analysis scripts are made available via GitHub and the recommendations produced by our system are available as part of the MAG on Azure to facilitate further research and light up novel research paper recommendation applications.",A Scalable Hybrid Research Paper Recommender System for Microsoft Academic,https://www.semanticscholar.org/paper/bb246e08bc6641672c2bb2b93d4214eccf3f84b6,not novel
"Develop a **hybrid interface for process control** that combines **ai explanations generated with different techniques** with **auto-adaptive multimedia interface** elements to support operators in managing complex real-time processes. The AI explanations will be tailored to the operators' tasks and presented through adaptive multimedia formats, enhancing usability and decision-making efficiency. **Industrial field-test applications** will be conducted to evaluate the effectiveness of this hybrid interface in real-world scenarios, focusing on improvements in task performance, user satisfaction, and decision accuracy.","['Artificial Intelligence', 'Explainable AI', 'Human-AI Interaction', 'Decision-Making', 'User Studies', 'Human-Centered AI', 'Industrial Applications', 'Process Control', 'Multimedia Interfaces']","Explainability (XAI) has matured in recent years to provide more human-centered explanations of AI-based decision systems. While static explanations remain predominant, interactive XAI has gathered momentum to support the human cognitive process of explaining. However, the evidence regarding the benefits of interactive explanations is unclear. In this paper, we map existing findings by conducting a detailed scoping review of 48 empirical studies in which interactive explanations are evaluated with human users. We also create a classification of interactive techniques specific to XAI and group the resulting categories according to their role in the cognitive process of explanation: ""selective"", ""mutable"" or ""dialogic"". We identify the effects of interactivity on several user-based metrics. We find that interactive explanations improve perceived usefulness and performance of the human+AI team but take longer. We highlight conflicting results regarding cognitive load and overconfidence. Lastly, we describe underexplored areas including measuring curiosity or learning or perturbing outcomes.","On Selective, Mutable and Dialogic XAI: a Review of What Users Say about Different Types of Interactive Explanations",https://www.semanticscholar.org/paper/5694c7a8759847a4f13a5c0ba7ee37297372f3ca,"AI explanations have been increasingly used to help people better utilize AI recommendations in AI-assisted decision making. While numerous technical transparency approaches have been established, a human-centered perspective is needed for understanding how human decision makers use and process AI explanations. In my thesis, I start with an empirical exploration of how AI explanations shape the way people understand and utilize AI decision aids. Next, I move to the time‑evolving nature of AI explanations, exploring how explanation changes due to AI model updates affect human decision makers’ perception and usage of AI models. Lastly, I construct computational human behavior models to gain a more quantitative understandings of human decision makers’ cognitive interactions with AI explanations. I conclude with future work on carefully identifying user needs for explainable AI in an era when AI models are becoming more complex and human-AI collaboration scenarios are increasingly diversified.",Human-Centered Evaluation of Explanations in AI-Assisted Decision-Making,https://www.semanticscholar.org/paper/af46821116b0b90f9043c50ee153d234d13b2533,"Despite the proliferation of explainable AI (XAI) methods, little is understood about end-users’ explainability needs and behaviors around XAI explanations. To address this gap and contribute to understanding how explainability can support human-AI interaction, we conducted a mixed-methods study with 20 end-users of a real-world AI application, the Merlin bird identification app, and inquired about their XAI needs, uses, and perceptions. We found that participants desire practically useful information that can improve their collaboration with the AI, more so than technical system details. Relatedly, participants intended to use XAI explanations for various purposes beyond understanding the AI’s outputs: calibrating trust, improving their task skills, changing their behavior to supply better inputs to the AI, and giving constructive feedback to developers. Finally, among existing XAI approaches, participants preferred part-based explanations that resemble human reasoning and explanations. We discuss the implications of our findings and provide recommendations for future XAI design.","""Help Me Help the AI"": Understanding How Explainability Can Support Human-AI Interaction",https://www.semanticscholar.org/paper/b4227655b5a6a9080613a0cbed7666948423a597,"This is an integrative review that address the question, ""What makes for a good explanation?"" with reference to AI systems. Pertinent literatures are vast. Thus, this review is necessarily selective. That said, most of the key concepts and issues are expressed in this Report. The Report encapsulates the history of computer science efforts to create systems that explain and instruct (intelligent tutoring systems and expert systems). The Report expresses the explainability issues and challenges in modern AI, and presents capsule views of the leading psychological theories of explanation. Certain articles stand out by virtue of their particular relevance to XAI, and their methods, results, and key points are highlighted. It is recommended that AI/XAI researchers be encouraged to include in their research reports fuller details on their empirical or experimental methods, in the fashion of experimental psychology research reports: details on Participants, Instructions, Procedures, Tasks, Dependent Variables (operational definitions of the measures and metrics), Independent Variables (conditions), and Control Conditions.","Explanation in Human-AI Systems: A Literature Meta-Review, Synopsis of Key Ideas and Publications, and Bibliography for Explainable AI",https://www.semanticscholar.org/paper/5c3a72f47ed8d58c0554210828af1ce4bbf2dbcd,"Empirical studies have extensively investigated human decision-making processes in various domains where AI systems are incorporated. However, comparing and replicating these studies can be challenging due to different experimental configurations. Moreover, the existing contexts often have limited scope and may not fully capture the complexity of real-world decision-making scenarios that are riddled with varying levels of uncertainty. Our framework addresses these practical gaps by providing a configurable and reproducible environment for conducting human-AI decision-making studies in the route planning domain that captures many complexities of real-world scenarios. Researchers can customize parameters, conditions, and factors involved in decision-making tasks to help address research and empirical gaps through rigorous experiments. With various modules such as map generation, chat components, and different AI systems available within the “DecisionTime” framework, researchers can effortlessly design experiments exploring multiple aspects of human-AI interaction and decision-making.",“DecisionTime”: A Configurable Framework for Reproducible Human-AI Decision-Making Studies,https://www.semanticscholar.org/paper/8e10162cef8920d3804f3f069bb8aee3a234e16e,"
 Intelligent systems have been rapidly evolving and play a pivotal role in assisting individuals across diverse domains, from healthcare to transportation. Understanding the dynamics of human-Artificial Intelligence (AI) partnering, particularly how humans trust and collaborate with intelligent systems, is becoming increasingly critical to design effective systems. This paper presents an experimental analysis to assess the impact of AI design attributes on users' trust, workload and performance when solving classification problems supported by an AI assistant. Specifically, we study the effect of transparency, fairness, and robustness in the design of an AI assistant and analyze the role of participants' gender and education background on the outcomes. The experiment is conducted with 47 students in undergraduate, master's and Ph.D. programs using a drawing game application where the users are asked to recognize incomplete sketches revealed progressively while receiving recommendations from multiple versions of an AI assistant. The results show that when collaborating with the AI, participants achieve a higher performance than their individual performance or the performance of the AI. The results also show that gender does not have an impact on users' trust and performance when collaborating with different versions of the AI system, whereas education level has a significant impact on the participants' performance but not on trust. Finally, the impact of design attributes on participants' trust and performance highly depends on the accuracy of the AI recommendations, and improvements in participants' performance and trust in some cases come at the expense of increased workload.","Trust, Workload and Performance in Human-AI Partnering: The Role of AI Attributes in Solving Classification Problems",https://www.semanticscholar.org/paper/7d37f08f2580850caad7a9b5436b3abcc2316ba6,"With Artificial Intelligence (AI) becoming ubiquitous in every application domain, the need for explanations is paramount to enhance transparency and trust among non-technical users. Despite the potential shown by Explainable AI (XAI) for enhancing understanding of complex AI systems, most XAI methods are designed for technical AI experts rather than non-technical consumers. Consequently, such explanations are overwhelmingly complex and seldom guide users in achieving their desired predicted outcomes. This paper presents ongoing research for crafting XAI systems tailored to guide users in achieving desired outcomes through improved human-AI interactions. This paper highlights the research objectives and methods, key takeaways and implications learned from user studies. It outlines open questions and challenges for enhanced human-AI collaboration, which the author aims to address in future work.",Towards Directive Explanations: Crafting Explainable AI Systems for Actionable Human-AI Interactions,https://www.semanticscholar.org/paper/f5013745b42fa4f7f7cca1921aa723fa502533c8,"Modern artificial intelligence (AI) and machine learning (ML) systems have become more capable and more widely used, but often involve underlying processes their users do not understand and may not trust. Some researchers have addressed this by developing algorithms that help explain the workings of the system using ‘Explainable’ AI algorithms (XAI), but these have not always been successful in improving their understanding. Alternatively, collaborative user-driven explanations may address the needs of users, augmenting or replacing algorithmic explanations. We evaluate one such approach called “collaborative explainable AI” (CXAI). Across two experiments, we examined CXAI to assess whether users’ mental models, performance, and satisfaction improved with access to user-generated explanations. Results showed that collaborative explanations afforded users a better understanding of and satisfaction with the system than users without access to the explanations, suggesting that a CXAI system may provide a useful support that more dominant XAI approaches do not.",Assessing Satisfaction in and Understanding of a Collaborative Explainable AI (Cxai) System through User Studies,https://www.semanticscholar.org/paper/e5d7ffb0e4d8c01679669ed0c384f54fb313a18d,". Explainable AI (XAI) can greatly enhance user trust and satisfaction in AI-assisted decision-making processes. Recent findings suggest that a single explainer may not meet the diverse needs of multiple users in an AI system; indeed, even individual users may require multiple explanations. This highlights the necessity for a “multi-shot” approach, employing a combination of explainers to form what we refer to as an “explanation strategy”. Tailored to a specific user or a user group, an “explanation experience” describes interactions with personalised strategies designed to enhance their AI decision-making processes. The iSee platform is designed for the intelligent sharing and reuse of explanation experiences, using Case-based Reasoning to advance best practices in XAI. The platform provides tools to AI system designers, i.e. design users, to design and iteratively revise the most suitable explanation strategy for their AI system that satisfies end-user needs. All knowledge generated within the iSee platform is formalised by the iSee ontology for interoperability. We evaluate the usability and utility of the iSee platform with six design users across various application domains. The findings showcase how the iSee platform generalises across applications and its potential to promote the adoption of XAI best practices.",iSee: Advancing Multi-Shot Explainable AI Using Case-based Recommendations,https://www.semanticscholar.org/paper/3ca0e854b4ea70701a07582bbdacbea647cffb52,"In recent years, the field of explainable AI (XAI) has produced a vast collection of algorithms, providing a useful toolbox for researchers and practitioners to build XAI applications. With the rich application opportunities, explainability is believed to have moved beyond a demand by data scientists or researchers to comprehend the models they develop, to an essential requirement for people to trust and adopt AI deployed in numerous domains. However, explainability is an inherently human-centric property and the field is starting to embrace human-centered approaches. Human-computer interaction (HCI) research and user experience (UX) design in this area are becoming increasingly important. In this chapter, we begin with a high-level overview of the technical landscape of XAI algorithms, then selectively survey our own and other recent HCI works that take human-centered approaches to design, evaluate, and provide conceptual and methodological tools for XAI. We ask the question""what are human-centered approaches doing for XAI""and highlight three roles that they play in shaping XAI technologies by helping navigate, assess and expand the XAI toolbox: to drive technical choices by users' explainability needs, to uncover pitfalls of existing XAI methods and inform new methods, and to provide conceptual frameworks for human-compatible XAI.",Human-Centered Explainable AI (XAI): From Algorithms to User Experiences,https://www.semanticscholar.org/paper/5e1746995debd1f17c24af01514c727598cc5613,novel
"Develop a **Co-Creative Interaction Framework** for LLM-assisted evaluations to **align llm evaluation with human preferences**. This framework will map out detailed interaction models between human evaluators and LLMs, including turn-taking, communication protocols, and iterative feedback loops. The framework will be evaluated through **qualitative study** involving user testing and thematic analysis of evaluator experiences, capturing how well the framework supports alignment with human preferences and identifying areas for iterative improvement.","['Computer Science', 'Human-Computer Interaction', 'Natural Language Processing', 'Large Language Models', 'Evaluation Methods', 'Human-Centered AI']","By simply composing prompts, developers can prototype novel generative applications with Large Language Models (LLMs). To refine prototypes into products, however, developers must iteratively revise prompts by evaluating outputs to diagnose weaknesses. Formative interviews (N=8) revealed that developers invest significant effort in manually evaluating outputs as they assess context-specific and subjective criteria. We present EvalLM, an interactive system for iteratively refining prompts by evaluating multiple outputs on user-defined criteria. By describing criteria in natural language, users can employ the system’s LLM-based evaluator to get an overview of where prompts excel or fail, and improve these based on the evaluator’s feedback. A comparative study (N=12) showed that EvalLM, when compared to manual evaluation, helped participants compose more diverse criteria, examine twice as many outputs, and reach satisfactory prompts with 59% fewer revisions. Beyond prompts, our work can be extended to augment model evaluation and alignment in specific application contexts.",EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria,https://www.semanticscholar.org/paper/a0d83f9e15e722f23c14eb83cb2f87c1d1ea6400,"Traditional reference-based metrics, such as BLEU and ROUGE, are less effective for assessing outputs from Large Language Models (LLMs) that produce highly creative or superior-quality text, or in situations where reference outputs are unavailable. While human evaluation remains an option, it is costly and difficult to scale. Recent work using LLMs as evaluators (LLM-as-a-judge) is promising, but trust and reliability remain a significant concern. Integrating human input is crucial to ensure criteria used to evaluate are aligned with the human’s intent, and evaluations are robust and consistent. This paper presents a user study of a design exploration called EvaluLLM, that enables users to leverage LLMs as customizable judges, promoting human involvement to balance trust and cost-saving potential with caution. Through interviews with eight domain experts, we identified the need for assistance in developing effective evaluation criteria aligning the LLM-as-a-judge with practitioners’ preferences and expectations. We offer findings and design recommendations to optimize human-assisted LLM-as-judge systems.",Human-Centered Design Recommendations for LLM-as-a-judge,https://www.semanticscholar.org/paper/52570796ad1fdc20a367caf1d099e729d6456241,"We introduce CheckEval, a novel evaluation framework using Large Language Models, addressing the challenges of ambiguity and inconsistency in current evaluation methods. CheckEval addresses these challenges by dividing evaluation criteria into detailed sub-aspects and constructing a checklist of Boolean questions for each, simplifying the evaluation. This approach not only renders the process more interpretable but also significantly enhances the robustness and reliability of results by focusing on specific evaluation dimensions. Validated through a focused case study using the SummEval benchmark, CheckEval indicates a strong correlation with human judgments. Furthermore, it demonstrates a highly consistent Inter-Annotator Agreement. These findings highlight the effectiveness of CheckEval for objective, flexible, and precise evaluations. By offering a customizable and interactive framework, CheckEval sets a new standard for the use of LLMs in evaluation, responding to the evolving needs of the field and establishing a clear method for future LLM-based evaluation.",CheckEval: Robust Evaluation Framework using Large Language Model via Checklist,https://www.semanticscholar.org/paper/afd83013ba5e6cefb0b1c09084e8c6a15a47e0c3,"The recent advancements in Large Language Models (LLMs) have significantly impacted numerous, and will impact more, real-world applications. However, these models also pose significant risks to individuals and society. To mitigate these issues and guide future model development, responsible evaluation and auditing of LLMs are essential. This workshop aims to address the current “evaluation crisis” in LLM research and practice by bringing together HCI and AI researchers and practitioners to rethink LLM evaluation and auditing from a human-centered perspective. The workshop will explore topics around understanding stakeholders’ needs and goals with evaluation and auditing LLMs, establishing human-centered evaluation and auditing methods, developing tools and resources to support these methods, building community and fostering collaboration. By soliciting papers, organizing invited keynote and panel, and facilitating group discussions, this workshop aims to develop a future research agenda for addressing the challenges in LLM evaluation and auditing.",Human-Centered Evaluation and Auditing of Language Models,https://www.semanticscholar.org/paper/2ab90f60ea2e0d345f8f6f66dd4efa5d347eac25,"Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment. Additionally, they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. To address these issues, we introduce Prometheus 2, a more powerful evaluator LM than its predecessor that closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, Prometheus 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Our models, code, and data are all publicly available at https://github.com/prometheus-eval/prometheus-eval.",Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models,https://www.semanticscholar.org/paper/ecdd53eaab7455daea27609b07a418a21aa7ad35,"Due to the cumbersome nature of human evaluation and limitations of code-based evaluation, Large Language Models (LLMs) are increasingly being used to assist humans in evaluating LLM outputs. Yet LLM-generated evaluators simply inherit all the problems of the LLMs they evaluate, requiring further human validation. We present a mixed-initiative approach to ``validate the validators'' -- aligning LLM-generated evaluation functions (be it prompts or code) with human requirements. Our interface, EvalGen, provides automated assistance to users in generating evaluation criteria and implementing assertions. While generating candidate implementations (Python functions, LLM grader prompts), EvalGen asks humans to grade a subset of LLM outputs; this feedback is used to select implementations that better align with user grades. A qualitative study finds overall support for EvalGen but underscores the subjectivity and iterative process of alignment. In particular, we identify a phenomenon we dub \emph{criteria drift}: users need criteria to grade outputs, but grading outputs helps users define criteria. What is more, some criteria appears \emph{dependent} on the specific LLM outputs observed (rather than independent criteria that can be defined \emph{a priori}), raising serious questions for approaches that assume the independence of evaluation from observation of model outputs. We present our interface and implementation details, a comparison of our algorithm with a baseline approach, and implications for the design of future LLM evaluation assistants.",Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences,https://www.semanticscholar.org/paper/6098ac103ba222f9c3b089714ef3100357993255,"The Experience Sampling Method (ESM) is commonly used to understand behaviors, thoughts, and feelings in the wild by collecting self-reports. Sustaining sufficient response rates, especially in long-running studies remains challenging. To avoid low response rates and dropouts, experimenters rely on their experience, proposed methodologies from earlier studies, trial and error, or the scarcely available participant behavior data from previous ESM protocols. This approach often fails in finding the acceptable study parameters, resulting in redesigning the protocol and repeating the experiment. Research has shown the potential of machine learning to personalize ESM protocols such that ESM prompts are delivered at opportune moments, leading to higher response rates. The corresponding training process is hindered due to the scarcity of open data in the ESM domain, causing a cold start, which could be mitigated by simulating participant behavior. Such simulations provide training data and insights for the experimenters to update their study design choices. Creating this simulation requires behavioral science, psychology, and programming expertise. Large language models (LLMs) have emerged as facilitators for information inquiry and programming, albeit random and occasionally unreliable. We aspire to assess the readiness of LLMs in an ESM use case. We conducted research using GPT-3.5 turbo-16k to tackle an ESM simulation problem. We explored several prompt design alternatives to generate ESM simulation programs, evaluated the output code in terms of semantics and syntax, and interviewed ESM practitioners. We found that engineering LLM-enabled ESM simulations have the potential to facilitate data generation, but they perpetuate trust and reliability challenges.",Evaluation of Code Generation for Simulating Participant Behavior in Experience Sampling Method by Iterative In-Context Learning of a Large Language Model,https://www.semanticscholar.org/paper/7a4bbfb0fdddbaa20214b90bffe0a0fc1d1aedf8,"As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer (""sycophancy"") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors.",Discovering Language Model Behaviors with Model-Written Evaluations,https://www.semanticscholar.org/paper/cef330bacf014d60daabbd489647b2006af130ca,"Automatic side-by-side evaluation has emerged as a promising approach to evaluating the quality of responses from large language models (LLMs). However, analyzing the results from this evaluation approach raises scalability and interpretability challenges. In this paper, we present LLM Comparator, a novel visual analytics tool for interactively analyzing results from automatic side-by-side evaluation. The tool supports interactive workflows for users to understand when and why a model performs better or worse than a baseline model, and how the responses from two models are qualitatively different. We iteratively designed and developed the tool by closely working with researchers and engineers at Google. This paper details the user challenges we identified, the design and development of the tool, and an observational study with participants who regularly evaluate their models.",LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models,https://www.semanticscholar.org/paper/f2fe5656f9ada5b487fbebdf63f217cdd548e31e,"Creating software tutorials involves developing accurate code examples and explanatory text that engages and informs the reader. Large Language Models (LLMs) demonstrate a strong capacity to generate both text and code, but their potential to assist tutorial writing is unknown. By interviewing and observing seven experienced writers using OpenAI playground as an exploration environment, we uncover design opportunities for leveraging LLMs in software tutorial writing. Our findings reveal background research, resource creation, and maintaining quality standards as critical areas where LLMs could significantly assist writers. We observe how tutorial writers generated tutorial content while exploring LLMs’ capabilities, formulating prompts, verifying LLM outputs, and reflecting on interaction goals and strategies. Our observation highlights that the unpredictability of LLM outputs and unintuitive interface design contributed to skepticism about LLM’s utility. Informed by these results, we contribute recommendations for designing LLM-based tutorial writing tools to mitigate usability challenges and harness LLMs’ full potential.",Do LLMs Meet the Needs of Software Tutorial Writers? Opportunities and Design Implications,https://www.semanticscholar.org/paper/58ba53930bb32c0361eff35dd2774bfcec2defb6,not novel
"Create an enhanced paper alert system that **improves paper recommendation relevance** by using an **intelligent skimming interface**. This system would highlight key sections of recommended papers and provide contextualized summaries that explain their relevance to the user's research interests. **Usability and diary studies** could be conducted to evaluate the system's effectiveness in helping researchers quickly identify and understand the most pertinent papers, measuring improvements in relevance detection and skimming efficiency.","['Computer Science', 'Human-Computer Interaction', 'Information Retrieval', 'Large Language Models', 'Scholarly Communication']","Scholars need to keep up with an exponentially increasing flood of scientific papers. To aid this challenge, we introduce Scim, a novel intelligent interface that helps experienced researchers skim – or rapidly review – a paper to attain a cursory understanding of its contents. Scim supports the skimming process by highlighting salient paper contents in order to direct a reader’s attention. The system’s highlights are faceted by content type, evenly distributed across a paper, and have a density configurable by readers at both the global and local level. We evaluate Scim with both an in-lab usability study and a longitudinal diary study, revealing how its highlights facilitate the more efficient construction of a conceptualization of a paper. We conclude by discussing design considerations and tensions for the design of future intelligent skimming tools.",Scim: Intelligent Skimming Support for Scientific Papers,https://www.semanticscholar.org/paper/903b3afd316837ffdf46f0cea7ee242d897d7956,"Scholars need to keep up with an exponentially increasing flood of scientific papers. To aid this challenge, we introduce Scim, a novel intelligent interface that helps scholars skim papers to rapidly review and gain a cursory understanding of its contents. Scim supports the skimming process by highlighting salient content within a paper, directing a scholar’s attention. These automatically-extracted highlights are faceted by content type, evenly distributed across a paper, and have a density configurable by scholars. We evaluate Scim with an in-lab usability study and a longitudinal diary study, revealing how its highlights facilitate the more efficient construction of a conceptualization of a paper. Finally, we describe the process of scaling highlights from their conception within Scim, a research prototype, to production on over 521,000 papers within the Semantic Reader, a publicly-available augmented reading interface for scientific papers. We conclude by discussing design considerations and tensions for the design of future skimming tools with augmented intelligence.",Accelerating Scientific Paper Skimming with Augmented Intelligence Through Customizable Faceted Highlights,https://www.semanticscholar.org/paper/937aa9229358b0c19ad3b9d224eb0599e0552986,"Researchers are expected to keep up with an immense literature, yet often find it prohibitively time-consuming to do so. This paper ex-plores how intelligent agents can help scaffold in-situ information seeking across scientific papers. Specifically, we present Scim, an AI-augmented reading interface designed to help researchers skim papers by automatically identifying, classifying, and highlighting salient sentences, organized into rhetorical facets rooted in common information needs. Using Scim as a design probe, we explore the benefits and drawbacks of imperfect AI assistance within an augmented reading interface. We found researchers used Scim in several different ways: from reading primarily in the ‘highlight browser’ (side panel) to making multiple passes through the paper with different facets activated (e.g., focusing solely on objective and novelty in their first pass). From our study, we identify six key design recommendations and avenues for future research in augmented reading interfaces.","Scim: Intelligent Faceted Highlights for Interactive, Multi-Pass Skimming of Scientific Papers",https://www.semanticscholar.org/paper/d1ca07561b24afe8b1bd18dd1c239dbbbd221964,"With the rapid growth of scholarly archives, researchers subscribe to “paper alert’’ systems that periodically provide them with recommendations of recently published papers that are similar to previously collected papers. However, researchers sometimes struggle to make sense of nuanced connections between recommended papers and their own research context, as existing systems only present paper titles and abstracts. To help researchers spot these connections, we present PaperWeaver, an enriched paper alerts system that provides contextualized text descriptions of recommended papers based on user-collected papers. PaperWeaver employs a computational method based on Large Language Models (LLMs) to infer users’ research interests from their collected papers, extract context-specific aspects of papers, and compare recommended and collected papers on these aspects. Our user study (N=15) showed that participants using PaperWeaver were able to better understand the relevance of recommended papers and triage them more confidently when compared to a baseline that presented the related work sections from recommended papers.",PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers,https://www.semanticscholar.org/paper/2cbdad900dda764efa362793765ec12cb5ae66f5,"Scholarly publications are key to the transfer of knowledge from scholars to others. However, research papers are information-dense, and as the volume of the scientific literature grows, the need for new technology to support the reading process grows. In contrast to the process of finding papers, which has been transformed by Internet technology, the experience of reading research papers has changed little in decades. The PDF format for sharing research papers is widely used due to its portability, but it has significant downsides including: static content, poor accessibility for low-vision readers, and difficulty reading on mobile devices. This paper explores the question""Can recent advances in AI and HCI power intelligent, interactive, and accessible reading interfaces -- even for legacy PDFs?""We describe the Semantic Reader Project, a collaborative effort across multiple institutions to explore automatic creation of dynamic reading interfaces for research papers. Through this project, we've developed ten research prototype interfaces and conducted usability studies with more than 300 participants and real-world users showing improved reading experiences for scholars. We've also released a production reading interface for research papers that will incorporate the best features as they mature. We structure this paper around challenges scholars and the public face when reading research papers -- Discovery, Efficiency, Comprehension, Synthesis, and Accessibility -- and present an overview of our progress and remaining open challenges.",The Semantic Reader Project: Augmenting Scholarly Documents through AI-Powered Interactive Reading Interfaces,https://www.semanticscholar.org/paper/096ca3c5da860d41811c741ddc29242d90d1ccea,"Reviewing the literature to understand relevant threads of past work is a critical part of research and vehicle for learning. However, as the scientific literature grows the challenges for users to find and make sense of the many different threads of research grow as well. Previous work has helped scholars to find and group papers with citation information or textual similarity using standalone tools or overview visualizations. Instead, in this work we explore a tool integrated into users’ reading process that helps them with leveraging authors’ existing summarization of threads, typically in introduction or related work sections, in order to situate their own work’s contributions. To explore this we developed a prototype that supports efficient extraction and organization of threads along with supporting evidence as scientists read research articles. The system then recommends further relevant articles based on user-created threads. We evaluate the system in a lab study and find that it helps scientists to follow and curate research threads without breaking out of their flow of reading, collect relevant papers and clips, and discover interesting new articles to further grow threads.",Threddy: An Interactive System for Personalized Thread-based Exploration and Organization of Scientific Literature,https://www.semanticscholar.org/paper/fa1e4d2cc397affeb8816bc98f0d1ef38a5ee8fb,"When reading a scholarly article, inline citations help researchers contextualize the current article and discover relevant prior work. However, it can be challenging to prioritize and make sense of the hundreds of citations encountered during literature reviews. This paper introduces CiteSee, a paper reading tool that leverages a user’s publishing, reading, and saving activities to provide personalized visual augmentations and context around citations. First, CiteSee connects the current paper to familiar contexts by surfacing known citations a user had cited or opened. Second, CiteSee helps users prioritize their exploration by highlighting relevant but unknown citations based on saving and reading history. We conducted a lab study that suggests CiteSee is significantly more effective for paper discovery than three baselines. A field deployment study shows CiteSee helps participants keep track of their explorations and leads to better situational awareness and increased paper discovery via inline citation when conducting real-world literature reviews.",CiteSee: Augmenting Citations in Scientific Papers with Persistent and Personalized Historical Context,https://www.semanticscholar.org/paper/81f7eb73883a559e39a5ab7754c77371488c4c7e,"Scholars who want to research a scientific topic must take time to read, extract meaning, and identify connections across many papers. As scientific literature grows, this becomes increasingly challenging. Meanwhile, authors summarize prior research in papers’ related work sections, though this is scoped to support a single paper. A formative study found that while reading multiple related work paragraphs helps overview a topic, it is hard to navigate overlapping and diverging references and research foci. In this work, we design a system, Relatedly, that scaffolds exploring and reading multiple related work paragraphs on a topic, with features including dynamic re-ranking and highlighting to spotlight unexplored dissimilar information, auto-generated descriptive paragraph headings, and low-lighting of redundant information. From a within-subjects user study (n=15), we found that scholars generate more coherent, insightful, and comprehensive topic outlines using Relatedly compared to a baseline paper list.",Relatedly: Scaffolding Literature Reviews with Existing Related Work Sections,https://www.semanticscholar.org/paper/c388626d1a342339078aaab7acc280efbc4f77fc,"Scientific writing involves retrieving, summarizing, and citing relevant papers, which can be time-consuming processes. Although in many workflows these processes are serially linked, there are opportunities for natural language processing (NLP) to provide end-to-end assistive tools. We propose SciLit, a pipeline that automatically recommends relevant papers, extracts highlights, and suggests a reference sentence as a citation of a paper, taking into consideration the user-provided context and keywords. SciLit efficiently recommends papers from large databases of hundreds of millions of papers using a two-stage pre-fetching and re-ranking literature search system that flexibly deals with addition and removal of a paper database. We provide a convenient user interface that displays the recommended papers as extractive summaries and that offers abstractively-generated citing sentences which are aligned with the provided context and which mention the chosen keyword(s). Our assistive tool for literature discovery and scientific writing is available at https://scilit.vercel.app","SciLit: A Platform for Joint Scientific Literature Discovery, Summarization and Citation Generation",https://www.semanticscholar.org/paper/2c17c9bfed8984188172ef94fdf6c7d78b053a89,"Navigating the vast scientific literature often starts with browsing a paper's abstract. However, when a reader seeks additional information, not present in the abstract, they face a costly cognitive chasm during their dive into the full text. To bridge this gap, we introduce recursively expandable abstracts, a novel interaction paradigm that dynamically expands abstracts by progressively incorporating additional information from the papers' full text. This lightweight interaction allows scholars to specify their information needs by quickly brushing over the abstract or selecting AI-suggested expandable entities. Relevant information is synthesized using a retrieval-augmented generation approach, presented as a fluid, threaded expansion of the abstract, and made efficiently verifiable via attribution to relevant source-passages in the paper. Through a series of user studies, we demonstrate the utility of recursively expandable abstracts and identify future opportunities to support low-effort and just-in-time exploration of long-form information contexts through LLM-powered interactions.",Qlarify: Recursively Expandable Abstracts for Directed Information Retrieval over Scientific Papers,https://www.semanticscholar.org/paper/05a6ef4b442a7cd37f40ef93216f68e461439b95,not novel
"Develop a **context-aware preference alignment system** for LLMs designed **to align language models with user preferences** by adapting the **social force model with human feedback** mechanism. This system will collect and analyze user feedback in real-time, where each interaction adjusts the LLM's output dynamically to better match individual preferences. The system will be evaluated using a **simulator sickness questionnaire** to assess user comfort and satisfaction over prolonged use, ensuring the adjustments lead to a seamless and pleasant user experience.","['Computer Science', 'Human-Computer Interaction', 'Natural Language Processing', 'Large Language Models', 'User Preference Learning', 'Interactive Learning Systems', 'Context-Aware Systems', 'User Feedback Analysis']","While finetuning language models from pairwise preferences has proven remarkably effective, the underspecified nature of natural language presents critical challenges. Direct preference feedback is uninterpretable, difficult to provide where multidimensional criteria may apply, and often inconsistent, either because it is based on incomplete instructions or provided by diverse principals. To address these challenges, we consider the two-step preference modeling procedure that first resolves the under-specification by selecting a context, and then evaluates preference with respect to the chosen context. We decompose reward modeling error according to these two steps, which suggests that supervising context in addition to context-specific preference may be a viable approach to aligning models with diverse human preferences. For this to work, the ability of models to evaluate context-specific preference is critical. To this end, we contribute context-conditioned preference datasets and accompanying experiments that investigate the ability of language models to evaluate context-specific preference. We use our datasets to (1) show that existing preference models benefit from, but fail to fully consider, added context, (2) finetune a context-aware reward model with context-specific performance exceeding that of GPT-4 and Llama 3 70B on tested datasets, and (3) investigate the value of context-aware preference modeling.",Improving Context-Aware Preference Modeling for Language Models,https://www.semanticscholar.org/paper/9ddda210b622e0127a43dd5d7cd02928d449b799,"This paper presents a novel approach to aligning large language models (LLMs) with individual human preferences, sometimes referred to as Reinforcement Learning from \textit{Personalized} Human Feedback (RLPHF). Given stated preferences along multiple dimensions, such as helpfulness, conciseness, or humor, the goal is to create an LLM without re-training that best adheres to this specification. Starting from specialized expert LLMs, each trained for one such particular preference dimension, we propose a black-box method that merges their outputs on a per-token level. We train a lightweight Preference Control Model (PCM) that dynamically translates the preference description and current context into next-token prediction weights. By combining the expert models' outputs at the token level, our approach dynamically generates text that optimizes the given preference. Empirical tests show that our method matches or surpasses existing preference merging techniques, providing a scalable, efficient alternative to fine-tuning LLMs for individual personalization.",Orchestrating LLMs with Different Personalizations,https://www.semanticscholar.org/paper/dd2283b979defe2310941c65e8f097b44289b512,"Although humans inherently have diverse values, current large language model (LLM) alignment methods often assume that aligning LLMs with the general public's preferences is optimal. A major challenge in adopting a more individualized approach to LLM alignment is its lack of scalability, as it involves repeatedly acquiring preference data and training new reward models and LLMs for each individual's preferences. To address these challenges, we propose a new paradigm where users specify what they value most within the system message, steering the LLM's generation behavior to better align with the user's intentions. However, a naive application of such an approach is non-trivial since LLMs are typically trained on a uniform system message (e.g.,""You are a helpful assistant"") which limits their ability to generalize to diverse, unseen system messages. To improve this generalization, we create the Multifaceted Collection, a preference dataset with 192k combinations of values beyond generic helpfulness and harmlessness, spanning 65k user instructions. Using this dataset, we train a 7B LLM called Janus and test it on 921 prompts from 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct) by adding various unseen system messages that reflect user preferences. Janus achieves tie+win rate of 75.2%, 72.4%, and 66.4% against Mistral 7B Instruct v0.2, GPT-3.5 Turbo, and GPT-4, respectively. Unexpectedly, on three benchmarks focused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto v0.1), Janus also outperforms LLaMA 3 8B Instruct by a +4.0%, +0.1%, +3.0% margin, underscoring that training with a vast array of system messages could also enhance alignment to the general public's preference as well. Our code, dataset, benchmark, and models are available at https://github.com/kaistAI/Janus.",Aligning to Thousands of Preferences via System Message Generalization,https://www.semanticscholar.org/paper/2433170286f1adf55670dc68ce73d816e1ccb0be,"The recent surge of versatile large language models (LLMs) largely depends on aligning increasingly capable foundation models with human intentions by preference learning, enhancing LLMs with excellent applicability and effectiveness in a wide range of contexts. Despite the numerous related studies conducted, a perspective on how human preferences are introduced into LLMs remains limited, which may prevent a deeper comprehension of the relationships between human preferences and LLMs as well as the realization of their limitations. In this survey, we review the progress in exploring human preference learning for LLMs from a preference-centered perspective, covering the sources and formats of preference feedback, the modeling and usage of preference signals, as well as the evaluation of the aligned LLMs. We first categorize the human feedback according to data sources and formats. We then summarize techniques for human preferences modeling and compare the advantages and disadvantages of different schools of models. Moreover, we present various preference usage methods sorted by the objectives to utilize human preference signals. Finally, we summarize some prevailing approaches to evaluate LLMs in terms of alignment with human intentions and discuss our outlooks on the human intention alignment for LLMs.",A Survey on Human Preference Learning for Large Language Models,https://www.semanticscholar.org/paper/a5f26555194d50955f6b3fdafb04d4330cb272dc,"Learning from human feedback is a prominent technique to align the output of large language models (LLMs) with human expectations. Reinforcement learning from human feedback (RLHF) leverages human preference signals that are in the form of ranking of response pairs to perform this alignment. However, human preference on LLM outputs can come in much richer forms including natural language, which may provide detailed feedback on strengths and weaknesses of a given response. In this work we investigate data efficiency of modeling human feedback that is in natural language. Specifically, we fine-tune an open-source LLM, e.g., Falcon-40B-Instruct, on a relatively small amount (1000 records or even less) of human feedback in natural language in the form of critiques and revisions of responses. We show that this model is able to improve the quality of responses from even some of the strongest LLMs such as ChatGPT, BARD, and Vicuna, through critique and revision of those responses. For instance, through one iteration of revision of ChatGPT responses, the revised responses have 56.6% win rate over the original ones, and this win rate can be further improved to 65.9% after applying the revision for five iterations.",Data-Efficient Alignment of Large Language Models with Human Feedback Through Natural Language,https://www.semanticscholar.org/paper/0f9995ec08e95bea09d512c59e40d19f0f44d7bb,"Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.",A General Language Assistant as a Laboratory for Alignment,https://www.semanticscholar.org/paper/3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e,"We present ALT (ALignment with Textual feedback), an approach that aligns language models with user preferences expressed in text. We argue that text offers greater expressiveness, enabling users to provide richer feedback than simple comparative preferences and this richer feedback can lead to more efficient and effective alignment. ALT aligns the model by conditioning its generation on the textual feedback. Our method relies solely on language modeling techniques and requires minimal hyper-parameter tuning, though it still presents the main benefits of RL-based alignment algorithms and can effectively learn from textual feedback. We explore the efficacy and efficiency of textual feedback across different tasks such as toxicity reduction, summarization, and dialog response generation. We find that ALT outperforms PPO for the task of toxicity reduction while being able to match its performance on summarization with only 20% of the samples. We also explore how ALT can be used with feedback provided by an existing LLM where we explore an LLM providing constrained and unconstrained textual feedback. We also outline future directions to align models with natural language feedback.",Towards Aligning Language Models with Textual Feedback,https://www.semanticscholar.org/paper/70dcd5bda31413c00696a95a769ea55635c32b70,"The success of AI assistants based on Language Models (LLMs) hinges on Reinforcement Learning from Human Feedback (RLHF) to comprehend and align with user intentions. However, traditional alignment algorithms, such as PPO, are hampered by complex annotation and training requirements. This reliance limits the applicability of RLHF and hinders the development of professional assistants tailored to diverse human preferences. In this work, we introduce \textit{Linear Alignment}, a novel algorithm that aligns language models with human preferences in one single inference step, eliminating the reliance on data annotation and model training. Linear alignment incorporates a new parameterization for policy optimization under divergence constraints, which enables the extraction of optimal policy in a closed-form manner and facilitates the direct estimation of the aligned response. Extensive experiments on both general and personalized preference datasets demonstrate that linear alignment significantly enhances the performance and efficiency of LLM alignment across diverse scenarios. Our code and dataset is published on \url{https://github.com/Wizardcoast/Linear_Alignment.git}.",Linear Alignment: A Closed-form Solution for Aligning Human Preferences without Tuning and Feedback,https://www.semanticscholar.org/paper/67eab08db30e397e400e3b36b3afd7526df83314,"Pre-trained Language Models (LMs) exhibit strong zero-shot and in-context learning capabilities; however, their behaviors are often difficult to control. By utilizing Reinforcement Learning from Human Feedback (RLHF), it is possible to fine-tune unsupervised LMs to follow instructions and produce outputs that reflect human preferences. Despite its benefits, RLHF has been shown to potentially harm a language model's reasoning capabilities and introduce artifacts such as hallucinations where the model may fabricate facts. To address this issue we introduce Direct Preference Heads (DPH), a fine-tuning framework that enables LMs to learn human preference signals through an auxiliary reward head without directly affecting the output distribution of the language modeling head. We perform a theoretical analysis of our objective function and find strong ties to Conservative Direct Preference Optimization (cDPO). Finally we evaluate our models on GLUE, RACE, and the GPT4All evaluation suite and demonstrate that our method produces models which achieve higher scores than those fine-tuned with Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO) alone.",Would I Lie To You? Inference Time Alignment of Language Models using Direct Preference Heads,https://www.semanticscholar.org/paper/60dc6b4e423497a87933437fa094508d1c600704,"A common technique for aligning large language models (LLMs) relies on acquiring human preferences by comparing multiple generations conditioned on a fixed context. This only leverages the pairwise comparisons when the generations are placed in an identical context. However, such conditional rankings often fail to capture the complex and multidimensional aspects of human preferences. In this work, we revisit the traditional paradigm of preference acquisition and propose a new axis that is based on eliciting preferences jointly over the instruction-response pairs. While prior preference optimizations are designed for conditional ranking protocols (e.g., DPO), our proposed preference acquisition protocol introduces DOVE, a new preference optimization objective that upweights the joint probability of the chosen instruction-response pair over the rejected instruction-response pair. Interestingly, we find that the LLM trained with joint instruction-response preference data using DOVE outperforms the LLM trained with DPO by 5.2% and 3.3% win-rate for the summarization and open-ended dialogue datasets, respectively. Our findings reveal that joint preferences over instruction and response pairs can significantly enhance the alignment of LLMs by tapping into a broader spectrum of human preference elicitation. The data and code is available at https://github.com/Hritikbansal/dove.",Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization,https://www.semanticscholar.org/paper/b30e8ef07654aa5f52ad26f6fbb1fa5ba7f9573f,novel
"Develop a system **to improve scientific paper revisions** by using **ethnomethodological conversation analysis** to deeply analyze the interactions between authors and reviewers. This system will not only collect and categorize review comments but also study the conversational context in which these comments are made to understand the implicit expectations and reasoning behind them. By leveraging this nuanced understanding, the system will generate more contextually appropriate and insightful revision suggestions. The effectiveness of the system will be measured through **microanalysis of video corpus**, where recorded interactions between authors revising their papers based on system suggestions and reviewers providing feedback will be analyzed to assess improvements in revision quality and alignment with reviewer expectations.","['Computer Science', 'Natural Language Processing', 'Human-Computer Interaction', 'Ethnomethodology', 'Conversation Analysis', 'Machine Learning', 'Scientific Writing and Editing']","Scientific publications are the primary means to communicate research discoveries, where the writing quality is of crucial importance. However, prior work studying the human editing process in this domain mainly focused on the abstract or introduction sections, resulting in an incomplete picture. In this work, we provide a complete computational framework for studying text revision in scientific writing. We first introduce arXivEdits, a new annotated corpus of 751 full papers from arXiv with gold sentence alignment across their multiple versions of revision, as well as fine-grained span-level edits and their underlying intentions for 1,000 sentence pairs. It supports our data-driven analysis to unveil the common strategies practiced by researchers for revising their papers. To scale up the analysis, we also develop automatic methods to extract revision at document-, sentence-, and word-levels. A neural CRF sentence alignment model trained on our corpus achieves 93.8 F1, enabling the reliable matching of sentences between different versions. We formulate the edit extraction task as a span alignment problem, and our proposed method extracts more fine-grained and explainable edits, compared to the commonly used diff algorithm. An intention classifier trained on our dataset achieves 78.9 F1 on the fine-grained intent classification task. Our data and system are released at tiny.one/arxivedits.",arXivEdits: Understanding the Human Revision Process in Scientific Writing,https://www.semanticscholar.org/paper/e53b0d9c061a1a108d87d79826c47cf77bac85d6,"Wikis have become a popular online collaboration platform. Their open nature can, and indeed does, lead to a large number of editors of their articles, who create a large number of revisions. These editors make various types of edits on an article, from minor ones such as spelling correction and text formatting, to major revisions such as new content introduction, whole article re-structuring, etc. Given the enormous number of revisions, it is difficult to identify the type of contributions made in these revisions through human observation alone. Moreover, different types of edits imply different edit significance. A revision that introduces new content is arguably more significant than a revision making a few spelling corrections. By taking edit types into account, better measurements of edit significance can be produced. This paper proposes a method for categorizing and presenting edits in an intuitive way and with a flexible measure of significance of each individual editor's contributions.",What did they do? Deriving high-level edit histories in Wikis,https://www.semanticscholar.org/paper/404062ad4b4b38341517276357d6539fa591c6c7,"Despite advancement in collaborative writing tools, the track changes capability in modern editors remains limited to highlighting syntactic changes, with authors still required to manually read through each of the revisions. We envision a collaborative authoring system where an author could accept all minor edits first and then focus on the substantial changes. To support this, we define the task of significant revision identification as the task of identifying the revisions between two versions of a text according to one of four categories, i.e. formal, meaning preserving, micro- and macro-structure. Micro-structure change corresponds to minor meaning change while macro-structure change corresponds to major meaning change. Our main contribution is to define a computational approach to this task, by framing the task as bi-directional entailment between the original and revised sentences. An existing recognition of textual entailment (RTE) system is applied to evaluate whether the revised texts entails. We evaluate the approach through a novel corpus consisting of multiple versions of drafts of academic papers written by multiple authors, which were annotated with the four revision types by both authors and non-authors of the papers. The proposed bi-directional textual entailment approach performs better than baseline edit distance approaches, which is similar to the current track changes capability built into most word processors.",Characterizing Text Revisions to Better Support Collaborative,https://www.semanticscholar.org/paper/7c311d7918800fc842dd50a46e6e9a8df86d6424,"Peer review constitutes a core component of scholarly publishing; yet it demands substantial expertise and training, and is susceptible to errors and biases. Various applications of NLP for peer reviewing assistance aim to support reviewers in this complex process, but the lack of clearly licensed datasets and multi-domain corpora prevent the systematic study of NLP for peer review. To remedy this, we introduce NLPeer– the first ethically sourced multidomain corpus of more than 5k papers and 11k review reports from five different venues. In addition to the new datasets of paper drafts, camera-ready versions and peer reviews from the NLP community, we establish a unified data representation and augment previous peer review datasets to include parsed and structured paper representations, rich metadata and versioning information. We complement our resource with implementations and analysis of three reviewing assistance tasks, including a novel guided skimming task.Our work paves the path towards systematic, multi-faceted, evidence-based study of peer review in NLP and beyond. The data and code are publicly available.",NLPeer: A Unified Resource for the Computational Study of Peer Review,https://www.semanticscholar.org/paper/54a316ecfb97352e55c5e85c06ccf7d013c4993b,"Writing is, by nature, a strategic, adaptive, and, more importantly, an iterative process. A crucial part of writing is editing and revising the text. Previous works on text revision have focused on defining edit intention taxonomies within a single domain or developing computational models with a single level of edit granularity, such as sentence-level edits, which differ from human’s revision cycles. This work describes IteraTeR: the first large-scale, multi-domain, edit-intention annotated corpus of iteratively revised text. In particular, IteraTeR is collected based on a new framework to comprehensively model the iterative text revisions that generalizes to a variety of domains, edit intentions, revision depths, and granularities. When we incorporate our annotated edit intentions, both generative and action-based text revision models significantly improve automatic evaluations. Through our work, we better understand the text revision process, making vital connections between edit intentions and writing quality, enabling the creation of diverse corpora to support computational modeling of iterative text revisions.",Understanding Iterative Revision from Human-Written Text,https://www.semanticscholar.org/paper/340b8d8f710459d809a3da1868cd3e011aeded67,"We introduce the task of automatically revising scientific papers based on peer feedback and release ARIES, a dataset of review comments and their corresponding paper edits. The data is drawn from real reviewer-author interactions from computer science, and we provide labels linking each reviewer comment to the specific paper edits made by the author in response. We automatically create a high-precision silver training set, as well as an expert-labeled test set that shows high inter-annotator agreement. In experiments with 10 models covering the state of the art, we find that they struggle even to identify which edits correspond to a comment -- especially when the relationship between the edit and the comment is indirect and requires reasoning to uncover. We also extensively analyze GPT-4's ability to generate edits given a comment and the original paper. We find that it often succeeds on a superficial level, but tends to rigidly follow the wording of the feedback rather than the underlying intent, and lacks technical details compared to human-written edits.",ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews,https://www.semanticscholar.org/paper/fd8c64d0b912795e1cefc0aba4c6d90499132755,"Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR machine learning conference (1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature journals, 39.23% for ICLR) is comparable to the overlap between two human reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The overlap between GPT-4 and human reviewers is larger for the weaker papers. We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half (57.4%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations.",Can large language models provide useful feedback on research papers? A large-scale empirical analysis,https://www.semanticscholar.org/paper/f2209eb5ac6747319a29b87dedabb97770be3243,"Studies of writing revisions rarely focus on revision quality. To address this issue, we introduce a corpus of between-draft revisions of student argumentative essays, annotated as to whether each revision improves essay quality. We demonstrate a potential usage of our annotations by developing a machine learning model to predict revision improvement. With the goal of expanding training data, we also extract revisions from a dataset edited by expert proofreaders. Our results indicate that blending expert and non-expert revisions increases model performance, with expert data particularly important for predicting low-quality revisions.",Annotation and Classification of Sentence-level Revision Improvement,https://www.semanticscholar.org/paper/7ee7e580798e9177dc8ff9bb94c895cd3d4ab780,"To assist human review process, we build a novel ReviewRobot to automatically assign a review score and write comments for multiple categories such as novelty and meaningful comparison. A good review needs to be knowledgeable, namely that the comments should be constructive and informative to help improve the paper; and explainable by providing detailed evidence. ReviewRobot achieves these goals via three steps: (1) We perform domain-specific Information Extraction to construct a knowledge graph (KG) from the target paper under review, a related work KG from the papers cited by the target paper, and a background KG from a large collection of previous papers in the domain. (2) By comparing these three KGs, we predict a review score and detailed structured knowledge as evidence for each review category. (3) We carefully select and generalize human review sentences into templates, and apply these templates to transform the review scores and evidence into natural language comments. Experimental results show that our review score predictor reaches 71.4%-100% accuracy. Human assessment by domain experts shows that 41.7%-70.5% of the comments generated by ReviewRobot are valid and constructive, and better than human-written ones for 20% of the time. Thus, ReviewRobot can serve as an assistant for paper reviewers, program chairs and authors.",ReviewRobot: Explainable Paper Review Generation based on Knowledge Synthesis,https://www.semanticscholar.org/paper/e7e46bc5ef80187084d2d2c626ca95e68ee6e74b,"Abstract Peer review is a key component of the publishing process in most fields of science. Increasing submission rates put a strain on reviewing quality and efficiency, motivating the development of applications to support the reviewing and editorial work. While existing NLP studies focus on the analysis of individual texts, editorial assistance often requires modeling interactions between pairs of texts—yet general frameworks and datasets to support this scenario are missing. Relationships between texts are the core object of the intertextuality theory—a family of approaches in literary studies not yet operationalized in NLP. Inspired by prior theoretical work, we propose the first intertextual model of text-based collaboration, which encompasses three major phenomena that make up a full iteration of the review–revise–and–resubmit cycle: pragmatic tagging, linking, and long-document version alignment. While peer review is used across the fields of science and publication formats, existing datasets solely focus on conference-style review in computer science. Addressing this, we instantiate our proposed model in the first annotated multidomain corpus in journal-style post-publication open peer review, and provide detailed insights into the practical aspects of intertextual annotation. Our resource is a major step toward multidomain, fine-grained applications of NLP in editorial support for peer review, and our intertextual framework paves the path for general-purpose modeling of text-based collaboration. We make our corpus, detailed annotation guidelines, and accompanying code publicly available.1",Revise and Resubmit: An Intertextual Model of Text-based Collaboration in Peer Review,https://www.semanticscholar.org/paper/35a7f396db1435efc6960064a875328f65082062,novel
"Design a gamified crowdsourcing application that leverages hybrid game elements to engage adolescents in public health data collection and analysis. The application will use game mechanics to incentivize participation and ensure data accuracy, contributing to real-time public health monitoring.",,"Every month at Google Campus in London, dozens of software developers, clinicians, behavioural scientists and investors get together to discuss new strategies to influence health behaviours. The collective aim of these networking events is to develop digital ‘games with purpose’ that can improve health by integrating software design and game mechanics with public health theory and behavioural insights. Gamification is a purposely-broad umbrella term used to encompass the process of using ‘gaming’ elements to motivate and engage people in non-game contexts.1 Enhanced opportunities now exist to deliver behaviour change interventions through game platforms on new smartphone devices. 
 
Defying traditional stereotypes, people across demographic boundaries now play video games on a wide range of digital devices.2 Whilst such games continue to be primarily used for entertainment purposes, there is increasing interest in their potential to influence positive changes in health behaviours.3 This has been encouraged by the finding that rather than spending hours being sedentary and chasing intangible outcomes, players of active video games (e.g. Nintendo Wii Fit) are motivated to exert themselves to achieve activity goals through game mechanics.4,5 
 
Whilst still in its infancy, we predict that gamification will become an increasingly familiar concept in healthcare as a consequence of two trends. The first builds on the consumer's appetite for new smartphone devices that provide games designers with a wider audience to target and more attractive tools to use in designing interactive health interventions. The second factor is the enthusiasm and willingness of developers to incorporate the latest behavioural insights into electronic interventions.",‘Gamification’: Influencing health behaviours with games,https://www.semanticscholar.org/paper/9a4d3da2afbfb9359e8538837953aec17fab3eaa,"
 BACKGROUND
 Games, when used as interventional tools, can influence behavior change by incentivizing, reinforcing, educating, providing feedback loops, prompting, persuading, or providing meaning, fun, and community. However, not all game elements will appeal to all consumers equally, and different elements might work for different people and in different contexts.
 
 
 OBJECTIVE
 The aim of this study was to conduct a realist review of tabletop games targeting behavior change and to propose a framework for designing effective behavior change games.
 
 
 METHODS
 A realist review was conducted to inform program theory in the development of tabletop games for health behavior change. The context, mechanisms used to change behavior, and outcomes of included studies were reviewed through a realist lens.
 
 
 RESULTS
 Thirty-one papers met the eligibility criteria and were included in the review. Several design methods were identified that enhanced the efficacy of the games to change behavior. These included design by local teams, pilot testing, clearly defined targets of behavior change, conscious attention to all aspects of game design, including game mechanics, dynamics, aesthetics, and the elicitation of emotions. Delivery with other mediums, leveraging behavioral insights, prior training for delivery, and repeated play were also important. Some design elements that were found to reduce efficacy included limited replayability or lack of fun for immersive engagement.
 
 
 CONCLUSIONS
 Game designers need to consider all aspects of the context and the mechanisms to achieve the desired behavior change outcomes. Careful design thinking should include consideration of the game mechanics, dynamics, aesthetics, emotions, and contexts of the game and the players. People who know the players and the contexts well should design the games or have significant input. Testing in real-world settings is likely to lead to better outcomes. Careful selection and purposeful design of the behavior change mechanisms at play is essential. Fun and enjoyment of the player should be considered, as without engagement, there will be no desired intervention effect.
",Tabletop Board Game Elements and Gamification Interventions for Health Behavior Change: Realist Review and Proposal of a Game Design Framework (Preprint),https://www.semanticscholar.org/paper/8d052de2d798755f62c4fef0e74ad1098f340d95,"Effective chronic disease management is essential to improve positive health outcomes, and incentive strategies are useful in promoting self-care with longevity. Gamification, applied with mHealth (mobile health) applications, has the potential to better facilitate patient self-management. This review article addresses a knowledge gap around the effective use of gamification design principles, or mechanics, in developing mHealth applications. Badges, leaderboards, points and levels, challenges and quests, social engagement loops, and onboarding are mechanics that comprise gamification. These mechanics are defined and explained from a design and development perspective. Health and fitness applications with gamification mechanics include: bant which uses points, levels, and social engagement, mySugr which uses challenges and quests, RunKeeper which uses leaderboards as well as social engagement loops and onboarding, Fitocracy which uses badges, and Mango Health, which uses points and levels. Specific design considerations are explored, an example of the efficacy of a gamified mHealth implementation in facilitating improved self-management is provided, limitations to this work are discussed, a link between the principles of gaming and gamification in health and wellness technologies is provided, and suggestions for future work are made. We conclude that gamification could be leveraged in developing applications with the potential to better facilitate self-management in persons with chronic conditions.",A game plan: Gamification design principles in mHealth applications for chronic disease management,https://www.semanticscholar.org/paper/f9be2234cbcb02cadb45c2994463cfecfab63a0c,"Background Smoking is recognized as the largest, single, preventable cause of death and disease in the developed world. While the majority of smokers report wanting to quit, and many try each year, smokers find it difficult to maintain long-term abstinence. Behavioral support, such as education, advice, goal-setting, and encouragement, is known to be beneficial in improving the likelihood of succeeding in a quit attempt, but it remains difficult to effectively deliver this behavioral support and keep the patient engaged with the process for a sufficient duration. In an attempt to solve this, there have been numerous mobile apps developed, yet engagement and retention have remained key challenges that limit the potential effectiveness of these interventions. Video games have been clearly linked with the effective delivery of health interventions, due to their capacity to increase motivation and engagement of players. Objective The objective of this study is to describe the design and development of a smartphone app that is theory-driven, and which incorporates gaming characteristics in order to promote engagement with content, and thereby help smokers to quit. Methods Game design and development was informed by a taxonomy of motivational affordances for meaningful gamified and persuasive technologies. This taxonomy describes a set of design components that is grounded in well-established psychological theories on motivation. Results This paper reports on the design and development process of Quittr, a mobile app, describing how game design principles, game mechanics, and game elements can be used to embed education and support content, such that the app actually requires the user to access and engage with relevant educational content. The next stage of this research is to conduct a randomized controlled trial to determine whether the additional incentivization game features offer any value in terms of the key metrics of engagement–how much content users are consuming, how many days users are persisting with using the app, and what proportion of users successfully abstain from smoking for 28 days, based on user-reported data and verified against a biochemical baseline using cotinine tests. Conclusions We describe a novel, and theoretically-informed mobile app design approach that has a broad range of potential applications. By using the virtual currency approach, we remove the need for the game to comprehensively integrate the healthy activity as part of its actual play mechanics. This opens up the potential for a wide variety of health problems to be tackled through games where no obvious play mechanic presents itself. The implications of this app are that similar approaches may be of benefit in areas such as managing chronic conditions (diabetes, heart disease, etc), treating substance abuse (alcohol, illicit drugs, etc), diet and exercise, eating disorders (anorexia, bulimia, and binge eating), and various phobias.",Quittr: The Design of a Video Game to Support Smoking Cessation,https://www.semanticscholar.org/paper/b353f1c4509bb0a2fc36bcf0c32a9f538eed4e09,"This paper presents a list of principles that could be used to conceptualize games for behavior change. These principles are derived from lessons learned after teaching two design-centered courses around Gaming and Narrative Technologies for Health Behavior Change. Course sessions were designed to create many rapid prototypes based on specific topics from behavior change theory coupled with iterative human-centered and games design techniques. The design task was composed of two broad goals: 1) designing efficacious technologies, with an emphasis on short-term behavior change and 2) using metaphors, dramatic arcs and game dynamics as vehicles for increased engagement and long-term sustained change. Some example prototypes resulting from this design approach are presented. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. CHI’13, April 27 – May 2, 2013, Paris, France. Copyright 2012 ACM 978-1-4503-1952-2...$10.00. Pablo Paredes Berkeley Institute of Design",Design Principles for the Conceptualization of Games for Health Behavior Change,https://www.semanticscholar.org/paper/1458ce36372b7e05180ea09f4f59a6651df96517,"Background Games, when used as interventional tools, can influence behavior change by incentivizing, reinforcing, educating, providing feedback loops, prompting, persuading, or providing meaning, fun, and community. However, not all game elements will appeal to all consumers equally, and different elements might work for different people and in different contexts. Objective The aim of this study was to conduct a realist review of tabletop games targeting behavior change and to propose a framework for designing effective behavior change games. Methods A realist review was conducted to inform program theory in the development of tabletop games for health behavior change. The context, mechanisms used to change behavior, and outcomes of included studies were reviewed through a realist lens. Results Thirty-one papers met the eligibility criteria and were included in the review. Several design methods were identified that enhanced the efficacy of the games to change behavior. These included design by local teams, pilot testing, clearly defined targets of behavior change, conscious attention to all aspects of game design, including game mechanics, dynamics, aesthetics, and the elicitation of emotions. Delivery with other mediums, leveraging behavioral insights, prior training for delivery, and repeated play were also important. Some design elements that were found to reduce efficacy included limited replayability or lack of fun for immersive engagement. Conclusions Game designers need to consider all aspects of the context and the mechanisms to achieve the desired behavior change outcomes. Careful design thinking should include consideration of the game mechanics, dynamics, aesthetics, emotions, and contexts of the game and the players. People who know the players and the contexts well should design the games or have significant input. Testing in real-world settings is likely to lead to better outcomes. Careful selection and purposeful design of the behavior change mechanisms at play is essential. Fun and enjoyment of the player should be considered, as without engagement, there will be no desired intervention effect.",Tabletop Board Game Elements and Gamification Interventions for Health Behavior Change: Realist Review and Proposal of a Game Design Framework,https://www.semanticscholar.org/paper/5c3fb96b75a79b7d15ae568e75a9f4e51925f7cf,"Games for health (G4H) aim to improve health outcomes and encourage behavior change. While existing theoretical frameworks describe features of both games and health interventions, there has been limited systematic investigation into how disciplinary and interdisciplinary stakeholders understand design features in G4H. We recruited 18 experts from the fields of game design, behavioral health, and games for health, and prompted them with 16 sample games. Applying methods including open card sorting and triading, we elicited themes and features (e.g., real-world interaction, game mechanics) around G4H. We found evidence of conceptual differences suggesting that a G4H perspective is not simply the sum of game and health perspectives. At the same time, we found evidence of convergence in stakeholder views, including areas where game experts provided insights about health and vice versa. We discuss how this work can be applied to provide conceptual tools, improve the G4H design process, and guide approaches to encoding G4H-related data for large-scale empirical analysis.",Design Features in Games for Health: Disciplinary and Interdisciplinary Expert Perspectives,https://www.semanticscholar.org/paper/b34d382158dd8bacf3cadd12d29e4f11a7278601,"games can be effective tools for motivating healthy behaviors and/or attitudes, and so recent years have witnessed an increasing number of persuasive games. However, most games adopt a one-size-fits-all approach to persuasion in their design. Studies on gameplay and player motivation have shown that treating gamers as a monolithic group is a bad design approach because a motivational approach that works for one individual may actually demotivate the desired behavior in others. To correct this problem, we conducted a large-scale study on 1108 gamers, which examined the persuasiveness of ten Persuasive Technology (PT) strategies, and the receptiveness of seven gamer types identified by BrianHex to the strategies most commonly used in PT design. We developed models showing the receptiveness of the gamer types to the ten strategies and created persuasive profiles, which are lists of strategies that can be employed to motivate behavior for each gamer type. Although we studied and created our models using ten strategies, in this paper, we report results of five strategies.",Selecting Effective Strategies for Tailoring Persuasive Health Games to Gamer Types,https://www.semanticscholar.org/paper/5517617540ba1501fd34549ec3d72488f4ea4735,"This chapter aims to provide an overall picture of the applications of electronic games for various health-related purposes, particularly for health education, health risk prevention, behavioral intervention, and disease self-management. We first summarize the electronic games for health that have been empirically tested by researchers in the past 20 years. Games that have not yet been evaluated but are promising and noteworthy are also included. These games are categorized based on their specific health-related functions (i.e., prevention, self-management, medical training, etc). Second, we synthesize the key features of electronic games that make them promising to be used for health-related purposes. Finally, implications of using electronic games for health-related purposes and future direction for research in this area are discussed. Game researchers, health providers, game designers, and potential game consumers will all find informative content in this chapter.",An Overview of Using Electronic Games for Health Purposes,https://www.semanticscholar.org/paper/26d44686d23f867a736f0fccd03bee9794b71d4e,"The application of gaming as a method to change health behavior carries with it the burden to impact the consequences of morbidity and mortality. No-where, then, is the contention of games having a “serious” purpose more relevant than in the domain of serious games for health. Serious games are gaining profile as a potential strategy to educate the public about health in new and novel ways. Computer games represent an emerging approach to the continued research and development of health education and health promotion programs in the service of national health objectives. Research has indicated that many evidence-based health educa-abstract Serious games are gaining profile as a novel strategy to impact health behavior change in the service of national health objectives. Research has indicated that many evidence-based programs are effective because they are grounded in behavioral and motivational theories and models such as the PRECEDE model, the Health Belief Model, Social Cognitive Theory, the Theory of Reasoned Action, the Transtheoretical Model, Attribution Theory, and the ARCS model. Such theories assist in understanding health behavior problems, developing salient interventions, and evaluating their effectiveness. It follows, therefore, that serious games can be made optimally effective in changing health behavior if they are also informed by these theories. A successful intervention development framework (Intervention Mapping) provides a means to enable game developers to use theory to inform the design of effective games for health. This chapter describes useful theories and models for health game design, introduces the intervention mapping process, and describes a case study of a theory-and empirically-based serious health game intervention that has used these approaches and has been rigorously evaluated.",Application of behavioral theory in computer game design for health behavior change,https://www.semanticscholar.org/paper/42de0a0f3d8eb5770dbd65d3df5ca37c483a274e,novel
"Develop a system that uses a **faceted author representation** to **evaluate scientific contributions**. This system would analyze author personas and their publication relations to discern the impact and novelty of their work. For instance, by comparing the contributions of authors with diverse citation profiles and publication venues, the system can identify unique contributions that might be overlooked by traditional metrics. The **case study comparison** would involve applying this system to various established and emerging fields to highlight its effectiveness in recognizing innovative research.","['Computer Science', 'Information Retrieval', 'Recommender Systems', 'Scientometrics', 'Human-Computer Interaction']","Isolated silos of scientific research and the growing challenge of information overload limit awareness across the literature and hinder innovation. Algorithmic curation and recommendation, which often prioritize relevance, can further reinforce these informational “filter bubbles.” In response, we describe Bridger, a system for facilitating discovery of scholars and their work. We construct a faceted representation of authors with information gleaned from their papers and inferred author personas, and use it to develop an approach that locates commonalities and contrasts between scientists to balance relevance and novelty. In studies with computer science researchers, this approach helps users discover authors considered useful for generating novel research directions. We also demonstrate an approach for displaying information about authors, boosting the ability to understand the work of new, unfamiliar scholars. Our analysis reveals that Bridger connects authors who have different citation profiles and publish in different venues, raising the prospect of bridging diverse scientific communities.",Bursting Scientific Filter Bubbles: Boosting Innovation via Novel Author Discovery,https://www.semanticscholar.org/paper/bea1c05b0584a4463cbfbcf4917d6afacaef0bde,"Scientific silos can hinder innovation. These information “filter bubbles” and the growing challenge of information overload limit awareness across the literature, making it difficult to keep track of even narrow areas of interest, let alone discover new ones. Al-gorithmic curation and recommendation, which often prioritize relevance, can further reinforce these bubbles. In response, we describe Bridger, a system for facilitating discovery of scholars and their work, to explore design tradeoffs among relevant and novel recommendations. We construct a faceted representation of authors using information extracted from their papers and inferred personas. We explore approaches both for recommending new content and for displaying it in a manner that helps researchers to understand the work of authors who they are unfamiliar with. In studies with computer science researchers, our approach substantially improves users’ abilities to do so. We develop an approach that locates commonalities and contrasts between scientists—retrieving partially similar authors, rather than aiming for strict similarity. We find this approach helps users discover authors useful for generating novel research ideas of relevance to their work, at a higher rate than a state-of-art neural model. Our analysis reveals that Bridger connects authors who have different citation profiles, publish in different venues, and are more distant in social co-authorship networks, raising the prospect of bridging diverse communities and facilitating discovery.",Bridger: Toward Bursting Scientific Filter Bubbles and Boosting Innovation via Novel Author Discovery,https://www.semanticscholar.org/paper/4d02f1a1b99a21665ef94bcdec733a3f8f4056b4,"In order to help scholars understand and follow a research topic, significant research has been devoted to creating systems that help scholars discover relevant papers and authors. Recent approaches have shown the usefulness of highlighting relevant authors while scholars engage in paper discovery. However, these systems do not capture and utilize users’ evolving knowledge of authors. We reflect on the design space and introduce ComLittee, a literature discovery system that supports author-centric exploration. In contrast to paper-centric interaction in prior systems, ComLittee’s author-centric interaction supports curating research threads from individual authors, finding new authors and papers using combined signals from a paper recommender and the curated authors’ authorship graphs, and understanding them in the context of those signals. In a within-subjects experiment that compares to a paper-centric discovery system with author-highlighting, we demonstrate how ComLittee improves author and paper discovery.",ComLittee: Literature Discovery with Personal Elected Author Committees,https://www.semanticscholar.org/paper/7f95d982f8ed3189d84577f1fdf07f93c99423f2,"AMiner is the second generation of the ArnetMiner system. We focus on developing author-centric analytic and mining tools for gaining a deep understanding of the large and heterogeneous networks formed by authors, papers, venues, and knowledge concepts. One fundamental goal is how to extract and integrate semantics from different sources. We have developed algorithms to automatically extract researchers' profiles from the Web and re- solve the name ambiguity problem, and connect different professional networks. We also developed methodologies to incorporate knowledge from the Wikipedia and other sources into the system to bridge the gap between network science and the web mining research. In this talk, I will focus on answering two fundamental questions for author-centric network analysis: who is who? and who are similar to each other? The system has been in operation since 2006 and has collected more than 100,000,000 author profiles, 100,000,000 publication papers, and 7,800,000 knowledge concepts. It has been widely used for collaboration recommendation, similarity analysis, and community evolution.",AMiner: Mining Deep Knowledge from Big Scholar Data,https://www.semanticscholar.org/paper/169edf0ad0b5e3ab31df4481a9069cbfdb774423,"Rexplore leverages novel solutions in data mining, semantic technologies and visual analytics, and provides an innovative environment for exploring and making sense of scholarly data. Rexplore allows users: 1) to detect and make sense of important trends in research; 2) to identify a variety of interesting relations between researchers, beyond the standard co-authorship relations provided by most other systems; 3) to perform fine-grained expert search with respect to detailed multi-dimensional parameters; 4) to detect and characterize the dynamics of interesting communities of researchers, identified on the basis of shared research interests and scientific trajectories; 5) to analyse research performance at different levels of abstraction, including individual researchers, organizations, countries, and research communities.",Online The Open University ’ s repository of research publications and other research outputs Understanding research dynamics,https://www.semanticscholar.org/paper/cfe3906ad90ef852a29c60aa948f0b261f8da416,"The surge in scientific publications challenges the use of publication counts as a measure of scientific progress, requiring alternative metrics that emphasize the quality and novelty of scientific contributions rather than sheer quantity. This paper proposes the use of Relaxed Word Mover's Distance (RWMD), a semantic text similarity measure, to evaluate the novelty of scientific papers. We hypothesize that RWMD can more effectively gauge the growth of scientific knowledge. To test such an assumption, we apply RWMD to evaluate seminal papers, with Hirsch's H-Index paper as a primary case study. We compare RWMD results across three groups: 1) H-Index-related papers, 2) scientometric studies, and 3) unrelated papers, aiming to discern redundant literature and hype from genuine innovations. Findings suggest that emphasizing knowledge claims offers a deeper insight into scientific contributions, marking RWMD as a promising alternative method to traditional citation metrics, thus better tracking significant scientific breakthroughs.",Decoding Knowledge Claims: The Evaluation of Scientific Publication Contributions through Semantic Analysis,https://www.semanticscholar.org/paper/0636f9f53b58e5ff8c6dcf0dd6c8019129800e0d,"Understanding the evolution of scholarly impact is essential for many real-life decision-making processes in academia, such as research planning, frontier exploration, and award selection. Popular platforms like Google Scholar and Web of Science rely on numerical indicators that are too abstract to convey the context and content of scientific impact, while most existing visualization approaches on mapping science do not consider the presentation of individual scholars' impact evolution using curated self-citation data. This paper builds on our previous work and proposes an integrated pipeline to visualize a scholar's impact evolution from multiple topic facets. A novel 3D prism-shaped visual metaphor is introduced as the overview of a scholar's impact, whilst their scientific evolution on each topic is displayed in a more structured manner. Additional designs by topic chord diagram, streamgraph visualization, and inter-topic flow map, optimized by an elaborate layout algorithm, assist in perceiving the scholar's scientific evolution across topics. A new six-degree-impact glyph metaphor highlights key interdisciplinary works driving the evolution. The proposed visualization methods are evaluated through case studies analyzing the careers of prestigious Turing award laureates and a major visualization venue.",GeneticPrism: Multifaceted Visualization of Scientific Impact Evolutions,https://www.semanticscholar.org/paper/8e62faec9f93c94e74a4bf2e2ed1438eab05de90,"Finding potential collaborators has become a challenge due to the growing number of scientists in organizations such as universities, research institutes, or companies. Collaboration Recommendation Systems (CRSs) have been developed to help researchers identify possible collaboration partners, but they often rely on citation graphs or paper abstracts which may not be readily available in organizational databases or online sources. However, scientific publication titles provide consistent bibliometric data that can provide insights into research areas. TOMOSCO is a topic modelling framework that uses transformer-based methods to extract research area information from small amounts of text, such as publication titles or brief project descriptions. TOMOSCO can classify, cluster, and match research topics across different disciplines, uncovering relationships among scientists and suggesting potential interdisciplinary collaborations. In experiments, TOMOSCO was able to identify existing collaborations with over 90% accuracy based solely on publication titles and propose new collaborations based on previously unseen publications and project descriptions.",Exploiting Topic Modelling for the Identification of Untapped Scientific Collaborations,https://www.semanticscholar.org/paper/ed12a2ed110c00c70a57b78fc476fda98b1231ca,"Understanding the intellectual landscape of scientific communities and their collaborations has become an indispensable part of research per se. In this regard, measuring similarities among scientific documents can help researchers to identify groups with similar interests as a basis for strengthening collaboration and university-industry linkages. To this end, we intend to evaluate the performance of hybrid crowd-computing methods in measuring the similarity between document pairs by comparing the results achieved by crowds and artificial intelligence (AI) algorithms. That said, in this paper we designed two types of experiments to illustrate some issues in calculating how similar an automatic solution is to a given ground truth. In the first type of experiments, we created a crowdsourcing campaign consisting of four human intelligence tasks (HITs) in which the participants had to indicate whether or not a set of papers belonged to the same author. The second type involves a set of natural language processing (NLP) processes in which we used the TF-IDF measure and the Bidirectional Encoder Representation from Transformers (BERT) model. The results of the two types of experiments carried out in this study provide preliminary insight into detecting major contributions from human-AI cooperation at similarity calculation in order to achieve better decision support. We believe that in this case decision makers can be better informed about potential collaborators based on content-based insights enhanced by hybrid human-AI mechanisms.",Towards a Human-AI Hybrid Framework for Inter-Researcher Similarity Detection,https://www.semanticscholar.org/paper/1fb3f432e8a30dbfbbec65241ba829c80de604f0,"Serendipity occurs when one finds an interesting discovery while searching for something else. While search engines seek to report work relevant to a targeted query, recommendation engines are particularly well-suited for serendipitous recommendations as such processes do not need to fulfill a targeted query. Junior researchers can use such an engine to broaden their horizon and learn new areas, while senior researchers can discover interdisciplinary frontiers to apply integrative research. We adapt a state-of-the-art scholarly paper recommendation system's user profile construction to make use of information drawn from 1) dissimilar users and 2) co-authors to specifically target serendipitous recommendation.",Serendipitous recommendation for scholarly papers considering relations among researchers,https://www.semanticscholar.org/paper/a7030fe66a8652ac91f1279b4b73d8b413dbdf22,novel
"Develop a **novel toolkit** that integrates **gaze and mouse tracking** to enhance the process **to support prompt engineering and hypothesis testing** for LLMs. The system will track users’ gaze and mouse movements to provide real-time feedback on which parts of the prompt and model responses attract the most attention. This feedback will be used to iteratively refine prompts by highlighting areas of interest or confusion. The effectiveness of this toolkit will be evaluated through **in-lab and interview studies**, focusing on how gaze and mouse tracking data improve the quality and efficiency of prompt engineering and hypothesis testing.","['Computer Science', 'Human-Computer Interaction', 'Natural Language Processing', 'Large Language Models', 'Prompt Engineering', 'User Experience Research', 'Gaze Tracking', 'Mouse Tracking']","Evaluating outputs of large language models (LLMs) is challenging, requiring making—and making sense of—many responses. Yet tools that go beyond basic prompting tend to require knowledge of programming APIs, focus on narrow domains, or are closed-source. We present ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs. ChainForge provides a graphical interface for comparison of responses across models and prompt variations. Our system was designed to support three tasks: model selection, prompt template design, and hypothesis testing (e.g., auditing). We released ChainForge early in its development and iterated on its design with academics and online users. Through in-lab and interview studies, we find that a range of people could use ChainForge to investigate hypotheses that matter to them, including in real-world settings. We identify three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.",ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing,https://www.semanticscholar.org/paper/2ed64d90670177bf58cdce6bda04a48a8731a18f,"Large Language Models (LLMs) have gained widespread popularity due to their ability to perform ad-hoc Natural Language Processing (NLP) tasks with a simple natural language prompt. Part of the appeal for LLMs is their approachability to the general public, including individuals with no prior technical experience in NLP techniques. However, natural language prompts can vary significantly in terms of their linguistic structure, context, and other semantics. Modifying one or more of these aspects can result in significant differences in task performance. Non-expert users may find it challenging to identify the changes needed to improve a prompt, especially when they lack domain-specific knowledge and lack appropriate feedback. To address this challenge, we present PromptAid, a visual analytics system designed to interactively create, refine, and test prompts through exploration, perturbation, testing, and iteration. PromptAid uses multiple, coordinated visualizations which allow users to improve prompts by using the three strategies: keyword perturbations, paraphrasing perturbations, and obtaining the best set of in-context few-shot examples. PromptAid was designed through an iterative prototyping process involving NLP experts and was evaluated through quantitative and qualitative assessments for LLMs. Our findings indicate that PromptAid helps users to iterate over prompt template alterations with less cognitive overhead, generate diverse prompts with help of recommendations, and analyze the performance of the generated prompts while surpassing existing state-of-the-art prompting interfaces in performance.","PromptAid: Prompt Exploration, Perturbation, Testing and Iteration using Visual Analytics for Large Language Models",https://www.semanticscholar.org/paper/a2c8d1c5470435176185bf891c76711a9b44808a,"By simply composing prompts, developers can prototype novel generative applications with Large Language Models (LLMs). To refine prototypes into products, however, developers must iteratively revise prompts by evaluating outputs to diagnose weaknesses. Formative interviews (N=8) revealed that developers invest significant effort in manually evaluating outputs as they assess context-specific and subjective criteria. We present EvalLM, an interactive system for iteratively refining prompts by evaluating multiple outputs on user-defined criteria. By describing criteria in natural language, users can employ the system’s LLM-based evaluator to get an overview of where prompts excel or fail, and improve these based on the evaluator’s feedback. A comparative study (N=12) showed that EvalLM, when compared to manual evaluation, helped participants compose more diverse criteria, examine twice as many outputs, and reach satisfactory prompts with 59% fewer revisions. Beyond prompts, our work can be extended to augment model evaluation and alignment in specific application contexts.",EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria,https://www.semanticscholar.org/paper/a0d83f9e15e722f23c14eb83cb2f87c1d1ea6400,"Prompt engineering for large language models (LLMs) is a critical to effectively leverage their capabilities. However, due to the inherent stochastic and opaque nature of LLMs, prompt engineering is far from an exact science. Crafting prompts that elicit the desired responses still requires a lot of trial and error to gain a nuanced understanding of a model’s strengths and limitations for one’s specific task context and target application. To support users in sensemaking around the outputs of LLMs, we create ChainForge, an open-source visual programming environment for prompt engineering. ChainForge is publicly available, both on the web (https://chainforge.ai) and as a locally installable Python package hosted on PyPI. We detail some features of ChainForge and how we iterated the design in response to internal and external feedback.",ChainForge: An open-source visual programming environment for prompt engineering,https://www.semanticscholar.org/paper/bb411609bbe11aa674e484507c989ad933b1e64c,"Large language model (LLM) prompting is a promising new approach for users to create and customize their own chatbots. However, current methods for steering a chatbot’s outputs, such as prompt engineering and fine-tuning, do not support users in converting their natural feedback on the model’s outputs to changes in the prompt or model. In this work, we explore how to enable users to interactively refine model outputs through their feedback, by helping them convert their feedback into a set of principles (i.e. a constitution) that dictate the model’s behavior. From a formative study, we (1) found that users needed support converting their feedback into principles for the chatbot and (2) classified the different principle types desired by users. Inspired by these findings, we developed ConstitutionMaker, an interactive tool for converting user feedback into principles, to steer LLM-based chatbots. With ConstitutionMaker, users can provide either positive or negative feedback in natural language, select auto-generated feedback, or rewrite the chatbot’s response; each mode of feedback automatically generates a principle that is inserted into the chatbot’s prompt. In a user study with 14 participants, we compare ConstitutionMaker to an ablated version, where users write their own principles. With ConstitutionMaker, participants felt that their principles could better guide the chatbot, that they could more easily convert their feedback into principles, and that they could write principles more efficiently, with less mental demand. ConstitutionMaker helped users identify ways to improve the chatbot, formulate their intuitive responses to the model into feedback, and convert this feedback into specific and clear principles. Together, these findings inform future tools that support the interactive critiquing of LLM outputs.",ConstitutionMaker: Interactively Critiquing Large Language Models by Converting Feedback into Principles,https://www.semanticscholar.org/paper/ccac57515a8fedc0631de58879f886e827e725ad,"Prototyping is notoriously difficult to do with machine learning (ML), but recent advances in large language models may lower the barriers to people prototyping with ML, through the use of natural language prompts. This case study reports on the real-world experiences of industry professionals (e.g. designers, program managers, front-end developers) prototyping new ML-powered feature ideas via prompt-based prototyping. Through interviews with eleven practitioners during a three-week sprint and a workshop, we find that prompt-based prototyping reduced barriers of access by substantially broadening who can prototype with ML, sped up the prototyping process, and grounded communication between collaborators. Yet, it also introduced new challenges, such as the need to reverse-engineer prompt designs, source example data, and debug and evaluate prompt effectiveness. Taken together, this case study provides important implications that lay the groundwork toward a new future of prototyping with ML.",PromptMaker: Prompt-based Prototyping with Large Language Models,https://www.semanticscholar.org/paper/5bc61019771a9fe2a12cc41bad1d9ae4222a152c,"In this paper, we investigate the effectiveness of state-of-the-art LLM, i.e., GPT-4, with three different prompting engineering techniques (i.e., basic prompting, in-context learning, and task-specific prompting) against 18 fine-tuned LLMs on three typical ASE tasks, i.e., code generation, code summarization, and code translation. Our quantitative analysis of these prompting strategies suggests that prompt engineering GPT-4 cannot necessarily and significantly outperform fine-tuning smaller/older LLMs in all three tasks. For comment generation, GPT-4 with the best prompting strategy (i.e., task-specific prompt) had outperformed the first-ranked fine-tuned model by 8.33% points on average in BLEU. However, for code generation, the first-ranked fine-tuned model outperforms GPT-4 with best prompting by 16.61% and 28.3% points, on average in BLEU. For code translation, GPT-4 and fine-tuned baselines tie as they outperform each other on different translation tasks. To explore the impact of different prompting strategies, we conducted a user study with 27 graduate students and 10 industry practitioners. From our qualitative analysis, we find that the GPT-4 with conversational prompts (i.e., when a human provides feedback and instructions back and forth with a model to achieve best results) showed drastic improvement compared to GPT-4 with automatic prompting strategies. Moreover, we observe that participants tend to request improvements, add more context, or give specific instructions as conversational prompts, which goes beyond typical and generic prompting strategies. Our study suggests that, at its current state, GPT-4 with conversational prompting has great potential for ASE tasks, but fully automated prompt engineering with no human in the loop requires more study and improvement.",Prompt Engineering or Fine Tuning: An Empirical Assessment of Large Language Models in Automated Software Engineering Tasks,https://www.semanticscholar.org/paper/59e0e0c1aa06d51430792eb5d8308911a1b0110f,"Large language models (LLMs) have made impressive progress in natural language processing. These models rely on proper human instructions (or prompts) to generate suitable responses. However, the potential of LLMs are not fully harnessed by commonly-used prompting methods: many human-in-the-loop algorithms employ ad-hoc procedures for prompt selection; while auto prompt generation approaches are essentially searching all possible prompts randomly and inefficiently. We propose Evoke, an automatic prompt refinement framework. In Evoke, there are two instances of a same LLM: one as a reviewer (LLM-Reviewer), it scores the current prompt; the other as an author (LLM-Author), it edits the prompt by considering the edit history and the reviewer's feedback. Such an author-reviewer feedback loop ensures that the prompt is refined in each iteration. We further aggregate a data selection approach to Evoke, where only the hard samples are exposed to the LLM. The hard samples are more important because the LLM can develop deeper understanding of the tasks out of them, while the model may already know how to solve the easier cases. Experimental results show that Evoke significantly outperforms existing methods. For instance, in the challenging task of logical fallacy detection, Evoke scores above 80, while all other baseline methods struggle to reach 20.",Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt Editing,https://www.semanticscholar.org/paper/a02341d26c41792894093f5fb21dea42f5ad4f89,"Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation. On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction. Based on this, we present the novel idea of Language Model Programming (LMP). LMP generalizes language model prompting from pure text prompts to an intuitive combination of text prompting and scripting. Additionally, LMP allows constraints to be specified over the language model output. This enables easy adaption to many tasks while abstracting language model internals and providing high-level semantics. To enable LMP, we implement LMQL (short for Language Model Query Language), which leverages the constraints and control flow from an LMP prompt to generate an efficient inference procedure that minimizes the number of expensive calls to the underlying language model. We show that LMQL can capture a wide range of state-of-the-art prompting methods in an intuitive way, especially facilitating interactive flows that are challenging to implement with existing high-level APIs. Our evaluation shows that we retain or increase the accuracy on several downstream tasks, while also significantly reducing the required amount of computation or cost in the case of pay-to-use APIs (26-85% cost savings).",Prompting Is Programming: A Query Language for Large Language Models,https://www.semanticscholar.org/paper/c2329c685f11efa25c562f97be71ff03103423fd,"Chaining language model (LM) calls as composable modules is fueling a new way of programming, but ensuring LMs adhere to important constraints requires heuristic""prompt engineering"". We introduce LM Assertions, a programming construct for expressing computational constraints that LMs should satisfy. We integrate our constructs into the recent DSPy programming model for LMs, and present new strategies that allow DSPy to compile programs with LM Assertions into more reliable and accurate systems. We also propose strategies to use assertions at inference time for automatic self-refinement with LMs. We report on four diverse case studies for text generation and find that LM Assertions improve not only compliance with imposed rules but also downstream task performance, passing constraints up to 164% more often and generating up to 37% more higher-quality responses. Our reference implementation of LM Assertions is integrated into DSPy at https://github.com/stanfordnlp/dspy",DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines,https://www.semanticscholar.org/paper/0789104dd0dfe229be1cff270e4c94d7791454bb,novel
Develop a comprehensive review-based tool designed to enhance literature review efficiency by integrating detailed summaries and contextual insights from a wide array of sources.,"['Computer Science', 'Human-Computer Interaction', 'Information Retrieval', 'Digital Libraries']","Making sense of research literature is a complicated process that involves various information seeking and compre-hension tasks. The lack of support for sensemaking in existing systems presents important design challenges and opportunities. This research proposes the design of an integral environment to support literature search, selection, organization and comprehension. Our system prototype, CiteSense, offers lightweight interaction tools and a smooth transition among various information activities. This research deepens our understanding of the design of systems that support the sensemaking of research literature.",CiteSense: supporting sensemaking of research literature,https://www.semanticscholar.org/paper/5b921f6efa2bf6990cbef689a8c8c972d0bf2e7c,"With the rapid growth of scholarly archives, researchers subscribe to “paper alert’’ systems that periodically provide them with recommendations of recently published papers that are similar to previously collected papers. However, researchers sometimes struggle to make sense of nuanced connections between recommended papers and their own research context, as existing systems only present paper titles and abstracts. To help researchers spot these connections, we present PaperWeaver, an enriched paper alerts system that provides contextualized text descriptions of recommended papers based on user-collected papers. PaperWeaver employs a computational method based on Large Language Models (LLMs) to infer users’ research interests from their collected papers, extract context-specific aspects of papers, and compare recommended and collected papers on these aspects. Our user study (N=15) showed that participants using PaperWeaver were able to better understand the relevance of recommended papers and triage them more confidently when compared to a baseline that presented the related work sections from recommended papers.",PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers,https://www.semanticscholar.org/paper/2cbdad900dda764efa362793765ec12cb5ae66f5,"Learn how to gather, organize, store, and add citations to your papers. This page only available immediately following our RefWorks Workshops",Research Guides: Citation Management with RefWorks 2 (Legacy): Workshop Feedback,https://www.semanticscholar.org/paper/265bd94319389795fb2bb953c85880cb3b065572,"In a growing digital landscape, enhancing the discoverability and resonance of scientific articles is essential. Here, we offer 10 recommendations to amplify the discoverability of studies in search engines and databases. Particularly, we argue that the strategic use and placement of key terms in the title, abstract and keyword sections can boost indexing and appeal. By surveying 230 journals in ecology and evolutionary biology, we found that current author guidelines may unintentionally limit article findability. Our survey of 5323 studies revealed that authors frequently exhaust abstract word limits—particularly those capped under 250 words. This suggests that current guidelines may be overly restrictive and not optimized to increase the dissemination and discoverability of digital publications. Additionally, 92% of studies used redundant keywords in the title or abstract, undermining optimal indexing in databases. We encourage adopting structured abstracts to maximize the incorporation of key terms in titles, abstracts and keywords. In addition, we encourage the relaxation of abstract and keyword limitations in journals with strict guidelines, and the inclusion of multilingual abstracts to broaden global accessibility. These recommendations to editors are designed to improve article engagement and facilitate evidence synthesis, thereby aligning scientific publishing with the modern needs of academic research.","Title, abstract and keywords: a practical guide to maximize the visibility and impact of academic papers",https://www.semanticscholar.org/paper/a530dd32a470a86fc8d16799eda3fe781a8728e0,"Writing an academic paper requires significant time and effort to find, read, and organize many related papers, which are complex knowledge tasks. We present a novel interactive system that allows users to perform these tasks quickly and easily on the 2D canvas with pen and multitouch inputs, turning users’ sketches and handwriting into node-link diagrams of papers and citations that users can iteratively expand in situ toward constructing a coherent narrative when writing Related Work sections. Through a pilot study involving researchers experienced in publishing academic papers, we show that our system can serve as a visual, integrated, and flexible workspace for conducting comprehensive literature reviews.","Understanding Visual, Integrated, and Flexible Workspace for Comprehensive Literature Reviews with SketchingRelatedWork",https://www.semanticscholar.org/paper/ba7ce5a2cceeee219f6466d8bb0f57ced7c1d5c5,"Searching large digital repositories can be extremely frustrating, as common list-based formats encourage users to adopt a convenience-sampling approach that favours chance discovery and random search, over meaningful exploration. We have designed a methodology that allows users to visually and thematically explore corpora, while developing personalised holistic reading strategies. We describe the results of a three-phase qualitative study, in which experienced researchers used our interactive visualisation approach to analyse a set of publications and select relevant themes and papers. Using in-depth semi-structured interviews and stimulated recall, we found that users: (i) selected papers that they otherwise would not have read, (ii) developed a more coherent reading strategy, and (iii) understood the thematic structure and relationships between papers more effectively. Finally, we make six design recommendations to enhance current digital repositories that we have shown encourage users to adopt a more holistic and thematic research approach.",Enhancing Reading Strategies by Exploring A Theme-based Approach to Literature Surveys,https://www.semanticscholar.org/paper/76f66e1f0f7d29a6b0b70f9fc4c0f3012238e33e,"A literature review is a critical task in performing research. However, even browsing an academic database and choosing must-read items can be daunting for novice researchers. In this paper, we introduce Papers101, an interactive system that supports novice researchers’ discovery of papers relevant to their research topics. Prior to system design, we performed a formative study to investigate what difficul-ties novice researchers often face and how experienced researchers address them. We found that novice researchers have difficulty in identifying appropriate search terms, choosing which papers to read first, and ensuring whether they have examined enough candidates. In this work, we identified key requirements for the system dedicated to novices: prioritizing search results, unifying the contexts of multiple search results, and refining and validating the search queries. Accordingly, Papers101 provides an opinionated perspective on selecting important metadata among papers. It also visualizes how the priority among papers is developed along with the users’ knowledge discovery process. Finally, we demonstrate the potential usefulness of our system with the case study on the metadata collection of papers in visualization and HCI community.",Papers101: Supporting the Discovery Process in the Literature Review Workflow for Novice Researchers,https://www.semanticscholar.org/paper/adc5ab3d883c0441e229497f9831878e4ad85ef7,"Scholars who want to research a scientific topic must take time to read, extract meaning, and identify connections across many papers. As scientific literature grows, this becomes increasingly challenging. Meanwhile, authors summarize prior research in papers’ related work sections, though this is scoped to support a single paper. A formative study found that while reading multiple related work paragraphs helps overview a topic, it is hard to navigate overlapping and diverging references and research foci. In this work, we design a system, Relatedly, that scaffolds exploring and reading multiple related work paragraphs on a topic, with features including dynamic re-ranking and highlighting to spotlight unexplored dissimilar information, auto-generated descriptive paragraph headings, and low-lighting of redundant information. From a within-subjects user study (n=15), we found that scholars generate more coherent, insightful, and comprehensive topic outlines using Relatedly compared to a baseline paper list.",Relatedly: Scaffolding Literature Reviews with Existing Related Work Sections,https://www.semanticscholar.org/paper/c388626d1a342339078aaab7acc280efbc4f77fc,"The literature review is a key component of academic research, which allows researchers to build upon each other's work. While modern search engines enable fast access to publications, there is a lack of support for filtering out the vast majority of papers that are irrelevant to the current research focus. We present PaperQuest, a visualization tool that supports efficient reading decisions, by only displaying the information useful at a given step of the review. We propose an algorithm to find and sort papers that are likely to be relevant to users, based on the papers they have already expressed interest in and the number of citations. The current implementation uses papers from the CHI, UIST, and VIS conferences, and citation counts from Google Scholar, but is easily extensible to other domains of the literature.",PaperQuest: A Visualization Tool to Support Literature Review,https://www.semanticscholar.org/paper/ee39f99c99ca2d372c039b8d84620e2537abff04,"Citations and their accompanying information such as citation context is a very important consideration when searching for relevant information in literature. With literature continually expanding, obtaining this information in an efficient and effective way, using a standard search tool becomes a very laborious task. In addition, scientific articles found in the literature employ rich interrelationships e.g., citation and their accompanying information making it very difficult for standard search applications to present these interrelationships to the searcher in an efficient manner. In order to alleviate this intensive task, we have designed a tool VisNavi (Visualization and Navigation) that supports scientific researchers in their literature review, in a form of a visualized star-center approach that, for the current treated paper places a citing author at its center and the richly embedded interrelationships are spun around it. The star design enables researchers to gain a clearer insight by interactively exploring these rich interrelationships along with their accompanying information. We designed a human judgment experiment to obtain a human rating on the functionality of the tool. We then cast the human rating as a reference point to improve the tool's design.","Supporting Literature Review by Searching, Visualizing and Navigating Related Papers",https://www.semanticscholar.org/paper/b961214ababf77e17e6c59e53b03a732e2ff3dd9,not novel
"Develop a system that **integrates follow-on work into paper reading** by providing **contextualized text descriptions** for sections of a paper that have been cited by subsequent work. This system would automatically generate descriptions that explain how each section of the paper has been referenced or built upon in later research, enhancing the reader's understanding of the paper's impact. **A user study with scientists** could evaluate the effectiveness of this system by comparing it with traditional methods of reading follow-on work, assessing improvements in comprehension and efficiency.","['Computer Science', 'Human-Computer Interaction', 'Information Retrieval', 'Large Language Models', 'Scholarly Communication']","When reading a scholarly paper, scientists oftentimes wish to understand how follow-on work has built on or engages with what they are reading. While a paper itself can only discuss prior work, some scientific search engines can provide a list of all subsequent citing papers; unfortunately, they are undifferentiated and disconnected from the contents of the original reference paper. In this work, we introduce a novel paper reading experience that integrates relevant information about follow-on work directly into a paper, allowing readers to learn about newer papers and see how a paper is discussed by its citing papers in the context of the reference paper. We built a tool, called CiteRead, that implements the following three contributions: 1) automated techniques for selecting important citing papers, building on results from a formative study we conducted, 2) an automated process for localizing commentary provided by citing papers to a place in the reference paper, and 3) an interactive experience that allows readers to seamlessly alternate between the reference paper and information from citing papers (e.g., citation sentences), placed in the margins. Based on a user study with 12 scientists, we found that in comparison to having just a list of citing papers and their citation sentences, the use of CiteRead while reading allows for better comprehension and retention of information about follow-on work.",CiteRead: Integrating Localized Citation Contexts into Scientific Paper Reading,https://www.semanticscholar.org/paper/277360f074e809135d4b49cf7fd2f572c0db6658,"Researchers spend lots of time for reading scientific papers as they need to stay updated with recent trends. However, navigating citations, which are indispensable elements of research papers, can act as a barrier for junior researchers as they do not have enough background knowledge and experience. We conduct a formative user study to identify challenges in navigating cited papers. We then prototype QuickRef, an interactive reader that provides additional information about cited papers on the side panel. A preliminary user study documents the usability of QuickRef. Further, we present practical design implications for citation navigation support.",QuickRef: Should I Read Cited Papers for Understanding This Paper?,https://www.semanticscholar.org/paper/02ff1b212cf28ee9af968180765d1eb2b2885d9d,"Scholars who want to research a scientific topic must take time to read, extract meaning, and identify connections across many papers. As scientific literature grows, this becomes increasingly challenging. Meanwhile, authors summarize prior research in papers’ related work sections, though this is scoped to support a single paper. A formative study found that while reading multiple related work paragraphs helps overview a topic, it is hard to navigate overlapping and diverging references and research foci. In this work, we design a system, Relatedly, that scaffolds exploring and reading multiple related work paragraphs on a topic, with features including dynamic re-ranking and highlighting to spotlight unexplored dissimilar information, auto-generated descriptive paragraph headings, and low-lighting of redundant information. From a within-subjects user study (n=15), we found that scholars generate more coherent, insightful, and comprehensive topic outlines using Relatedly compared to a baseline paper list.",Relatedly: Scaffolding Literature Reviews with Existing Related Work Sections,https://www.semanticscholar.org/paper/c388626d1a342339078aaab7acc280efbc4f77fc,"With the rapid growth of scholarly archives, researchers subscribe to “paper alert’’ systems that periodically provide them with recommendations of recently published papers that are similar to previously collected papers. However, researchers sometimes struggle to make sense of nuanced connections between recommended papers and their own research context, as existing systems only present paper titles and abstracts. To help researchers spot these connections, we present PaperWeaver, an enriched paper alerts system that provides contextualized text descriptions of recommended papers based on user-collected papers. PaperWeaver employs a computational method based on Large Language Models (LLMs) to infer users’ research interests from their collected papers, extract context-specific aspects of papers, and compare recommended and collected papers on these aspects. Our user study (N=15) showed that participants using PaperWeaver were able to better understand the relevance of recommended papers and triage them more confidently when compared to a baseline that presented the related work sections from recommended papers.",PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers,https://www.semanticscholar.org/paper/2cbdad900dda764efa362793765ec12cb5ae66f5,"When starting a new research activity, it is essential to study related work. Traditional search engines and dedicated social networks are generally used to search for relevant literature. Current technologies rely on keyword based searches which, however, do not provide the support of a wider context. Cite-as-you-write aims to simplify and shorten this exploratory task: given a verbose description of the problem to be investigated, the system automatically recommends related papers/citations.",Cite-as-you-write,https://www.semanticscholar.org/paper/48296ce509c66d8f306aff53d886eb5cd7ff3cb6,"Discussing and documenting scientific papers is an important and challenging task. To address it, we have developed a comprehensive paper process model which we have applied, improved and adapted over the past two years at weekly discussion sessions. The model captures paper selection and discussion, as well as the recording and retrieval of discussion results. HyperRef is a system that provides online support for many facets of this process model, in particular, for its discussion part and for data management. The system gives guidance throughout the paper discussions, supporting the systematic and detailed classification, analysis, and evaluation of research papers, and guaranteeing that all relevant information of the discussions is retained in an annotated bibliography database. The initial motivation for this work was to educate graduate students; our approach and supporting system is now applied by several research groups as well as by individual researchers to scientific papers and research literature in general.",Computer-Aided Discussion and Management of Research Literature,https://www.semanticscholar.org/paper/a4ecb35821753dc574255f74b72b919290faf7b5,"The present study proposes LitStoryTeller, an interactive system for visually exploring the semantic structure of a scientific article. We demonstrate how LitStoryTeller could be used to answer some of the most fundamental research questions, such as how a new method was built on top of existing methods, based on what theoretical proof and experimental evidences. More importantly, LitStoryTeller can assist users to understand the full and interesting story a scientific paper, with a concise outline and important details. The proposed system borrows a metaphor from screen play, and visualizes the storyline of a scientific paper by arranging its characters (scientific concepts or terminologies) and scenes (paragraphs/sentences) into a progressive and interactive storyline. Such storylines help to preserve the semantic structure and logical thinking process of a scientific paper. Semantic structures, such as scientific concepts and comparative sentences, are extracted using existing named entity recognition APIs and supervised classifiers, from a scientific paper automatically. Two supplementary views, ranked entity frequency view and entity co-occurrence network view, are provided to help users identify the ""main plot"" of such scientific storylines. When collective documents are ready, LitStoryTeller also provides a temporal entity evolution view and entity community view for collection digestion.",LitStoryTeller: An Interactive System for Visual Exploration of Scientific Papers Leveraging Named entities and Comparative Sentences,https://www.semanticscholar.org/paper/aa576289b22c82b991dc4ce66584257c335e934d,"Scholars need to keep up with an exponentially increasing flood of scientific papers. To aid this challenge, we introduce Scim, a novel intelligent interface that helps scholars skim papers to rapidly review and gain a cursory understanding of its contents. Scim supports the skimming process by highlighting salient content within a paper, directing a scholar’s attention. These automatically-extracted highlights are faceted by content type, evenly distributed across a paper, and have a density configurable by scholars. We evaluate Scim with an in-lab usability study and a longitudinal diary study, revealing how its highlights facilitate the more efficient construction of a conceptualization of a paper. Finally, we describe the process of scaling highlights from their conception within Scim, a research prototype, to production on over 521,000 papers within the Semantic Reader, a publicly-available augmented reading interface for scientific papers. We conclude by discussing design considerations and tensions for the design of future skimming tools with augmented intelligence.",Accelerating Scientific Paper Skimming with Augmented Intelligence Through Customizable Faceted Highlights,https://www.semanticscholar.org/paper/937aa9229358b0c19ad3b9d224eb0599e0552986,"This work presents a new, scalable solution to the problem of extracting citation contexts: the textual fragments surrounding citation references. These citation contexts can be used to navigate digital libraries of research papers to help users in deciding what to read. We have developed a prototype system which can retrieve, on-demand, citation contexts from the full text of over 15 million research articles in the Mendeley catalog for a given reference research paper. The evaluation results show that our citation extraction system provides additional functionality over existing tools, has two orders of magnitude faster runtime performance, while providing a 9% improvement in F-measure over the current state-of-the-art.",What Others Say About This Work ? Scalable Extraction of Citation Contexts from Research Papers Journal Item,https://www.semanticscholar.org/paper/02d720838f6f7333bf76f253a37a6d10c7064544,"When reading a scholarly article, inline citations help researchers contextualize the current article and discover relevant prior work. However, it can be challenging to prioritize and make sense of the hundreds of citations encountered during literature reviews. This paper introduces CiteSee, a paper reading tool that leverages a user’s publishing, reading, and saving activities to provide personalized visual augmentations and context around citations. First, CiteSee connects the current paper to familiar contexts by surfacing known citations a user had cited or opened. Second, CiteSee helps users prioritize their exploration by highlighting relevant but unknown citations based on saving and reading history. We conducted a lab study that suggests CiteSee is significantly more effective for paper discovery than three baselines. A field deployment study shows CiteSee helps participants keep track of their explorations and leads to better situational awareness and increased paper discovery via inline citation when conducting real-world literature reviews.",CiteSee: Augmenting Citations in Scientific Papers with Persistent and Personalized Historical Context,https://www.semanticscholar.org/paper/81f7eb73883a559e39a5ab7754c77371488c4c7e,not novel
"Design and implement a gamified crowdsourcing application to collect data on student learning behaviors in science education, and analyze how Computational Thinking (CT) principles can be applied to improve educational strategies.","['Education Technology', 'Gamification', 'Crowdsourcing', 'Computational Thinking', 'Science Education']","Gamification, the application of game elements to non-game settings, continues to grow in popularity as a method to increase student engagement in the classroom. We tested students across two courses, measuring their motivation, social comparison, effort, satisfaction, learner empowerment, and academic performance at four points during a 16-week semester. One course received a gamified curriculum, featuring a leaderboard and badges, whereas the other course received the same curriculum without the gamified elements. Our results found that students in the gamified course showed less motivation, satisfaction, and empowerment over time than those in the non-gamified class. The effect of course type on students' final exam scores was mediated by students' levels of intrinsic motivation, with students in the gamified course showing less motivation and lower final exam scores than the non-gamified class. This suggests that some care should be taken when applying certain gamification mechanics to educational settings. © 2014 Elsevier Ltd. All rights reserved.","Assessing the effects of gami fi cation in the classroom : A longitudinal study on intrinsic motivation , social comparison , satisfaction , effort , and academic performance",https://www.semanticscholar.org/paper/0e10b3ab96fbe41cc1ec24542cf111acd8b4a0a6,"Over the last 10 years, research on gamification, the use of game elements in non-game contexts, has increased in the field of education, due to its potential to enhance learning performance. Yet, the majority of available research rather focuses on the evaluation of motivation and engagement as key dependent variables. Hence, the purpose of this study is to review available studies on gamification, with an exclusive focus on learning performance as the key dependent variable. Through a systematic search and selection process, building on ""Web of Science"" articles and by considering studies between 2000 and 2016 related to gamification and learning, 582 articles were identified. Further inclusion and exclusion criteria, regarding setting (education), study focus (empirical), journal access (full access) and dependent variables (learning performance), resulted in a review of 23 articles meeting the criteria. The analysis of these articles showed how gamification could be linked to a direct increase in learning performance of students. Nevertheless, some studies also reflect weaker statistical differences between being involved or not in a gamified environment. The review analysis results are especially helpful to define a future agenda for gamification research, addressing the following gaps in the literature. First, include mediating and moderating variables to find more empirical research that can prove an indirect effect of gamification on learning performance. Second, carry out additional research that empirically underpins the direct linkage between gamification and learning performance. Third, include specific individual gamification elements to be able to determine explicit differential effects of these elements on learning performance. Fourth, conduct research in a broader range of knowledge fields to develop empirical evidence in the context of other knowledge domains next to computer sciences. Finally, consider involving larger sample and setting up longer experimental interventions, to avoid novelty effects and risks of lack of generalization.",Gamification and learning performance : a systematic review of the literature,https://www.semanticscholar.org/paper/287c795f6c348b10923a18c7e230ea21772a6c80,"Gamification is a term that refers to the use of game elements in non-game contexts with the goal of engaging people in a variety of tasks. There is a growing interest in gamification as well as its applications and implications in the field of Education since it provides an alternative to engage and motivate students during the process of learning. Despite this increasing interest, to the best of our knowledge, there are no studies that cover and classify the types of research being published and the most investigated topics in the area. As a first step towards bridging this gap, we carried out a systematic mapping to synthesize an overview of the area. We went through 357 papers on gamification. Among them, 48 were related to education and only 26 met the criteria for inclusion and exclusion of articles defined in this study. These 26 papers were selected and categorized according to their contribution. As a result, we provide an overview of the area. Such an overview suggests that most studies focus on investigating how gamification can be used to motivate students, improve their skills, and maximize learning.",A systematic mapping on gamification applied to education,https://www.semanticscholar.org/paper/2e35876bd2ccd680f8628d61fb8faadbd26f0eba,"ABSTRACT Whether gamification is an organized structure that contributes to student achievement, a simple pontification process or total nonsense is a matter of debate. In such, this study was conducted to provide a scientific answer while exhibiting the gamification effect on student achievement with the meta-analysis method, which is based on experimental research that investigates the effect of gamification on student achievement between 2010 and 2016. As a result of the completed meta-analysis, a common effect size value that was estimated by compiling experimental studies was found to be 0.557. This value shows that gamification has a moderately positive (Cohen, 1988) effect on student achievement. It is exhibited that there is no publication bias regarding determining the effect’s validity. Calculated 0.557 Hedges’ g value’s omega square equivalent is .072. This value shows that gamification has added 7.2% positive value to academic achievement according to 45 experimental results involving 3,487 students from different countries.",The effects of gamification on students’ academic achievement: a meta-analysis study,https://www.semanticscholar.org/paper/4db8d2baea0288e092e95b01ff7c3a05e9447ba9,"Gamification systems have the potential to foster students’ engagement and enhance their learning performance. Although the literature to date has provided relevant contributions in explaining how gamification design can be effective, several studies seek to address the constructs used to measure the effects of gamification. To analyze how gamification outcomes should be properly measured, this paper systematically reviews the literature and provides a map of the most frequent scales used to measure gamification outcomes in the educational context. As research findings, we identify motivation, engagement, self-efficacy, and flow/cognitive absorption as the primary constructs addressed to experiential outcomes. Additionally, there are research opportunities to develop a better understanding of the effects of extrinsic motivation rewards on experiential outcomes and problem-solving transfer posited as instrumental outcome.",Using Gamification in Education: A Systematic Literature Review,https://www.semanticscholar.org/paper/7927ae9430e3045a3bbf921de6cc93c5df898cc9,"subject areas of science and social studies. The effects of games on learning are measured in both cognitive and affective domains through quantitative measures, often involving a control group. Overall, the efficacy of games and gamification is positive .",A systematic literature review of game-based learning and gamification research in Asia,https://www.semanticscholar.org/paper/d4f71e73735cd25695b6782b86e31a10cebc4114,"This manuscript reports the effects of gamification elements on primary and college students’ motivation and learning. This mixed methods research reports two years of data collection of primary and college students’ reflections, preand post-test results, and survey results as well as observations made by the author. Business communication classes and mathematics classes experienced gamification by introducing game elements and frameworks in the classrooms while maintaining the integrity of the learning outcomes. Students expressed increased motivation and engagement at both the primary and college level as well as improved learning.",Gamification Effects on Motivation and Learning: Application to Primary and College Students,https://www.semanticscholar.org/paper/fe3aa0e8f080189c32635920b45a92063f5758c7,"Aim/Purpose: This study reviewed previous research on the role of gamification techniques in promoting students’ learning.

Background: The role of gamification in promoting students’ learning has been investigated empirically by many scholars. To date, mixed results about the effectiveness of gamification have been reported, and researchers frequently argue that the inappropriateness of certain techniques may have contributed to these mixed findings.

Methodology: The Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) protocol was used to assess the criteria required for this review. A total of 40 studies were identified and included in the systematic review. The selected studies were used to assess the association between certain gamification techniques and students’ learning in this study.

Findings: The results showed that gamification techniques differently affect students’ learning. In addition, it is important for students to be instructed about the application of gamification approach before they engage in a gamified learning task. The key challenges relating to the use of gamification techniques were also discussed.

Recommendations for Practitioners: This review can help educational decision makers and practitioners to stimulate certain learning outcomes of the students with the help of specific gamification techniques. 

",The Role of Gamification Techniques in Promoting Student Learning: A Review and Synthesis,https://www.semanticscholar.org/paper/e0fe048ad57710e1f3b1edec2a8c61aefb05208f,"Gamification is the application of game features, mainly video game elements, into non-game context for the purpose of promoting motivation and engagement in learning. The application of gamification in a pedagogical context provides some remedy for many students who find themselves alienated by traditional methods of instruction. The use of gamification could provide a partial solution to the decline in learners’ motivation and engagement the schooling system is facing today. Specifically, the college environment could benefit a lot from gamifying not only their graduate recruitment strategies, but also the college course content and curricula. This critical analysis of literature on gamification is intended to be part of a sequence on the effect of gamification on motivation and engagement. A proposed methodology in the study of gamification effect on motivation and engagement in addition to an empirical study on three college courses are being finalized to complete this trilogy. The paper aims to discuss these issues.,Themes covered in the literature review include: conceptualizing gamification, advantages of gamification over game-based learning, theoretical connections to gamification, motivation and engagement, connecting gamification to motivation and engagement, emotions and fun in gamification, player types and gamification features, gamification in action, and implementation guidelines.,The literature on the effect of gamification on motivation and gamification is still limited on multiple levels. There is a gap between theory and practice in the study of gamification. There is limited literature on the implementation guidelines of the gamified designs.,This critical analysis of literature is followed by connecting it to future research by the same author as part of a sequence on the effect of gamification on motivation and engagement. The second project, will be proposing a methodology for any successful design to provide a holistic understanding of the topic of gamification. Finally, an empirical study on the effect of gamification on students’ motivation and engagement in three college courses will be submitted to complete the trilogy.,This paper is a literature review, so there is a strong connection to literature on this topic. However, the synthesis of the themes and ideas are original. The literature review is extensive and covers the different aspects of the topic of gamification and its relationship to motivation and engagement.",The effect of gamification on motivation and engagement,https://www.semanticscholar.org/paper/7c2c23195bcef0d90cbef64378de7a641e5e69a6,"In learning contexts, gamification is a technique to motivate learners and enhance their participation in learning activities by applying game elements and components. But it still pays little attention to using gamification in Adult learning activities. Through a systematic literature review, this study investigates the literature on the motivational and behavioral theories underlying gamification in the context of learning to synthesize the essential factors to improve learning outcomes. This paper presents a conceptual model of gamification in a learning context. A theory-driven model was created in order to categorize into a multi-dimensional model. In addition, this model can be extended to any kind of games not only educational games because the whole gaming experience is based on the same theory as human learning. It originates from behavioral science and it has progressed well-stablished learning algorithm. By investigating and synthesizing the earlier studies and categorizing them in accordance with a contemporary approach, a conceptual model for gamification of adult learning has been proposed. Then in order to validate and test this theory-driven model should be implemented and tested in experimental studies involving organization employees.",A Model for Utilizing the Potential of Gamification in Learning,https://www.semanticscholar.org/paper/71c81d628e10765870997f7a01b25f1c361f0255,novel
"Develop a new hierarchical topic model that leverages **ensemble simulations** **to improve hierarchical topic modeling**. By creating multiple topic models with different algorithmic configurations and merging their outputs, the model can better capture the nuances of large text corpora. The model will be evaluated through a **spread analysis** to understand the variability and robustness of topic hierarchies generated by the ensemble approach. This can significantly enhance the accuracy and coherence of the discovered topic structures, especially in diverse datasets.","['Computer Science', 'Machine Learning', 'Natural Language Processing', 'Topic Modeling', 'Hyperbolic Geometry']","In the past decade, a number of advances in topic modeling have produced sophisticated models that are capable of generating hierarchies of topics. One challenge for these models is scalability: they are incapable of working at the massive scale of millions of documents and hundreds of thousands of terms. We address this challenge with a technique that learns a hierarchy of topics by iteratively applying topic models and processing subtrees of the hierarchy in parallel. This approach has a number of scalability advantages compared to existing techniques, and shows promising results in experiments assessing runtime and human evaluations of quality. We detail extensions to this approach that may further improve hierarchical topic modeling for large-scale applications.",Large-Scale Hierarchical Topic Models,https://www.semanticscholar.org/paper/767156cd88da3e3eb4c79e860fc87a4526012bbd,"Large-scale topic models serve as basic tools for feature extraction and dimensionality reduction in many practical applications. As a natural extension of flat topic models, hierarchical topic models (HTMs) are able to learn topics of different levels of abstraction, which lead to deeper understanding and better generalization than their flat counterparts. However, existing scalable systems for flat topic models cannot handle HTMs, due to their complicated data structures such as trees and concurrent dynamically growing matrices, as well as their susceptibility to local optima. 
 
In this paper, we study the hierarchical latent Dirichlet allocation (hLDA) model which is a powerful nonparametric Bayesian HTM. We propose an efficient partially collapsed Gibbs sampling algorithm for hLDA, as well as an initialization strategy to deal with local optima introduced by tree-structured models. We also identify new system challenges in building scalable systems for HTMs, and propose efficient data layout for vectorizing HTM as well as distributed data structures including dynamic matrices and trees. Empirical studies show that our system is 87 times more efficient than the previous open-source implementation for hLDA, and can scale to thousands of CPU cores. We demonstrate our scalability on a 131-million-document corpus with 28 billion tokens, which is 4--5 orders of magnitude larger than previously used corpus. Our distributed implementation can extract 1,722 topics from the corpus with 50 machines in just 7 hours.",Scalable Training of Hierarchical Topic Models,https://www.semanticscholar.org/paper/d8cfcea364c1b43abd971d29a5a8e12f73f1ef40,"
 
 Hierarchical latent tree analysis (HLTA) is recently proposed as a new method for topic detection. It differs fundamentally from the LDA-based methods in terms of topic definition, topic-document relationship, and learning method. It has been shown to discover significantly more coherent topics and better topic hierarchies. However, HLTA relies on the Expectation-Maximization (EM) algorithm for parameter estimation and hence is not efficient enough to deal with large datasets. In this paper, we propose a method to drastically speed up HLTA using a technique inspired by the advances in the method of moments. Empirical experiments show that our method greatly improves the efficiency of HLTA. It is as efficient as the state-of-the-art LDA-based method for hierarchical topic detection and finds substantially better topics and topic hierarchies.
 
",Progressive EM for Latent Tree Models and Hierarchical Topic Detection,https://www.semanticscholar.org/paper/4c40252c773268a82c8841f94e496fe9e5bf4a19,"Hierarchical topic models (HTMs)—especially those based on Bayesian deep learning—are gaining increasing attention from the ML community. However, in contrast to their ﬂat counterparts, their proper evaluation is rarely addressed. We propose several measures to evaluate HTMs in terms of their (branch-wise and layer-wise) topic hierarchy. We apply these measures to benchmark several HTMs on a wide range of datasets. We compare neural HTMs to traditional statistical HTMs in topic quality and interpretability. Our ﬁndings may help better judge advantages and disadvantages in different deep hierarchical topic models and drive future research in this area",Hierarchical Topic Evaluation: Statistical vs. Neural Models,https://www.semanticscholar.org/paper/f52899a633d55ecb802759d14f484fbc70bf4214,"Topic models provide an efficient way of extracting insights from text and supporting decision-making. Recently, novel methods have been proposed to model topic hierarchy or temporality. Modeling temporality provides more precise topics by separating topics that are characterized by similar words but located over distinct time periods. Conversely, modeling hierarchy provides a more detailed view of the content of a corpus by providing topics and sub-topics. However, no models have been proposed to incorporate both hierarchy and temporality which could be beneficial for applications such as environment scanning. Therefore, we propose a novel method to perform Hierarchical Topic Modelling Over Time (HTMOT). We evaluate the performance of our approach on a corpus of news articles using the Word Intrusion task. Results demonstrate that our model produces topics that elegantly combine a hierarchical structure and a temporal aspect. Furthermore, our proposed Gibbs sampling implementation shows competitive performance compared to previous state-of-the-art methods.",HTMOT: Hierarchical Topic Modelling over Time,https://www.semanticscholar.org/paper/16373c4907a40db094709935ea34fbda70145a53,"Hierarchical Topic Models (HTMs) are useful for discovering topic hierarchies in a collection of documents. However, traditional HTMs often produce hierarchies where lowerlevel topics are unrelated and not specific enough to their higher-level topics. Additionally, these methods can be computationally expensive. We present HyHTM - a Hyperbolic geometry based Hierarchical Topic Models - that addresses these limitations by incorporating hierarchical information from hyperbolic geometry to explicitly model hierarchies in topic models. Experimental results with four baselines show that HyHTM can better attend to parent-child relationships among topics. HyHTM produces coherent topic hierarchies that specialise in granularity from generic higher-level topics to specific lowerlevel topics. Further, our model is significantly faster and leaves a much smaller memory footprint than our best-performing baseline.We have made the source code for our algorithm publicly accessible.",HyHTM: Hyperbolic Geometry based Hierarchical Topic Models,https://www.semanticscholar.org/paper/0737ba8dc2ce8b606f9778c0e7667fbf7b6b67d4,"Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding,https://www.semanticscholar.org/paper/67afc9564e85a38c7f786b959614088a97d4787c,"This paper presents a tree-structured neural topic model, which has a topic distribution over a tree with an infinite number of branches. Our model parameterizes an unbounded ancestral and fraternal topic distribution by applying doubly-recurrent neural networks. With the help of autoencoding variational Bayes, our model improves data scalability and achieves competitive performance when inducing latent topics and tree structures, as compared to a prior tree-structured topic model (Blei et al., 2010). This work extends the tree-structured topic model such that it can be incorporated with neural models for downstream tasks.",Tree-Structured Neural Topic Model,https://www.semanticscholar.org/paper/4ab013772b898b56a2c2e290887d2a4694be5b5e,"Connectivity across documents often exhibits a hierarchical network structure. Hyperbolic Graph Neural Networks (HGNNs) have shown promise in preserving network hierarchy. However, they do not model the notion of topics, thus document representations lack semantic interpretability. On the other hand, a corpus of documents usually has high variability in degrees of topic specificity. For example, some documents contain general content (e.g., sports), while others focus on specific themes (e.g., basketball and swimming). Topic models indeed model latent topics for semantic interpretability, but most assume a flat topic structure and ignore such semantic hierarchy. Given these two challenges, we propose a Hyperbolic Graph Topic Modeling Network to integrate both network hierarchy across linked documents and semantic hierarchy within texts into a unified HGNN framework. Specifically, we construct a two-layer document graph. Intra- and cross-layer encoding captures network hierarchy. We design a topic tree for text decoding to preserve semantic hierarchy and learn interpretable topics. Supervised and unsupervised experiments verify the effectiveness of our model.",Hyperbolic Graph Topic Modeling Network with Continuously Updated Topic Tree,https://www.semanticscholar.org/paper/240f102c391c0f5ca5a67b090f271aeff52cf036,"One important task of topic modeling for text analysis is interpretability. By discovering structured topics one is able to yield improved interpretability as well as modeling accuracy. In this paper, we propose a novel topic model with a deep structure that explores both inter-topic and intra-topic structures informed by word embeddings. Specifically, our model discovers inter topic structures in the form of topic hierarchies and discovers intra topic structures in the form of sub-topics, each of which is informed by word embeddings and captures a fine-grained thematic aspect of a normal topic. Extensive experiments demonstrate that our model achieves the state-of-the-art performance in terms of perplexity, document classification, and topic quality. Moreover, with topic hierarchies and sub-topics, the topics discovered in our model are more interpretable, providing an illuminating means to understand text data.",Inter and Intra Topic Structure Learning with Word Embeddings,https://www.semanticscholar.org/paper/060a7ecf67ae5febedb1c22e29b9f8a880cf4f23,novel
Develop a hybrid gamification intervention that integrates AI-enhanced serious games and tabletop elements to improve rehabilitation engagement among adolescents recovering from sports injuries. This intervention will use AI to personalize game challenges based on individual recovery progress and physical capabilities.,"['Health Behavior Change', 'Gamification', 'Game Design', 'Artificial Intelligence', 'Serious Games', 'Rehabilitation', 'Human-Computer Interaction']","
 BACKGROUND
 Games, when used as interventional tools, can influence behavior change by incentivizing, reinforcing, educating, providing feedback loops, prompting, persuading, or providing meaning, fun, and community. However, not all game elements will appeal to all consumers equally, and different elements might work for different people and in different contexts.
 
 
 OBJECTIVE
 The aim of this study was to conduct a realist review of tabletop games targeting behavior change and to propose a framework for designing effective behavior change games.
 
 
 METHODS
 A realist review was conducted to inform program theory in the development of tabletop games for health behavior change. The context, mechanisms used to change behavior, and outcomes of included studies were reviewed through a realist lens.
 
 
 RESULTS
 Thirty-one papers met the eligibility criteria and were included in the review. Several design methods were identified that enhanced the efficacy of the games to change behavior. These included design by local teams, pilot testing, clearly defined targets of behavior change, conscious attention to all aspects of game design, including game mechanics, dynamics, aesthetics, and the elicitation of emotions. Delivery with other mediums, leveraging behavioral insights, prior training for delivery, and repeated play were also important. Some design elements that were found to reduce efficacy included limited replayability or lack of fun for immersive engagement.
 
 
 CONCLUSIONS
 Game designers need to consider all aspects of the context and the mechanisms to achieve the desired behavior change outcomes. Careful design thinking should include consideration of the game mechanics, dynamics, aesthetics, emotions, and contexts of the game and the players. People who know the players and the contexts well should design the games or have significant input. Testing in real-world settings is likely to lead to better outcomes. Careful selection and purposeful design of the behavior change mechanisms at play is essential. Fun and enjoyment of the player should be considered, as without engagement, there will be no desired intervention effect.
",Tabletop Board Game Elements and Gamification Interventions for Health Behavior Change: Realist Review and Proposal of a Game Design Framework (Preprint),https://www.semanticscholar.org/paper/8d052de2d798755f62c4fef0e74ad1098f340d95,"Background Games, when used as interventional tools, can influence behavior change by incentivizing, reinforcing, educating, providing feedback loops, prompting, persuading, or providing meaning, fun, and community. However, not all game elements will appeal to all consumers equally, and different elements might work for different people and in different contexts. Objective The aim of this study was to conduct a realist review of tabletop games targeting behavior change and to propose a framework for designing effective behavior change games. Methods A realist review was conducted to inform program theory in the development of tabletop games for health behavior change. The context, mechanisms used to change behavior, and outcomes of included studies were reviewed through a realist lens. Results Thirty-one papers met the eligibility criteria and were included in the review. Several design methods were identified that enhanced the efficacy of the games to change behavior. These included design by local teams, pilot testing, clearly defined targets of behavior change, conscious attention to all aspects of game design, including game mechanics, dynamics, aesthetics, and the elicitation of emotions. Delivery with other mediums, leveraging behavioral insights, prior training for delivery, and repeated play were also important. Some design elements that were found to reduce efficacy included limited replayability or lack of fun for immersive engagement. Conclusions Game designers need to consider all aspects of the context and the mechanisms to achieve the desired behavior change outcomes. Careful design thinking should include consideration of the game mechanics, dynamics, aesthetics, emotions, and contexts of the game and the players. People who know the players and the contexts well should design the games or have significant input. Testing in real-world settings is likely to lead to better outcomes. Careful selection and purposeful design of the behavior change mechanisms at play is essential. Fun and enjoyment of the player should be considered, as without engagement, there will be no desired intervention effect.",Tabletop Board Game Elements and Gamification Interventions for Health Behavior Change: Realist Review and Proposal of a Game Design Framework,https://www.semanticscholar.org/paper/5c3fb96b75a79b7d15ae568e75a9f4e51925f7cf,"Games are a successful pedagogical tool to change attitudes and behaviors. This chapter will examine how games facilitate change, discuss common pitfalls, and outline best practices for making serious games for clinical practice. Sustained engagement and motivation are key to lasting clinical interventions. When developing a game for clinical practice, the designer should avoid “punishing by rewards” (Kohn, 1993), damaging motivation towards the desired goal. Understanding game design principles is crucial to creating intrinsically engaging experiences that lead to lasting motivation. The Mechanics, Dynamics, and Aesthetics (MDA) framework is widely accepted by game designers as a framework to make compelling games. Using MDA as a base for understanding how to create engaging experiences, this chapter proposes a new framework for serious games called Mechanics, Dynamics, Aesthetics, and Outcomes. MDAO describes how to design a game that is intrinsically motivating and effective by focusing on the interplay between outcomes and other vectors of design.","Guidelines for Designing Effective Games as Clinical Interventions: Mechanics, Dynamics, Aesthetics, and Outcomes (MDAO) Framework",https://www.semanticscholar.org/paper/61b886482091631d93e1790ca813b6ae13f0024c,"Effective chronic disease management is essential to improve positive health outcomes, and incentive strategies are useful in promoting self-care with longevity. Gamification, applied with mHealth (mobile health) applications, has the potential to better facilitate patient self-management. This review article addresses a knowledge gap around the effective use of gamification design principles, or mechanics, in developing mHealth applications. Badges, leaderboards, points and levels, challenges and quests, social engagement loops, and onboarding are mechanics that comprise gamification. These mechanics are defined and explained from a design and development perspective. Health and fitness applications with gamification mechanics include: bant which uses points, levels, and social engagement, mySugr which uses challenges and quests, RunKeeper which uses leaderboards as well as social engagement loops and onboarding, Fitocracy which uses badges, and Mango Health, which uses points and levels. Specific design considerations are explored, an example of the efficacy of a gamified mHealth implementation in facilitating improved self-management is provided, limitations to this work are discussed, a link between the principles of gaming and gamification in health and wellness technologies is provided, and suggestions for future work are made. We conclude that gamification could be leveraged in developing applications with the potential to better facilitate self-management in persons with chronic conditions.",A game plan: Gamification design principles in mHealth applications for chronic disease management,https://www.semanticscholar.org/paper/f9be2234cbcb02cadb45c2994463cfecfab63a0c,"Background In health care, the use of game-based interventions to increase motivation, engagement, and overall sustainability of health behaviors is steadily becoming more common. The most prevalent types of game-based interventions in health care research are gamification and serious games. Various researchers have discussed substantial conceptual differences between these 2 concepts, supported by empirical studies showing differences in the effects on specific health behaviors. However, researchers also frequently report cases in which terms related to these 2 concepts are used ambiguously or even interchangeably. It remains unclear to what extent existing health care research explicitly distinguishes between gamification and serious games and whether it draws on existing conceptual considerations to do so. Objective This study aims to address this lack of knowledge by capturing the current state of conceptualizations of gamification and serious games in health care research. Furthermore, we aim to provide tools for researchers to disambiguate the reporting of game-based interventions. Methods We used a 2-step research approach. First, we conducted a systematic literature review of 206 studies, published in the Journal of Medical Internet Research and its sister journals, containing terms related to gamification, serious games, or both. We analyzed their conceptualizations of gamification and serious games, as well as the distinctions between the two concepts. Second, based on the literature review findings, we developed a set of guidelines for researchers reporting on game-based interventions and evaluated them with a group of 9 experts from the field. Results Our results show that less than half of the concept mentions are accompanied by an explicit definition. To distinguish between the 2 concepts, we identified four common approaches: implicit distinction, synonymous use of terms, serious games as a type of gamified system, and distinction based on the full game dimension. Our Game-Based Intervention Reporting Guidelines (GAMING) consist of 25 items grouped into four topics: conceptual focus, contribution, mindfulness about related concepts, and individual concept definitions. Conclusions Conceptualizations of gamification and serious games in health care literature are strongly heterogeneous, leading to conceptual ambiguity. Following the GAMING can support authors in rigorous reporting on study results of game-based interventions.",Conceptual Ambiguity Surrounding Gamification and Serious Games in Health Care: Literature Review and Development of Game-Based Intervention Reporting Guidelines (GAMING),https://www.semanticscholar.org/paper/6299a0c373093d3a025ec783d090bb1d84b8ee3f,"Participants in this hands-on workshop will learn the mechanics of clinically tested behavior change interventions, as well as techniques game designers use to motivate, engage and reward players through a game's lifecycle. A practical, step-by-step methodology will be introduced and built upon throughout this 4 hour course, resulting in a scalable framework and process for designing playful and practical behavior change games.",Beyond gamification: designing behavior change games,https://www.semanticscholar.org/paper/e46bfefbbd6beea199da6776b3f12027345f8199,"Background The idea of using serious games to effectuate better outcomes in health care has gained significant traction among a growing community of researchers, developers, and health care professionals. Many now recognize the importance of creating evidence-based games that are purposefully designed to address physical and mental health challenges faced by end users. To date, no regulatory resources have been established to guide the development of serious games for health (SGH). Developers must therefore look elsewhere for guidance. Although a more robust level of evidence exists in the research literature, it is neither structured nor is there any clear consensus. Developers currently use a variety of approaches and methodologies. The establishment of a well-defined framework that represents the consensus views of the SGH research community would help developers improve the efficiency of internal development processes, as well as chances of success. A consensus framework would also enhance the credibility of SGH and help provide quality evidence of their effectiveness. Objective This research aimed to (1) identify and evaluate the requirements, recommendations, and guidelines proposed by the SGH community in the research literature, and; (2) develop a consensus framework to guide developers, designers, researchers, and health care professionals in the development of evidence-based SGH. Methods A critical review of the literature was performed in October to November 2018. A 3-step search strategy and a predefined set of inclusion criteria were used to identify relevant articles in PubMed, ScienceDirect, Institute of Electrical and Electronics Engineers Xplore, CiteSeerX, and Google Scholar. A supplemental search of publications from regulatory authorities was conducted to capture their specific requirements. Three researchers independently evaluated the identified articles. The evidence was coded and categorized for analysis. Results This review identified 5 categories of high-level requirements and 20 low-level requirements suggested by the SGH community. These advocate a methodological approach that is multidisciplinary, iterative, and participatory. On the basis of the requirements identified, we propose a framework for developing theory-driven, evidence-based SGH. It comprises 5 stages that are informed by various stakeholders. It focuses on building strong scientific and design foundations that guide the creative and technical development. It includes quantitative trials to evaluate whether the SGH achieve the intended outcomes, as well as efforts to disseminate trial findings and follow-up monitoring after the SGH are rolled out for use. Conclusions This review resulted in the formulation of a framework for developing theory-driven, evidence-based SGH that represents many of the requirements set out by SGH stakeholders in the literature. It covers all aspects of the development process (scientific, technological, and design) and is transparently described in sufficient detail to allow SGH stakeholders to implement it in a wide variety of projects, irrespective of discipline, health care segments, or focus.","Developing Theory-Driven, Evidence-Based Serious Games for Health: Framework Based on Research Community Insights",https://www.semanticscholar.org/paper/f985a35d2b257d23cacd9e0aa8e3dcbaec9bd802,"Serious video games for health are designed to entertain while changing a specific health behavior. This article identifies behavioral principles that can guide the development of serious video games focused on changing a variety of health behaviors, including those attempting to decrease risk of obesity and type 2 diabetes. Guidelines discussed include how to develop video games that provide a solid foundation for behavior change by enhancing a player's knowledge and skill, ways in which personal mastery experiences can be incorporated into a video game environment, using game characters and avatars to promote observational learning, creating personalized experiences through tailoring, and the importance of achieving a balance between “fun-ness” and “seriousness.” The article concludes with suggestions for future research needed to inform this rapidly growing field.",Designing Serious Video Games for Health Behavior Change: Current Status and Future Directions,https://www.semanticscholar.org/paper/6363cf00a0e53cf8fcf004fcd9748e72060d9ae4,"The application of gaming as a method to change health behavior carries with it the burden to impact the consequences of morbidity and mortality. No-where, then, is the contention of games having a “serious” purpose more relevant than in the domain of serious games for health. Serious games are gaining profile as a potential strategy to educate the public about health in new and novel ways. Computer games represent an emerging approach to the continued research and development of health education and health promotion programs in the service of national health objectives. Research has indicated that many evidence-based health educa-abstract Serious games are gaining profile as a novel strategy to impact health behavior change in the service of national health objectives. Research has indicated that many evidence-based programs are effective because they are grounded in behavioral and motivational theories and models such as the PRECEDE model, the Health Belief Model, Social Cognitive Theory, the Theory of Reasoned Action, the Transtheoretical Model, Attribution Theory, and the ARCS model. Such theories assist in understanding health behavior problems, developing salient interventions, and evaluating their effectiveness. It follows, therefore, that serious games can be made optimally effective in changing health behavior if they are also informed by these theories. A successful intervention development framework (Intervention Mapping) provides a means to enable game developers to use theory to inform the design of effective games for health. This chapter describes useful theories and models for health game design, introduces the intervention mapping process, and describes a case study of a theory-and empirically-based serious health game intervention that has used these approaches and has been rigorously evaluated.",Application of behavioral theory in computer game design for health behavior change,https://www.semanticscholar.org/paper/42de0a0f3d8eb5770dbd65d3df5ca37c483a274e,"games can be effective tools for motivating healthy behaviors and/or attitudes, and so recent years have witnessed an increasing number of persuasive games. However, most games adopt a one-size-fits-all approach to persuasion in their design. Studies on gameplay and player motivation have shown that treating gamers as a monolithic group is a bad design approach because a motivational approach that works for one individual may actually demotivate the desired behavior in others. To correct this problem, we conducted a large-scale study on 1108 gamers, which examined the persuasiveness of ten Persuasive Technology (PT) strategies, and the receptiveness of seven gamer types identified by BrianHex to the strategies most commonly used in PT design. We developed models showing the receptiveness of the gamer types to the ten strategies and created persuasive profiles, which are lists of strategies that can be employed to motivate behavior for each gamer type. Although we studied and created our models using ten strategies, in this paper, we report results of five strategies.",Selecting Effective Strategies for Tailoring Persuasive Health Games to Gamer Types,https://www.semanticscholar.org/paper/5517617540ba1501fd34549ec3d72488f4ea4735,novel
"Develop a system **to enhance language model problem-solving** by integrating **exploration function generation with large language models**, allowing the model to dynamically generate and prune reasoning paths. This would enable it to explore multiple strategic options and eliminate less promising ones, thereby refining its problem-solving process. The system's effectiveness will be measured through **performance comparison in various scenarios** involving complex reasoning and planning tasks, such as advanced puzzle-solving and strategic game environments.","['Computer Science', 'Natural Language Processing', 'Large Language Models', 'Artificial Intelligence', 'Machine Learning', 'Problem Solving', 'Strategic Planning']","In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implemented a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. Our implementation of the ToT-based Sudoku solver is available on GitHub: \url{https://github.com/jieyilong/tree-of-thought-puzzle-solver}.",Large Language Model Guided Tree-of-Thought,https://www.semanticscholar.org/paper/bda605928d6ebe4db906e69ab5d343df75918727,"Large Language Models (LLMs) have achieved tremendous progress, yet they still often struggle with challenging reasoning problems. Current approaches address this challenge by sampling or searching detailed and low-level reasoning chains. However, these methods are still limited in their exploration capabilities, making it challenging for correct solutions to stand out in the huge solution space. In this work, we unleash LLMs' creative potential for exploring multiple diverse problem solving strategies by framing an LLM as a hierarchical policy via in-context learning. This policy comprises of a visionary leader that proposes multiple diverse high-level problem-solving tactics as hints, accompanied by a follower that executes detailed problem-solving processes following each of the high-level instruction. The follower uses each of the leader's directives as a guide and samples multiple reasoning chains to tackle the problem, generating a solution group for each leader proposal. Additionally, we propose an effective and efficient tournament-based approach to select among these explored solution groups to reach the final answer. Our approach produces meaningful and inspiring hints, enhances problem-solving strategy exploration, and improves the final answer accuracy on challenging problems in the MATH dataset. Code will be released at https://github.com/lz1oceani/LLM-As-Hierarchical-Policy.",Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving,https://www.semanticscholar.org/paper/c9a6aae4bedf6fd7a85b359e76137848265d4d1e,"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",Tree of Thoughts: Deliberate Problem Solving with Large Language Models,https://www.semanticscholar.org/paper/2f3822eb380b5e753a6d579f31dfc3ec4c4a0820,"While language models (LMs) have shown potential across a range of decision-making tasks, their reliance on simple acting processes limits their broad deployment as autonomous agents. In this paper, we introduce Language Agent Tree Search (LATS) -- the first general framework that synergizes the capabilities of LMs in reasoning, acting, and planning. By leveraging the in-context learning ability of LMs, we integrate Monte Carlo Tree Search into LATS to enable LMs as agents, along with LM-powered value functions and self-reflections for proficient exploration and enhanced decision-making. A key feature of our approach is the incorporation of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that surpasses the constraints of existing techniques. Our experimental evaluation across diverse domains, including programming, interactive question-answering (QA), web navigation, and math, validates the effectiveness and generality of LATS in decision-making while maintaining competitive or improved reasoning performance. Notably, LATS achieves state-of-the-art pass@1 accuracy (92.7%) for programming on HumanEval with GPT-4 and demonstrates gradient-free performance (average score of 75.9) comparable to gradient-based fine-tuning for web navigation on WebShop with GPT-3.5. Code can be found at https://github.com/lapisrocks/LanguageAgentTreeSearch",Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models,https://www.semanticscholar.org/paper/700bd9681f1b9e9e2212e10415d27b11c7e6836b,"Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\textit{world model}$ to predict the world $\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\underline{R}$easoning vi$\underline{a}$ $\underline{P}$lanning $\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.",Reasoning with Language Model is Planning with World Model,https://www.semanticscholar.org/paper/5dbffedcabe3fa43060ebbe2b1789500edfd871f,"Recent advancements in Large Language Models (LLMs) have revolutionized decision-making by breaking down complex problems into more manageable language sequences referred to as""thoughts"". An effective thought design should consider three key perspectives: performance, efficiency, and flexibility. However, existing thought can at most exhibit two of these attributes. To address these limitations, we introduce a novel thought prompting approach called""Everything of Thoughts""(XoT) to defy the law of""Penrose triangle of existing thought paradigms. XoT leverages pretrained reinforcement learning and Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge into thoughts, thereby enhancing LLMs' capabilities and enabling them to generalize to unseen problems efficiently. Through the utilization of the MCTS-LLM collaborative thought revision framework, this approach autonomously produces high-quality comprehensive cognitive mappings with minimal LLM interactions. Additionally, XoT empowers LLMs to engage in unconstrained thinking, allowing for flexible cognitive mappings for problems with multiple solutions. We evaluate XoT on several challenging multi-solution problem-solving tasks, including Game of 24, 8-Puzzle, and Pocket Cube. Our results demonstrate that XoT significantly outperforms existing approaches. Notably, XoT can yield multiple solutions with just one LLM call, showcasing its remarkable proficiency in addressing complex problems across diverse domains.",Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation,https://www.semanticscholar.org/paper/0639e2e209213ecb54eb4d6555e271d070344842,"Chain-of-Thought (CoT) prompting has proven to be effective in enhancing the reasoning capabilities of Large Language Models (LLMs) with at least 100 billion parameters. However, it is ineffective or even detrimental when applied to reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion parameters. To address this limitation, we introduce Dialogue-guided Chain-of-Thought (DialCoT) which employs a dialogue format to generate intermediate reasoning steps, guiding the model toward the final answer. Additionally, we optimize the model's reasoning path selection using the Proximal Policy Optimization (PPO) algorithm, further enhancing its reasoning capabilities. Our method offers several advantages compared to previous approaches. Firstly, we transform the process of solving complex reasoning questions by breaking them down into a series of simpler sub-questions, significantly reducing the task difficulty and making it more suitable for SLMs. Secondly, we optimize the model's reasoning path selection through the PPO algorithm. We conduct comprehensive experiments on four arithmetic reasoning datasets, demonstrating that our method achieves significant performance improvements compared to state-of-the-art competitors.",DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models,https://www.semanticscholar.org/paper/a83724fd55cd2bcf5583ca181373c34571ac1f73,"While the recently introduced Tree of Thoughts (ToT) has heralded advancements in allowing Large Language Models (LLMs) to reason through foresight and backtracking for global decision-making, it has overlooked the inherent local uncertainties in intermediate decision points or ""thoughts"". These local uncertainties, intrinsic to LLMs given their potential for diverse responses, remain a significant concern in the reasoning process. Addressing this pivotal gap, we introduce the Tree of Uncertain Thoughts (TouT) — a reasoning framework tailored for LLMs. Our TouT effectively leverages Monte Carlo Dropout to quantify uncertainty scores associated with LLMs’ diverse local responses at these intermediate steps. By marrying this local uncertainty quantification with global search algorithms, TouT enhances the model’s precision in response generation. We substantiate our approach with rigorous experiments on two demanding planning tasks: Game of 24 and Mini Crosswords. The empirical evidence underscores TouT’s superiority over both ToT and chain-of-thought prompting methods.",Tree of Uncertain Thoughts Reasoning for Large Language Models,https://www.semanticscholar.org/paper/875d71bae61a66f7e65a2b6d363b7a0a27a6ed25,"Recent works like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the reasoning capabilities of LLMs by using tree-search algorithms to guide multi-step reasoning. These methods rely on prompting a pre-trained model to serve as a value function and focus on problems with low search depth. As a result, these methods will not work in domains where the pre-trained LLM does not have enough knowledge to serve as an effective value function or in domains that require long-horizon planning. To address these limitations, we present an AlphaZero-like tree-search learning framework for LLMs (termed TS-LLM), systematically illustrating how tree-search with a learned value function can guide LLM decoding. TS-LLM distinguishes itself in two key ways. (1) Leveraging a learned value function and AlphaZero-like algorithms, our approach can be generally adaptable to a wide range of tasks, language models of any size, and tasks of varying search depths. (2) Our approach can guide LLMs during both inference and training, iteratively improving the LLM. Empirical results across reasoning, planning, alignment, and decision-making tasks show that TS-LLM outperforms existing approaches and can handle trees with a depth of 64.",Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training,https://www.semanticscholar.org/paper/e8df1cf6742b50a15500b8dd3dde3942e9c91418,"We introduce Graph of Thoughts (GoT): a framework that
advances prompting capabilities in large language models
(LLMs) beyond those offered by paradigms such as 
Chain-of-Thought or Tree of Thoughts (ToT). The key idea and 
primary advantage of GoT is the ability to model the information 
generated by an LLM as an arbitrary graph, where units of 
information (""LLM thoughts"") are vertices, and edges correspond
to dependencies between these vertices. This approach enables 
combining arbitrary LLM thoughts into synergistic outcomes, 
distilling the essence of whole networks of thoughts,
or enhancing thoughts using feedback loops. We illustrate
that GoT offers advantages over state of the art on different
tasks, for example increasing the quality of sorting by 62%
over ToT, while simultaneously reducing costs by >31%.
We ensure that GoT is extensible with new thought 
transformations and thus can be used to spearhead new prompting
schemes. This work brings the LLM reasoning closer to human 
thinking or brain mechanisms such as recurrence, both
of which form complex networks",Graph of Thoughts: Solving Elaborate Problems with Large Language Models,https://www.semanticscholar.org/paper/aade40af0d85b0b4fe15c97f6222d5c2e4d6d9b3,not novel
"Develop a system that uses **<b style=""background-color:#FFFACD;"">natural language processing with linked data</b>** to create enriched, interconnected profiles of scientists based on their publications and inferred research themes. This system would aim **<b style=""background-color:#F0FFF0;"">to boost scientific innovation</b>** by making it easier to identify and access diverse, high-quality scientific contributions across different languages and domains. **<b style=""background-color:#E6E6FA;"">Performance evaluation of annotation approach</b>** would be conducted to ensure the system effectively enhances the discoverability and accessibility of novel research work, fostering cross-disciplinary innovation and collaboration.","['Computer Science', 'Information Retrieval', 'Natural Language Processing', 'Recommender Systems', 'Scientific Innovation', 'Cross-Disciplinary Collaboration']","Motivation: Scientists increasingly rely on intelligent information systems to help them in their daily tasks, in particular for managing research objects, like publications or datasets. The relatively young research field of Semantic Publishing has been addressing the question how scientific applications can be improved through semantically rich representations of research objects, in order to facilitate their discovery and re-use. To complement the efforts in this area, we propose an automatic workflow to construct semantic user profiles of scholars, so that scholarly applications, like digital libraries or data repositories, can better understand their users' interests, tasks, and competences, by incorporating these user profiles in their design. To make the user profiles sharable across applications, we propose to build them based on standard semantic web technologies, in particular the Resource Description Framework (RDF) for representing user profiles and Linked Open Data (LOD) sources for representing competence topics.To avoid the cold start problem, we suggest to automatically populate these profiles by analyzing the publications (co-)authored by users, which we hypothesize reflect their research competences. Results: We developed a novel approach, ScholarLens , which can automatically generate semantic user profiles for authors of scholarly literature.For modeling the competences of scholarly users and groups, we surveyed a number of existing linked open data vocabularies. In accordance with the LOD best practices, we propose an RDF Schema (RDFS) based model for competence records that reuses existing vocabularies where appropriate. To automate the creation of semantic user profiles, we developed a complete, automated workflow that can generate semantic user profiles by analyzing full-text research articles through various natural language processing (NLP) techniques.In our method, we start by processing a set of research articles for ABSTRACT Motivation: Scientists increasingly rely on intelligent information systems to help them in their daily tasks, in particular for managing research objects, like publications or datasets. The relatively young research ﬁeld of Semantic Publishing has been addressing the question how scientiﬁc applications can be improved through semantically rich representations of research objects, in order to facilitate their discovery and re-use. To complement the efforts in this area, we propose an automatic workﬂow to construct semantic user proﬁles of scholars, so that scholarly applications, like digital libraries or data repositories, can better understand their users’ interests, tasks, and competences, by incorporating these user proﬁles in their design. To make the user proﬁles sharable across applications, we propose to build them based on standard semantic web technologies, in particular the Resource Description Framework (RDF) for representing user proﬁles and Linked Open Data (LOD) sources for representing competence topics. To avoid the cold start problem, we suggest to automatically populate these proﬁles by analyzing the publications (co-)authored by users, which we hypothesize reﬂect their research competences. Results: We developed a novel approach, ScholarLens , which can automatically generate semantic user proﬁles for authors of scholarly literature. For modeling the competences of scholarly users and groups, we surveyed a number of existing linked open data vocabularies. In accordance with the LOD best practices, we propose an RDF Schema (RDFS) based model for competence records that reuses existing vocabularies where appropriate. To automate the creation of semantic user proﬁles, we developed a complete, automated workﬂow that can generate semantic user proﬁles by analyzing full-text research articles through various natural language processing (NLP) we by by mining the and LOD entity linking steps. We then populate a knowledge base in with proﬁles competences. We implemented our as an source library and evaluated our system through two user studies, resulting in mean average precision (MAP) of to 95%. As part of the we also analyze the of on accuracy of resulting Finally, we demonstrate ﬁnd",ScholarLens: extracting competences from research publications for the automatic generation of semantic user profiles,https://www.semanticscholar.org/paper/cb7de3eafa6ce1be3579a760f53adac93c3c4875,"Isolated silos of scientific research and the growing challenge of information overload limit awareness across the literature and hinder innovation. Algorithmic curation and recommendation, which often prioritize relevance, can further reinforce these informational “filter bubbles.” In response, we describe Bridger, a system for facilitating discovery of scholars and their work. We construct a faceted representation of authors with information gleaned from their papers and inferred author personas, and use it to develop an approach that locates commonalities and contrasts between scientists to balance relevance and novelty. In studies with computer science researchers, this approach helps users discover authors considered useful for generating novel research directions. We also demonstrate an approach for displaying information about authors, boosting the ability to understand the work of new, unfamiliar scholars. Our analysis reveals that Bridger connects authors who have different citation profiles and publish in different venues, raising the prospect of bridging diverse scientific communities.",Bursting Scientific Filter Bubbles: Boosting Innovation via Novel Author Discovery,https://www.semanticscholar.org/paper/bea1c05b0584a4463cbfbcf4917d6afacaef0bde,"Scientific silos can hinder innovation. These information “filter bubbles” and the growing challenge of information overload limit awareness across the literature, making it difficult to keep track of even narrow areas of interest, let alone discover new ones. Al-gorithmic curation and recommendation, which often prioritize relevance, can further reinforce these bubbles. In response, we describe Bridger, a system for facilitating discovery of scholars and their work, to explore design tradeoffs among relevant and novel recommendations. We construct a faceted representation of authors using information extracted from their papers and inferred personas. We explore approaches both for recommending new content and for displaying it in a manner that helps researchers to understand the work of authors who they are unfamiliar with. In studies with computer science researchers, our approach substantially improves users’ abilities to do so. We develop an approach that locates commonalities and contrasts between scientists—retrieving partially similar authors, rather than aiming for strict similarity. We find this approach helps users discover authors useful for generating novel research ideas of relevance to their work, at a higher rate than a state-of-art neural model. Our analysis reveals that Bridger connects authors who have different citation profiles, publish in different venues, and are more distant in social co-authorship networks, raising the prospect of bridging diverse communities and facilitating discovery.",Bridger: Toward Bursting Scientific Filter Bubbles and Boosting Innovation via Novel Author Discovery,https://www.semanticscholar.org/paper/4d02f1a1b99a21665ef94bcdec733a3f8f4056b4,"Interdisciplinary studies often require researchers to explore literature in diverse branches of knowledge. Yet, navigating through the highly scattered knowledge from unfamiliar disciplines poses a significant challenge. In this paper, we introduce DiscipLink, a novel interactive system that facilitates collaboration between researchers and large language models (LLMs) in interdisciplinary information seeking (IIS). Based on users' topics of interest, DiscipLink initiates exploratory questions from the perspectives of possible relevant fields of study, and users can further tailor these questions. DiscipLink then supports users in searching and screening papers under selected questions by automatically expanding queries with disciplinary-specific terminologies, extracting themes from retrieved papers, and highlighting the connections between papers and questions. Our evaluation, comprising a within-subject comparative experiment and an open-ended exploratory study, reveals that DiscipLink can effectively support researchers in breaking down disciplinary boundaries and integrating scattered knowledge in diverse fields. The findings underscore the potential of LLM-powered tools in fostering information-seeking practices and bolstering interdisciplinary research.",DiscipLink: Unfolding Interdisciplinary Information Seeking Process via Human-AI Co-Exploration,https://www.semanticscholar.org/paper/ed1471ddca3ae6842367d299badc8fef14eca44a,"AMiner is the second generation of the ArnetMiner system. We focus on developing author-centric analytic and mining tools for gaining a deep understanding of the large and heterogeneous networks formed by authors, papers, venues, and knowledge concepts. One fundamental goal is how to extract and integrate semantics from different sources. We have developed algorithms to automatically extract researchers' profiles from the Web and re- solve the name ambiguity problem, and connect different professional networks. We also developed methodologies to incorporate knowledge from the Wikipedia and other sources into the system to bridge the gap between network science and the web mining research. In this talk, I will focus on answering two fundamental questions for author-centric network analysis: who is who? and who are similar to each other? The system has been in operation since 2006 and has collected more than 100,000,000 author profiles, 100,000,000 publication papers, and 7,800,000 knowledge concepts. It has been widely used for collaboration recommendation, similarity analysis, and community evolution.",AMiner: Mining Deep Knowledge from Big Scholar Data,https://www.semanticscholar.org/paper/169edf0ad0b5e3ab31df4481a9069cbfdb774423,"Rexplore leverages novel solutions in data mining, semantic technologies and visual analytics, and provides an innovative environment for exploring and making sense of scholarly data. Rexplore allows users: 1) to detect and make sense of important trends in research; 2) to identify a variety of interesting relations between researchers, beyond the standard co-authorship relations provided by most other systems; 3) to perform fine-grained expert search with respect to detailed multi-dimensional parameters; 4) to detect and characterize the dynamics of interesting communities of researchers, identified on the basis of shared research interests and scientific trajectories; 5) to analyse research performance at different levels of abstraction, including individual researchers, organizations, countries, and research communities.",Online The Open University ’ s repository of research publications and other research outputs Understanding research dynamics,https://www.semanticscholar.org/paper/cfe3906ad90ef852a29c60aa948f0b261f8da416,"The TRIPLE project (Targeting Researchers through Innovative Practices and Linked Exploration) is developing a discovery platform dedicated to the social sciences and humanities (SSH). It gathers 21 partners from 15 European countries and is funded under the European Commission programme INFRA EOSC-02-2019 “Prototyping new innovative services’’.The multicultural and multilingual GoTriple platform is harvesting three kinds of data available in nine languages (Croatian, English, French, German, Italian, Greek, Polish, Portuguese, and Spanish). The beta version, available from October 2021, gives access to more than 1.6 million publications and data sets from big aggregators and smaller repositories to cover all SSH disciplines. GoTriple also responds to requests from SSH researchers to foster interdisciplinarity and enhance collaboration. For this reason, GoTriple will make available researcher profiles and research projects. Five innovative services complement this approach by making available the so called Trust Building System, Recommender System, Visualization Tool, Open Annotation Tool, and Crowdfunding Platform.The TRIPLE project seeks to improve fairness of open science and ensure that research material is more accessible to SSH researchers and to society by (1) making SSH scientific publications available to as many people as possible without popularizing or simplifying the content and by (2) encouraging better connections between SSH and various sections of society (e.g. SMEs, Journalists, Policy Makers).The TRIPLE project has made use of a co-design approach from the very start (late 2019), which includes relevant stakeholders through interviews and workshops to ensure that their needs are understood and that the platform will meet their needs.The TRIPLE project has also developed an innovative approach to evaluate the success of the platform based on compass indicators of equality, diversity, and community building. These indicators will help to advance open science. Moreover, the project is currently engaging users to build a thriving community.The OPERAS research infrastructure, supporting open scholarly communication in the social sciences and humanities in the European Research Area will take an active part in the governance and sustainability of the platform.At the conference we will present:▸ some aspects of the research underpinning the creation of GoTriple, including the Co-design work,▸ the key functionalities of GoTriple and how they can benefit research in the humanities,▸ how the platform will ensure discoverability and accessibility of SSH resources and highlight European diversity in terms of culture, language, and practices, and▸ the complexities of building new technologies serving the humanities.","GoTriple, a central access point for the social sciences and humanities",https://www.semanticscholar.org/paper/7732e9d5cad05d64112ba48d14f6d1f0c9f7fde2,"Scientific journals can capture a scholar’s research career. A researcher’s publication data often reflects his/her research interests and their social relations. It is demonstrated that scientist collaboration networks can be constructed based on co-authorship data from journal papers. The problem with such a network is that researchers are limited within their professional social network. This work proposes the idea of constructing a researcher’s social network based on data harvested from metadata of scientific publications and personal online profiles. We hypothesize that data, such as, publication keywords, personal interests, the themes of the conferences where papers are published, and co-authors of the papers, either directly or indirectly represent the authors’ research interests, and by measuring the similarity between these data we are able to construct a researcher social network. Based on the four types of data mentioned above, social network graphs were plotted, studied and analyzed. These graphs were then evaluated by the researchers themselves by giving ratings. Based on this evaluation, we estimated the weight for each type of data, in order to blend all data together to construct one ideal researcher’s social network. Interestingly, our results showed that a graph based on publication’s keywords were more representative than the one based on publication’s co-authorship. The findings from the evaluation were used to propose a dynamic social network data model.",The Researcher Social Network: A Social Network Based on Metadata of Scientific Publications,https://www.semanticscholar.org/paper/5485472657dd978ceb920317367124e608bac8c4,"Finding potential collaborators has become a challenge due to the growing number of scientists in organizations such as universities, research institutes, or companies. Collaboration Recommendation Systems (CRSs) have been developed to help researchers identify possible collaboration partners, but they often rely on citation graphs or paper abstracts which may not be readily available in organizational databases or online sources. However, scientific publication titles provide consistent bibliometric data that can provide insights into research areas. TOMOSCO is a topic modelling framework that uses transformer-based methods to extract research area information from small amounts of text, such as publication titles or brief project descriptions. TOMOSCO can classify, cluster, and match research topics across different disciplines, uncovering relationships among scientists and suggesting potential interdisciplinary collaborations. In experiments, TOMOSCO was able to identify existing collaborations with over 90% accuracy based solely on publication titles and propose new collaborations based on previously unseen publications and project descriptions.",Exploiting Topic Modelling for the Identification of Untapped Scientific Collaborations,https://www.semanticscholar.org/paper/ed12a2ed110c00c70a57b78fc476fda98b1231ca,"In the extensive recommender systems literature, novelty and diversity have been identified as key properties of useful recommendations. However, these properties have received limited attention in the specific sub-field of research paper recommender systems. In this work, we argue for the importance of offering novel and diverse research paper recommendations to scientists. This approach aims to reduce siloed reading, break down filter bubbles, and promote interdisciplinary research. We propose a novel framework for evaluating the novelty and diversity of research paper recommendations that leverages methods from network analysis and natural language processing. Using this framework, we show that the choice of representational method within a larger research paper recommendation system can have a measurable impact on the nature of downstream recommendations, specifically on their novelty and diversity. We introduce a novel paper embedding method, which we demonstrate offers more innovative and diverse recommendations without sacrificing precision, compared to other state-of-the-art baselines.",The Role of Document Embedding in Research Paper Recommender Systems: To Breakdown or to Bolster Disciplinary Borders?,https://www.semanticscholar.org/paper/3ff3765268e076e14cc3a092f35506c3cb300833,not novel
"Develop a gamified platform that integrates AI to personalize Computational Thinking (CT) exercises in science education, and evaluate its impact on student engagement and learning outcomes.","['Education Technology', 'Gamification', 'Artificial Intelligence', 'Computational Thinking', 'Science Education']","Over the last 10 years, research on gamification, the use of game elements in non-game contexts, has increased in the field of education, due to its potential to enhance learning performance. Yet, the majority of available research rather focuses on the evaluation of motivation and engagement as key dependent variables. Hence, the purpose of this study is to review available studies on gamification, with an exclusive focus on learning performance as the key dependent variable. Through a systematic search and selection process, building on ""Web of Science"" articles and by considering studies between 2000 and 2016 related to gamification and learning, 582 articles were identified. Further inclusion and exclusion criteria, regarding setting (education), study focus (empirical), journal access (full access) and dependent variables (learning performance), resulted in a review of 23 articles meeting the criteria. The analysis of these articles showed how gamification could be linked to a direct increase in learning performance of students. Nevertheless, some studies also reflect weaker statistical differences between being involved or not in a gamified environment. The review analysis results are especially helpful to define a future agenda for gamification research, addressing the following gaps in the literature. First, include mediating and moderating variables to find more empirical research that can prove an indirect effect of gamification on learning performance. Second, carry out additional research that empirically underpins the direct linkage between gamification and learning performance. Third, include specific individual gamification elements to be able to determine explicit differential effects of these elements on learning performance. Fourth, conduct research in a broader range of knowledge fields to develop empirical evidence in the context of other knowledge domains next to computer sciences. Finally, consider involving larger sample and setting up longer experimental interventions, to avoid novelty effects and risks of lack of generalization.",Gamification and learning performance : a systematic review of the literature,https://www.semanticscholar.org/paper/287c795f6c348b10923a18c7e230ea21772a6c80,"Gamification is a term that refers to the use of game elements in non-game contexts with the goal of engaging people in a variety of tasks. There is a growing interest in gamification as well as its applications and implications in the field of Education since it provides an alternative to engage and motivate students during the process of learning. Despite this increasing interest, to the best of our knowledge, there are no studies that cover and classify the types of research being published and the most investigated topics in the area. As a first step towards bridging this gap, we carried out a systematic mapping to synthesize an overview of the area. We went through 357 papers on gamification. Among them, 48 were related to education and only 26 met the criteria for inclusion and exclusion of articles defined in this study. These 26 papers were selected and categorized according to their contribution. As a result, we provide an overview of the area. Such an overview suggests that most studies focus on investigating how gamification can be used to motivate students, improve their skills, and maximize learning.",A systematic mapping on gamification applied to education,https://www.semanticscholar.org/paper/2e35876bd2ccd680f8628d61fb8faadbd26f0eba,"Gamification, the application of game elements to non-game settings, continues to grow in popularity as a method to increase student engagement in the classroom. We tested students across two courses, measuring their motivation, social comparison, effort, satisfaction, learner empowerment, and academic performance at four points during a 16-week semester. One course received a gamified curriculum, featuring a leaderboard and badges, whereas the other course received the same curriculum without the gamified elements. Our results found that students in the gamified course showed less motivation, satisfaction, and empowerment over time than those in the non-gamified class. The effect of course type on students' final exam scores was mediated by students' levels of intrinsic motivation, with students in the gamified course showing less motivation and lower final exam scores than the non-gamified class. This suggests that some care should be taken when applying certain gamification mechanics to educational settings. © 2014 Elsevier Ltd. All rights reserved.","Assessing the effects of gami fi cation in the classroom : A longitudinal study on intrinsic motivation , social comparison , satisfaction , effort , and academic performance",https://www.semanticscholar.org/paper/0e10b3ab96fbe41cc1ec24542cf111acd8b4a0a6,"This manuscript reports the effects of gamification elements on primary and college students’ motivation and learning. This mixed methods research reports two years of data collection of primary and college students’ reflections, preand post-test results, and survey results as well as observations made by the author. Business communication classes and mathematics classes experienced gamification by introducing game elements and frameworks in the classrooms while maintaining the integrity of the learning outcomes. Students expressed increased motivation and engagement at both the primary and college level as well as improved learning.",Gamification Effects on Motivation and Learning: Application to Primary and College Students,https://www.semanticscholar.org/paper/fe3aa0e8f080189c32635920b45a92063f5758c7,"In this paper, we reflected our experience on using gamification platform to facilitate pre-service teachers to engage in different approach for improving teaching knowledge and skills. The platform called “Berguru” (http://berguru.net) was developed to build strong and meaningful connection between students in pre-service teacher education program with master or real teachers in school institution. 34 students were involved in this experiment and trained to use the platform in a very narrow time to understand how adaptive the platform was. These students came from cross different teaching subjects, i.e. economics, engineering, science, and literacy. We measured and examined their responses based on the interaction occurred naturally during the process. These responses are indicated through several aspects, such as layout design, color composition, accessibility, control, clarity of purpose, personalization, and gamification features. By using Likert scale on a single questionnaire and accompanied by short interview to some students, it revealed that the gamification platform was highly adaptive, helpful, and potential to offer students a new engaging way of learning experience.",Engagement Experiences on Using Gamified Platform in Pre-service Teacher Education,https://www.semanticscholar.org/paper/1173bbde714ecf5239d2e741147493584346fa5d,"Recent research shows that gamification is a valuable tool to improve students’ learning effectiveness. However, its application continues to be limited. Educators remain reluctant to use games due to factors like limited resources, game complexity, inadaptability to various learning-outcomes, weak student involvement, and difficulty to integrate in course structures. In this article we argue that, when purposefully designed, educational games can address those factors that hinder adoption. Accordingly, we identified seven design principles that, if satisfied, are expected to yield educational games that are useful to both educators and students. The principles accentuate the importance of designing educational platforms over which games can be created and played. Game platforms must (1) adapt to various educational purposes, (2) enable educators control over student engagement (switching on/off game features like rewards, personifications, etc.), (3) scale up/down to achieve the desired level of complexity, and (4) maintain student arousal by dynamically balancing the challenge level with skill level. To evaluate the design principles, we intend to design a business game platform with educators that reflect the seven proposed principles and evaluate it in class settings. If fruitful, this research will advance extant knowledge on learning strategies and specifically the design of educational games.",A Design Science Approach To Gamify Education: From Games To Platforms,https://www.semanticscholar.org/paper/73d089b1131c9163824c1cbc78709efa33371406,"Aim/Purpose: This study reviewed previous research on the role of gamification techniques in promoting students’ learning.

Background: The role of gamification in promoting students’ learning has been investigated empirically by many scholars. To date, mixed results about the effectiveness of gamification have been reported, and researchers frequently argue that the inappropriateness of certain techniques may have contributed to these mixed findings.

Methodology: The Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) protocol was used to assess the criteria required for this review. A total of 40 studies were identified and included in the systematic review. The selected studies were used to assess the association between certain gamification techniques and students’ learning in this study.

Findings: The results showed that gamification techniques differently affect students’ learning. In addition, it is important for students to be instructed about the application of gamification approach before they engage in a gamified learning task. The key challenges relating to the use of gamification techniques were also discussed.

Recommendations for Practitioners: This review can help educational decision makers and practitioners to stimulate certain learning outcomes of the students with the help of specific gamification techniques. 

",The Role of Gamification Techniques in Promoting Student Learning: A Review and Synthesis,https://www.semanticscholar.org/paper/e0fe048ad57710e1f3b1edec2a8c61aefb05208f,"Gamification systems have the potential to foster students’ engagement and enhance their learning performance. Although the literature to date has provided relevant contributions in explaining how gamification design can be effective, several studies seek to address the constructs used to measure the effects of gamification. To analyze how gamification outcomes should be properly measured, this paper systematically reviews the literature and provides a map of the most frequent scales used to measure gamification outcomes in the educational context. As research findings, we identify motivation, engagement, self-efficacy, and flow/cognitive absorption as the primary constructs addressed to experiential outcomes. Additionally, there are research opportunities to develop a better understanding of the effects of extrinsic motivation rewards on experiential outcomes and problem-solving transfer posited as instrumental outcome.",Using Gamification in Education: A Systematic Literature Review,https://www.semanticscholar.org/paper/7927ae9430e3045a3bbf921de6cc93c5df898cc9,"Gamification is the application of game features, mainly video game elements, into non-game context for the purpose of promoting motivation and engagement in learning. The application of gamification in a pedagogical context provides some remedy for many students who find themselves alienated by traditional methods of instruction. The use of gamification could provide a partial solution to the decline in learners’ motivation and engagement the schooling system is facing today. Specifically, the college environment could benefit a lot from gamifying not only their graduate recruitment strategies, but also the college course content and curricula. This critical analysis of literature on gamification is intended to be part of a sequence on the effect of gamification on motivation and engagement. A proposed methodology in the study of gamification effect on motivation and engagement in addition to an empirical study on three college courses are being finalized to complete this trilogy. The paper aims to discuss these issues.,Themes covered in the literature review include: conceptualizing gamification, advantages of gamification over game-based learning, theoretical connections to gamification, motivation and engagement, connecting gamification to motivation and engagement, emotions and fun in gamification, player types and gamification features, gamification in action, and implementation guidelines.,The literature on the effect of gamification on motivation and gamification is still limited on multiple levels. There is a gap between theory and practice in the study of gamification. There is limited literature on the implementation guidelines of the gamified designs.,This critical analysis of literature is followed by connecting it to future research by the same author as part of a sequence on the effect of gamification on motivation and engagement. The second project, will be proposing a methodology for any successful design to provide a holistic understanding of the topic of gamification. Finally, an empirical study on the effect of gamification on students’ motivation and engagement in three college courses will be submitted to complete the trilogy.,This paper is a literature review, so there is a strong connection to literature on this topic. However, the synthesis of the themes and ideas are original. The literature review is extensive and covers the different aspects of the topic of gamification and its relationship to motivation and engagement.",The effect of gamification on motivation and engagement,https://www.semanticscholar.org/paper/7c2c23195bcef0d90cbef64378de7a641e5e69a6,"Gamification has received increased attention in education in recent years, and is seen as a way to improve student engagement, motivation, attendance, and academic performance. While empirical studies on gamification in higher education are showing modest gains in some areas, this data can be difficult to interpret because of the many ways that gamification can be designed and implemented. Gamification is also controversial for appearing exploitative, seeming oversimplified, and having the tendency to rely on extrinsic motivation and learning analytics that may not translate to student learning. This paper provides a brief overview of gamification in higher education and looks at findings from recent empirical studies. It then examines its key criticisms as well as its potential contributions to improving instructional design in higher education. A practical example and a set of recommendations are provided to show how instructors new to gamification and interested in implementing it can adapt it for their courses. Gamification is the application of game mechanics in a non-gaming context (Deterding, Dixon, Khaled, & Nacke, 2011; Zichermann & Cunningham, 2011). Typically, this involves the use of mechanisms such as quests, levels, badges, points, leaderboards, virtual goods, avatars, narratives, and progress bars, used in isolation or in various combinations. Other forms of gamification draw on design principles inspired by digital games, such as giving students the freedom to fail and retry a task without penalty, and freedom to choose activities and learning pathways that best suit their interests. The growing interest in learning analytics (Dietz-Uhler & Hurn, 2013; Dyckhoff, Zielke, Bültmann, Chatti, & Schroeder, 2012) and big data generated by learning management systems (LMS) has also made gamification a potential way to leverage this data to inform instructional design and improve student performance. Gamification overlaps with other game-related educational interventions, including game-based learning, serious games, and learning by design, each of which values different aspects of games and tends to approach games and learning from different points of departure. Game-based learning involves learning by playing games, either ones developed specifically for education or commercial games seen to have educational value (Squire, 2005, 2011). Serious games tend to focus on raising awareness on social issues (Sanford, Starr, Merkel, & Kurki, 2015) or improving lifestyles, for example, by teaching players how to improve health, control addiction, and boost nutrition. Learning by design involves having students design games in order to learn about complex issues and to promote systems thinking (Kafai, 1995, 2006). The influence of gamification on education has been promising but controversial. On the one hand, gamification has seen its share of success in areas such as business and marketing, for example, Journal of Interactive Online Learning Hung 58 with frequent flier miles and loyalty points (Burke, 2014). On the other hand, its use has been criticized, even by game designers themselves, as a form of exploitation (Bogost, 2011b) and an over-simplified approach to game design (Robertson, 2010). Empirical studies on gamification in higher education have been growing, and the results have been mixed to positive (Dicheva, Dichev, Agre, & Angelova, 2015; Nah, Zeng, Telaprolu, Ayyappa, & Eschenbrenner, 2014; Wiggins, 2016). However, it is hard to interpret the implications of these studies because of how broadly gamification is defined and implemented. The present article focuses on gamification in higher education. It begins with a brief overview of gamification and the empirical studies so far that show its impact on higher education. It then discusses the main criticisms and concerns raised against gamification and its implementation. The article then examines ways that gamification can improve instructional practice when gamification is designed meaningfully with a user-centered approach, drawing on the author’s own experience of using it as a way to inform instructional design. A Review of Literature Defining Gamification Gamification, game-based learning, serious games, and learning by design can be seen as different ways of addressing the question: What is the most effective way to use games in the classroom? Is it the “game-ness” of the game that holds the most potential, or the encouragement of playful behaviors that are conducive to learning? Proponents of gamification focus primarily on game mechanics, which are the building blocks of games. The aim is not to design a full-fledged game; instead, the goal is to harness these mechanics to encourage and reward behaviors that support learning and foster productive social interactions. One way is to add coherence and purpose by giving the course a narrative or designing quests that students have to complete in order to show their competence (Kapp, 2012). This is also to give the course a better sense of direction and relevance because the activities are directly related to a larger storyline (Bartel, Figas, & Hagel, 2015). Ideally, there would also be different kinds of quests that students can choose from, depending on their interests and preferences, giving them multiple pathways to reach the same goal. For example, Sheldon (2011) re-designed his university course by turning grades into experience points (XP) and student groups into guilds. XP corresponded to levels, which in turn corresponded to grades. Students were greeted on the first day of class with an F but were told they could level up by completing required and optional assignments. Students also had the opportunity to re-do assignments for rescoring, just as they could with a digital game. Gamification can also create cooperation and/or competition, where individuals and teams compete for finite resources, levels, badges, and points. Leaderboards can further emphasize competition by displaying how students are ranked, letting them compare themselves against their fellow classmates (de Byl, 2013). These rewards can also serve as a form of feedback for students to get a sense of where they are in the class. Projects such as Open Badges extend gamification beyond the classroom by allowing students to connect their earned badges to professional social networks such as LinkedIn for potential employees to see. Empirical Studies Most empirical studies on gamification and higher education so far have focused on enhancing the visible status of students by displaying achievements with points, levels, badges, Journal of Interactive Online Learning Hung 59 and leaderboards (Dicheva et al., 2015). Other priorities include improving social engagement, giving students the freedom to fail and re-do assignments, giving students more choice, and providing faster feedback. Since most of these studies were conducted in computer science (CS) or game design courses, some of the researchers were able to program customized scripts to process data logged by the LMS and generate information based on student logins, page visits, and so on. Gamification plug-ins for LMSs also allow instructors to gamify the LMS, for example, by designing badges that would be automatically awarded to students who have met the criteria set by the instructor. Third-party platforms designed with gamification in mind (e.g. Classcraft) or have gamification plug-ins (e.g., Wordpress) have also been used in these studies. In general, these studies suggest a positive response to gamification from the students (Dicheva et al., 2015; Nah et al., 2014; Wiggins, 2016), with most improvements seen in attendance, participation, and motivation (Barata, Gama, Jorge, & Gonçalves, 2013; Caton & Greenhill, 2014; Mitchell, Danino, & May, 2013; O’Donovan, Gain, Marais, Donovan, & Marais, 2013). Other studies show a more mixed response, with some students finding the gamification too complex or overly competitive (Berkling & Thomas, 2013; Domínguez et al., 2013; Haaranen, Ihantola, Hakulinen, & Korhonen, 2014). Barata, Gama, Jorge and Gonçalves (2014) suggest that different types of students may be drawn to gamification in different ways, with “achievers” being the most proactive and engaged, “disheartened” being those who start strong and lose interest along the way, and “underachievers” showing low levels of participation, least engagement, and poorer performance. There are a few challenges in interpreting what these studies mean. Firstly, gamification is broad. For example, badges, a common form for gamification, can be designed and implemented in any number of ways. Game mechanics can also be implemented in combination with other mechanics, making it hard to isolate what aspect of a gamified class had the most impact. Furthermore, a given course can be gamified to varying degrees. Gamification can be an additional layer or it can be deeply integrated into every part of the course. Secondly, it takes a lot of effort to design and implement gamification, and even more to get it to work well (Nicholson, 2013; O’Donovan et al, 2013). Even the studies that show improvements in student motivation and engagement admit that there is little to no impact on student grades (Barata et al., 2013). If the choice is between a class that awards badges and one that does not, it is unsurprising that students would prefer one that does. An important question is whether the positive student responses also translate to other improvements and/or lead to long-term benefits. Thirdly, since most of these studies have been conducted with students in CS, game design, and engineering schools (Dicheva et al., 2015), it is important to consider whether gamification will have the same impact in other disciplines. Students studying to become computer programmers and game designers are likely to be more familiar with games and systems thinking, and, therefore, are more comfortable with g",A Critique and Defense of Gamification.,https://www.semanticscholar.org/paper/d8f4250fc81583e00f28f797da6f619cd81cd9d4,novel
"To enhance the discovery of novel research directions across diverse scientific fields, develop an interactive system that integrates Bridger with a cross-disciplinary knowledge graph. This system would map connections between different scientific domains and highlight potential interdisciplinary collaborations. Evaluate the system through a longitudinal study involving researchers from various fields, measuring the impact on the generation of interdisciplinary research projects and publications.","['Computer Science', 'Human-Computer Interaction', 'Recommender Systems', 'Information Retrieval', 'Scientometrics', 'Interdisciplinary Research']","Scientific silos can hinder innovation. These information “filter bubbles” and the growing challenge of information overload limit awareness across the literature, making it difficult to keep track of even narrow areas of interest, let alone discover new ones. Al-gorithmic curation and recommendation, which often prioritize relevance, can further reinforce these bubbles. In response, we describe Bridger, a system for facilitating discovery of scholars and their work, to explore design tradeoffs among relevant and novel recommendations. We construct a faceted representation of authors using information extracted from their papers and inferred personas. We explore approaches both for recommending new content and for displaying it in a manner that helps researchers to understand the work of authors who they are unfamiliar with. In studies with computer science researchers, our approach substantially improves users’ abilities to do so. We develop an approach that locates commonalities and contrasts between scientists—retrieving partially similar authors, rather than aiming for strict similarity. We find this approach helps users discover authors useful for generating novel research ideas of relevance to their work, at a higher rate than a state-of-art neural model. Our analysis reveals that Bridger connects authors who have different citation profiles, publish in different venues, and are more distant in social co-authorship networks, raising the prospect of bridging diverse communities and facilitating discovery.",Bridger: Toward Bursting Scientific Filter Bubbles and Boosting Innovation via Novel Author Discovery,https://www.semanticscholar.org/paper/4d02f1a1b99a21665ef94bcdec733a3f8f4056b4,"Isolated silos of scientific research and the growing challenge of information overload limit awareness across the literature and hinder innovation. Algorithmic curation and recommendation, which often prioritize relevance, can further reinforce these informational “filter bubbles.” In response, we describe Bridger, a system for facilitating discovery of scholars and their work. We construct a faceted representation of authors with information gleaned from their papers and inferred author personas, and use it to develop an approach that locates commonalities and contrasts between scientists to balance relevance and novelty. In studies with computer science researchers, this approach helps users discover authors considered useful for generating novel research directions. We also demonstrate an approach for displaying information about authors, boosting the ability to understand the work of new, unfamiliar scholars. Our analysis reveals that Bridger connects authors who have different citation profiles and publish in different venues, raising the prospect of bridging diverse scientific communities.",Bursting Scientific Filter Bubbles: Boosting Innovation via Novel Author Discovery,https://www.semanticscholar.org/paper/bea1c05b0584a4463cbfbcf4917d6afacaef0bde,"Rexplore leverages novel solutions in data mining, semantic technologies and visual analytics, and provides an innovative environment for exploring and making sense of scholarly data. Rexplore allows users: 1) to detect and make sense of important trends in research; 2) to identify a variety of interesting relations between researchers, beyond the standard co-authorship relations provided by most other systems; 3) to perform fine-grained expert search with respect to detailed multi-dimensional parameters; 4) to detect and characterize the dynamics of interesting communities of researchers, identified on the basis of shared research interests and scientific trajectories; 5) to analyse research performance at different levels of abstraction, including individual researchers, organizations, countries, and research communities.",Online The Open University ’ s repository of research publications and other research outputs Understanding research dynamics,https://www.semanticscholar.org/paper/cfe3906ad90ef852a29c60aa948f0b261f8da416,"Expanding a set of known experts with new ones that share similar expertise is a problem that emerges in various real-life applications. We demonstrate VeTo-web, an open source, publicly available tool that deals with this problem in the context of searching for academic experts. VeTo-web exploits analysis techniques for scholarly knowledge graphs to identify scholars that share similar research activities with a given expert group and offers a Web-based user interface to assist its users in expanding a set of academic experts with additional scholars with similar expertise.",VeTo-web: A Recommendation Tool for the Expansion of Sets of Scholars,https://www.semanticscholar.org/paper/1551d4c248f9081a83050c2af20e703c9af44940,"AMiner is the second generation of the ArnetMiner system. We focus on developing author-centric analytic and mining tools for gaining a deep understanding of the large and heterogeneous networks formed by authors, papers, venues, and knowledge concepts. One fundamental goal is how to extract and integrate semantics from different sources. We have developed algorithms to automatically extract researchers' profiles from the Web and re- solve the name ambiguity problem, and connect different professional networks. We also developed methodologies to incorporate knowledge from the Wikipedia and other sources into the system to bridge the gap between network science and the web mining research. In this talk, I will focus on answering two fundamental questions for author-centric network analysis: who is who? and who are similar to each other? The system has been in operation since 2006 and has collected more than 100,000,000 author profiles, 100,000,000 publication papers, and 7,800,000 knowledge concepts. It has been widely used for collaboration recommendation, similarity analysis, and community evolution.",AMiner: Mining Deep Knowledge from Big Scholar Data,https://www.semanticscholar.org/paper/169edf0ad0b5e3ab31df4481a9069cbfdb774423,"As communities of researchers continue to become quite large and to grow incessantly, collaboration among researchers can be conducive to greater research pro- ductivity. Nevertheless, it is difficult for a researcher to find suitable collaborators from all researchers around the world. In this paper, we have used bibliographic DBLP data to extract information of a researcher and to discover the relationship between the co-authors and between authors and conferences. We evaluated some of the similarity measures and developed an innovative random walk model to find potential co-authors for a given researcher. These measures were then used to design a best model to rec- ommend co-authors. We have also applied an HITS algorithm and proposed a ranking algorithm to rank researchers and conferences with the intent of recommending authors or conferences.",Similarity-based Complex Publication Network Analytics for Recommending Potential Collaborations,https://www.semanticscholar.org/paper/f40931c7b6a8a455c9b49884f1d58e05f0e84a9b,"Successful research collaborations may facilitate major outcomes in science and their applications. Thus, identifying effective collaborators may be a key factor that affects success. However, it is very difficult to identify potential collaborators and it is particularly difficult for young researchers who have less knowledge about other researchers and experts in their research domain. This study introduces and defines the problem of collaborator recommendation for 'isolated' researchers who have no links with others in co author networks. Existing approaches such as link-based and content-based methods may not be suitable for isolated researchers because of their lack of links and content information. Thus, we propose a new approach that uses additional information as new features to make recommendations, i.e., the strength of the relationship between organizations, the importance rating, and the activity scores of researchers. We also propose a new method for evaluating the quality of collaborator recommendations. We performed experiments by crawling publications from the Microsoft Academic Search Web site. The metadata were extracted from these publications, including the year, authors, organizational affiliations of authors, citations, and references. The metadata from publications between 2001 and 2005 were used as the training data while those from 2006 to 2011 were used for validation. The experimental results demonstrated the effectiveness and efficiency of our proposed approach.",Collaborator Recommendation for Isolated Researchers,https://www.semanticscholar.org/paper/361a20cbd78548b4073b890279b6dad81e09bc0a,"Collaboration between research scientists, particularly those with diverse backgrounds, is a driver of scientific innovation. However, finding the right collaborator is often an unscientific process that is subject to chance. This paper explores recommending collaborators based on repeating patterns of previous successful collaboration experiences, what we term prototypical collaborations. We investigate a method for discovering such prototypes to use them as a basis to guide the recommendation of new collaborations. To this end, we also examine two methods for matching collaboration seekers to these prototypical collaborations. Our initial studies reveal that though promising, improving collaborations through recommendation is a complex goal.",Discovering Patterns of Collaboration for Recommendation,https://www.semanticscholar.org/paper/d14bf8dc82ec9715fd08505924876c9863b62e7d,"Unlike expertise location systems which users query actively when looking for an expert, expert recommender systems suggest individuals without the context of a specific problem. An interesting research question is whether expert recommender systems should consider a users' social context when recommending potential research collaborators. One may argue that it might be easier for scientists to collaborate with colleagues in their social network, because initiating collaboration with socially unconnected researchers is burdensome and fraught with risk, despite potentially relevant expertise. However, many scientists also initiate collaborations outside of their social network when they seek to work with individuals possessing relevant expertise or acknowledged experts. In this paper, we studied how well content-based, social and hybrid recommendation algorithms predicted co-author relationships among a random sample of 17,525 biomedical scientists. To generate recommendations, we used authors' research expertise inferred from publication metadata and their professional social networks derived from their co-authorship history. We used 80% of our data set (articles published before 2007) as our training set, and the remaining data as our test set (articles published in 2007 or later). Our results show that a hybrid algorithm combining expertise and social network information outperformed all other algorithms with regards to Top 10 and Top 20 recommendations. For the Top 2 and Top 5 recommendations, social network-based information alone generated the most useful recommendations. Our study provides evidence that integrating social network information in expert recommendations may outperform a purely expertise-based approach.",Recommending collaborators using social features and MeSH terms,https://www.semanticscholar.org/paper/2518d91ec5f685ba6c3cd57628b7f04fed071dd8,"Research collaborations are always encouraged, as they often yield good results. However, the researcher network contains massive amounts of experts in various disciplines and it is difficult for the individual researcher to decide which experts will match his own expertise best. As a result, collaboration outcomes are often uncertain and research teams are poorly organized. We propose a method for building link predictors in networks, where nodes can represent researchers and links - collaborations. In this case, predictors might offer good suggestions for future collaborations. We test our method on a researcher coauthorship network and obtain link predictors of encouraging accuracy. This leads us to believe our method could be useful in building and maintaining strong research teams. It could also help with choosing vocabulary for expert description, since link predictors contain implicit information about which structural attributes of the network are important with respect to the link prediction problem.",Finding Experts by Link Prediction in Co-authorship Networks,https://www.semanticscholar.org/paper/950af9f69ac33bb9f5d279c32290c77ea1714881,novel
"Develop an Interactive Sketch Question Answering (ISQA) system to enhance vision-based emergent communication through multi-round interactions. This system will enable two collaborative players to communicate via sketches to answer image-related questions, addressing the limitations of previous single-round approaches. Evaluate the system based on question answering accuracy, drawing complexity, and human interpretability, demonstrating its effectiveness in facilitating targeted and efficient communication between intelligent agents.","['Computer Science', 'Human-Computer Interaction', 'Vision-based Communication', 'Interactive Systems', 'Artificial Intelligence', 'Multi-agent Systems']","A goal-oriented visual dialogue involves multi-turn interactions between two agents, Questioner and Oracle. During which, the answer given by Oracle is of great significance, as it provides golden response to what Questioner concerns. Based on the answer, Questioner updates its belief on target visual content and further raises another question. Notably, different answers drive into different visual beliefs and future questions. However, existing methods always indiscriminately encode answers after much longer questions, resulting in a weak utilization of answers. In this paper, we propose an Answer-Driven Visual State Estimator (ADVSE) to impose the effects of different answers on visual states. First, we propose an Answer-Driven Focusing Attention (ADFA) to capture the answer-driven effect on visual attention by sharpening question-related attention and adjusting it by answer-based logical operation at each turn. Then based on the focusing attention, we get the visual state estimation by Conditional Visual Information Fusion (CVIF), where overall information and difference information are fused conditioning on the question-answer state. We evaluate the proposed ADVSE to both question generator and guesser tasks on the large-scale GuessWhat?! dataset and achieve the state-of-the-art performances on both tasks. The qualitative results indicate that the ADVSE boosts the agent to generate highly efficient questions and obtains reliable visual attentions during the reasonable question generation and guess processes.",Answer-Driven Visual State Estimator for Goal-Oriented Visual Dialogue,https://www.semanticscholar.org/paper/ce5b6ed64807cda4835f62e9d72fbc5af1beebda,"While there have been many proposals on making AI algorithms explainable, few have attempted to evaluate the impact of AI-generated explanations on human performance in conducting human-AI collaborative tasks. To bridge the gap, we propose a Twenty-Questions style collaborative image retrieval game, Explanation-assisted Guess Which (ExAG), as a method of evaluating the efficacy of explanations (visual evidence or textual justification) in the context of Visual Question Answering (VQA). In our proposed ExAG, a human user needs to guess a secret image picked by the VQA agent by asking natural language questions to it. We show that overall, when AI explains its answers, users succeed more often in guessing the secret image correctly. Notably, a few correct explanations can readily improve human performance when VQA answers are mostly incorrect as compared to no-explanation games. Furthermore, we also show that while explanations rated as “helpful” significantly improve human performance, “incorrect” and “unhelpful” explanations can degrade performance as compared to no-explanation games. Our experiments, therefore, demonstrate that ExAG is an effective means to evaluate the efficacy of AI-generated explanation on a human-AI collaborative task.",Can You Explain That? Lucid Explanations Help Human-AI Collaborative Image Retrieval,https://www.semanticscholar.org/paper/e46d636532657a6e3f6692b6f20230f7ce5f36f4,"Embodied Question Answering (EQA) is a recently proposed task, where an agent is placed in a rich 3D environment and must act based solely on its egocentric input to answer a given question. The desired outcome is that the agent learns to combine capabilities such as scene understanding, navigation and language understanding in order to perform complex reasoning in the visual world. However, initial advancements combining standard vision and language methods with imitation and reinforcement learning algorithms have shown EQA might be too complex and challenging for these techniques. In order to investigate the feasibility of EQA-type tasks, we build the VideoNavQA dataset that contains pairs of questions and videos generated in the House3D environment. The goal of this dataset is to assess question-answering performance from nearly-ideal navigation paths, while considering a much more complete variety of questions than current instantiations of the EQA task. We investigate several models, adapted from popular VQA methods, on this new benchmark. This establishes an initial understanding of how well VQA-style methods can perform within this novel EQA paradigm.",VideoNavQA: Bridging the Gap between Visual and Embodied Question Answering,https://www.semanticscholar.org/paper/ad732e16296b62ba1de9a22bd01452590b52a2fc,"Vision-language models (VLMs) have recently shown promising results in traditional downstream tasks. Evaluation studies have emerged to assess their abilities, with the majority focusing on the third-person perspective, and only a few addressing specific tasks from the first-person perspective. However, the capability of VLMs to""think""from a first-person perspective, a crucial attribute for advancing autonomous agents and robotics, remains largely unexplored. To bridge this research gap, we introduce EgoThink, a novel visual question-answering benchmark that encompasses six core capabilities with twelve detailed dimensions. The benchmark is constructed using selected clips from egocentric videos, with manually annotated question-answer pairs containing first-person information. To comprehensively assess VLMs, we evaluate eighteen popular VLMs on EgoThink. Moreover, given the open-ended format of the answers, we use GPT-4 as the automatic judge to compute single-answer grading. Experimental results indicate that although GPT-4V leads in numerous dimensions, all evaluated VLMs still possess considerable potential for improvement in first-person perspective tasks. Meanwhile, enlarging the number of trainable parameters has the most significant impact on model performance on EgoThink. In conclusion, EgoThink serves as a valuable addition to existing evaluation benchmarks for VLMs, providing an indispensable resource for future research in the realm of embodied artificial intelligence and robotics.",EgoThink: Evaluating First-Person Perspective Thinking Capability of Vision-Language Models,https://www.semanticscholar.org/paper/48557ff1c18c4403fd05caa9249e09c756058b7b,"While there have been many proposals on how to make AI algorithms more transparent, few have attempted to evaluate the impact of AI explanations on human performance on a task using AI. We propose a Twenty-Questions style collaborative image guessing game, Explanation-assisted Guess Which (ExAG) as a method of evaluating the efficacy of explanations in the context of Visual Question Answering (VQA) - the task of answering natural language questions on images. We study the effect of VQA agent explanations on the game performance as a function of explanation type and quality. We observe that ""helpful"" explanations are conducive to game performance (by almost 22% for ""excellent"" rated explanation games), and having at least one ""correct"" explanation is significantly helpful when VQA system answers are mostly noisy (by almost 30% compared to no explanation games). We also see that players develop a preference for explanations even when penalized and that the explanations are mostly rated as ""helpful"".",Lucid Explanations Help: Using a Human-AI Image-Guessing Game to Evaluate Machine Explanation Helpfulness,https://www.semanticscholar.org/paper/4ccf6fecd46267fe69b20aca6c670c0a8a84756a,"Evidence that visual communication preceded written language and provided a basis for it goes back to prehistory, in forms such as cave and rock paintings depicting traces of our distant ancestors. Emergent communication research has sought to explore how agents can learn to communicate in order to collaboratively solve tasks. Existing research has focused on language, with a learned communication channel transmitting sequences of discrete tokens between the agents. In this work, we explore a visual communication channel between agents that are allowed to draw with simple strokes. Our agents are parameterised by deep neural networks, and the drawing procedure is differentiable, allowing for end-to-end training. In the framework of a referential communication game, we demonstrate that agents can not only successfully learn to communicate by drawing, but with appropriate inductive biases, can do so in a fashion that humans can interpret. We hope to encourage future research to consider visual communication as a more flexible and directly interpretable alternative of training collaborative agents.",Learning to Draw: Emergent Communication through Sketching,https://www.semanticscholar.org/paper/c535914ddd4d49e5a9714f78b57f82b962c3ca3e,"In this work, we propose a goal-driven collaborative task that contains vision, language, and action in a virtual environment as its core components. Specifically, we develop a collaborative `Image Drawing' game between two agents, called CoDraw. Our game is grounded in a virtual world that contains movable clip art objects. Two players, Teller and Drawer, are involved. The Teller sees an abstract scene containing multiple clip arts in a semantically meaningful configuration, while the Drawer tries to reconstruct the scene on an empty canvas using available clip arts. The two players communicate via two-way communication using natural language. We collect the CoDraw dataset of ~10K dialogs consisting of 138K messages exchanged between a Teller and a Drawer from Amazon Mechanical Turk (AMT). We analyze our dataset and present three models to model the players' behaviors, including an attention model to describe and draw multiple clip arts at each round. The attention models are quantitatively compared to the other models to show how the conventional approaches work for this new task. We also present qualitative visualizations.",CoDraw: Visual Dialog for Collaborative Drawing,https://www.semanticscholar.org/paper/b68773df340498768e88487abe8f7fbac5fcb52d,"In this work, we propose a goal-driven collaborative task that combines language, perception, and action. Specifically, we develop a Collaborative image-Drawing game between two agents, called CoDraw. Our game is grounded in a virtual world that contains movable clip art objects. The game involves two players: a Teller and a Drawer. The Teller sees an abstract scene containing multiple clip art pieces in a semantically meaningful configuration, while the Drawer tries to reconstruct the scene on an empty canvas using available clip art pieces. The two players communicate with each other using natural language. We collect the CoDraw dataset of ~10K dialogs consisting of ~138K messages exchanged between human players. We define protocols and metrics to evaluate learned agents in this testbed, highlighting the need for a novel “crosstalk” evaluation condition which pairs agents trained independently on disjoint subsets of the training data. We present models for our task and benchmark them using both fully automated evaluation and by having them play the game live with humans.",CoDraw: Collaborative Drawing as a Testbed for Grounded Goal-driven Communication,https://www.semanticscholar.org/paper/58641a3a4b4653b5d63e57dc6dfe3935b866d78f,"Humans communicate with graphical sketches apart from symbolic languages. Primarily focusing on the latter, recent studies of emergent communication overlook the sketches; they do not account for the evolution process through which symbolic sign systems emerge in the trade-off between iconicity and symbolicity. In this work, we take the very first step to model and simulate this process via two neural agents playing a visual communication game; the sender communicates with the receiver by sketching on a canvas. We devise a novel reinforcement learning method such that agents are evolved jointly towards successful communication and abstract graphical conventions. To inspect the emerged conventions, we define three fundamental properties -- iconicity, symbolicity, and semanticity -- and design evaluation methods accordingly. Our experimental results under different controls are consistent with the observation in studies of human graphical conventions. Of note, we find that evolved sketches can preserve the continuum of semantics under proper environmental pressures. More interestingly, co-evolved agents can switch between conventionalized and iconic communication based on their familiarity with referents. We hope the present research can pave the path for studying emergent communication with the modality of sketches.",Emergent Graphical Conventions in a Visual Communication Game,https://www.semanticscholar.org/paper/71536316e209e988d6667e4ccaf595493fa07084,"Most existing approaches to Visual Question Answering (VQA) answer questions directly, however, people usually decompose a complex question into a sequence of simple sub questions and finally obtain the answer to the original question after answering the sub question sequence(SQS). By simulating the process, this paper proposes a conversation-based VQA (Co-VQA) framework, which consists of three components: Questioner, Oracle, and Answerer. Questioner raises the sub questions using an extending HRED model, and Oracle answers them one-by-one. An Adaptive Chain Visual Reasoning Model (ACVRM) for Answerer is also proposed, where the question-answer pair is used to update the visual representation sequentially. To perform supervised learning for each model, we introduce a well-designed method to build a SQS for each question on VQA 2.0 and VQA-CP v2 datasets. Experimental results show that our method achieves state-of-the-art on VQA-CP v2. Further analyses show that SQSs help build direct semantic connections between questions and images, provide question-adaptive variable-length reasoning chains, and with explicit interpretability as well as error traceability.",Co-VQA : Answering by Interactive Sub Question Sequence,https://www.semanticscholar.org/paper/403e6adc7186a3070261ab67487ac99149b4299b,novel
"Develop a dynamic prompt learning mechanism to enhance text-based image editing by addressing cross-attention leakage. This approach will update dynamic tokens for nouns in text prompts using leakage repairment losses, ensuring precise modifications to targeted objects while preventing unintended changes to other regions. Evaluate the method through quantitative metrics like CLIP score and qualitative user evaluations, demonstrating improved performance in complex multi-object scenes.","['Computer Science', 'Artificial Intelligence', 'Generative AI', 'Text-to-Image Generation', 'Image Editing', 'Cross-Attention Mechanisms', 'Diffusion Models']","Recent text-driven image editing in diffusion models has shown remarkable success. However, the existing methods assume that the user's description sufficiently grounds the contexts in the source image, such as objects, background, style, and their relations. This assumption is unsuitable for real-world applications because users have to manually engineer text prompts to find optimal descriptions for different images. From the users' standpoint, prompt engineering is a labor-intensive process, and users prefer to provide a target word for editing instead of a full sentence. To address this problem, we first demonstrate the importance of a detailed text description of the source image, by dividing prompts into three categories based on the level of semantic details. Then, we propose simple yet effective methods by combining prompt generation frameworks, thereby making the prompt engineering process more user-friendly. Extensive qualitative and quantitative experiments demonstrate the importance of prompts in text-driven image editing and our method is comparable to ground-truth prompts.",User-friendly Image Editing with Minimal Text Input: Leveraging Captioning and Injection Techniques,https://www.semanticscholar.org/paper/0809c278fcdec2ce297da3a9d6e031fc192263f6,"Recent text-to-image (T2I) diffusion models show outstanding performance in generating high-quality images conditioned on textual prompts. However, they fail to semantically align the generated images with the prompts due to their limited compositional capabilities, leading to attribute leakage, entity leakage, and missing entities. In this paper, we propose a novel attention mask control strategy based on predicted object boxes to address these issues. In particular, we first train a BoxNet to predict a box for each entity that possesses the attribute specified in the prompt. Then, depending on the predicted boxes, a unique mask control is applied to the cross- and self-attention maps. Our approach produces a more semantically accurate synthesis by constraining the attention regions of each token in the prompt to the image. In addition, the proposed method is straightforward and effective and can be readily integrated into existing cross-attention-based T2I generators. We compare our approach to competing methods and demonstrate that it can faithfully convey the semantics of the original text to the generated content and achieve high availability as a ready-to-use plugin. Please refer to https://github.com/OPPO-Mente-Lab/attention-mask-control.",Compositional Text-to-Image Synthesis with Attention Map Control of Diffusion Models,https://www.semanticscholar.org/paper/3fd516043d29e683d66c6d51aa641a3a9fdef8e0,"Existing text-to-image diffusion models struggle to synthesize realistic images given dense captions, where each text prompt provides a detailed description for a specific image region. To address this, we propose DenseDiffusion, a training-free method that adapts a pre-trained text-to-image model to handle such dense captions while offering control over the scene layout. We first analyze the relationship between generated images’ layouts and the pre-trained model’s intermediate attention maps. Next, we develop an attention modulation method that guides objects to appear in specific regions according to layout guidance. Without requiring additional fine-tuning or datasets, we improve image generation performance given dense captions regarding both automatic and human evaluation scores. In addition, we achieve similar-quality visual results with models specifically trained with layout conditions. Code and data are available at https://github.com/naver-ai/DenseDiffusion.",Dense Text-to-Image Generation with Attention Modulation,https://www.semanticscholar.org/paper/c15803110bfacac95f7f0283b0b34492e20a1a3b,"Recent text-to-image diffusion models have reached an unprecedented level in generating high-quality images. However, their exclusive reliance on textual prompts often falls short in precise control of image compositions. In this paper, we propose LoCo, a training-free approach for layout-to-image Synthesis that excels in producing high-quality images aligned with both textual prompts and layout instructions. Specifically, we introduce a Localized Attention Constraint (LAC), leveraging semantic affinity between pixels in self-attention maps to create precise representations of desired objects and effectively ensure the accurate placement of objects in designated regions. We further propose a Padding Token Constraint (PTC) to leverage the semantic information embedded in previously neglected padding tokens, improving the consistency between object appearance and layout instructions. LoCo seamlessly integrates into existing text-to-image and layout-to-image models, enhancing their performance in spatial control and addressing semantic failures observed in prior methods. Extensive experiments showcase the superiority of our approach, surpassing existing state-of-the-art training-free layout-to-image methods both qualitatively and quantitatively across multiple benchmarks.",LoCo: Locally Constrained Training-Free Layout-to-Image Synthesis,https://www.semanticscholar.org/paper/183ff1cfd2370cbcf4088780bf1f6e8d6db8be0f,"Recent large-scale text-driven synthesis models have attracted much attention thanks to their remarkable capabilities of generating highly diverse images that follow given text prompts. Such text-based synthesis methods are particularly appealing to humans who are used to verbally describe their intent. Therefore, it is only natural to extend the text-driven image synthesis to text-driven image editing. Editing is challenging for these generative models, since an innate property of an editing technique is to preserve most of the original image, while in the text-based models, even a small modification of the text prompt often leads to a completely different outcome. State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region. In this paper, we pursue an intuitive prompt-to-prompt editing framework, where the edits are controlled by text only. To this end, we analyze a text-conditioned model in depth and observe that the cross-attention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt. With this observation, we present several applications which monitor the image synthesis by editing the textual prompt only. This includes localized editing by replacing a word, global editing by adding a specification, and even delicately controlling the extent to which a word is reflected in the image. We present our results over diverse images and prompts, demonstrating high-quality synthesis and fidelity to the edited prompts.",Prompt-to-Prompt Image Editing with Cross Attention Control,https://www.semanticscholar.org/paper/04e541391e8dce14d099d00fb2c21dbbd8afe87f,"Text-to-image synthesis has achieved high-quality results with recent advances in diffusion models. However, text input alone has high spatial ambiguity and limited user controllability. Most existing methods allow spatial control through additional visual guidance (e.g., sketches and semantic masks) but require additional training with annotated images. In this paper, we propose a method for spatially controlling text-to-image generation without further training of diffusion models. Our method is based on the insight that the cross-attention maps reflect the positional relationship between words and pixels. Our aim is to control the attention maps according to given semantic masks and text prompts. To this end, we first explore a simple approach of directly swapping the cross-attention maps with constant maps computed from the semantic regions. Some prior works also allow training-free spatial control of text-to-image diffusion models by directly manipulating cross-attention maps. However, these approaches still suffer from misalignment to given masks because manipulated attention maps are far from actual ones learned by diffusion models. To address this issue, we propose masked-attention guidance, which can generate images more faithful to semantic masks via indirect control of attention to each word and pixel by manipulating noise images fed to diffusion models. Masked-attention guidance can be easily integrated into pre-trained off-the-shelf diffusion models (e.g., Stable Diffusion) and applied to the tasks of text-guided image editing. Experiments show that our method enables more accurate spatial control than baselines qualitatively and quantitatively.",Masked-Attention Diffusion Guidance for Spatially Controlling Text-to-Image Generation,https://www.semanticscholar.org/paper/72c7634849834220e9dd495fbfa26def2f7d80bb,"Text-guided diffusion models such as DALLE-2, Imagen, and Stable Diffusion are able to generate an effectively endless variety of images given only a short text prompt describing the desired image content. In many cases the images are of very high quality. However, these models often struggle to compose scenes containing several key objects such as characters in specified positional relationships. The missing capability to ``direct'' the placement of characters and objects both within and across images is crucial in storytelling, as recognized in the literature on film and animation theory. In this work, we take a particularly straightforward approach to providing the needed direction. Drawing on the observation that the cross-attention maps for prompt words reflect the spatial layout of objects denoted by those words, we introduce an optimization objective that produces ``activation'' at desired positions in these cross-attention maps. The resulting approach is a step toward generalizing the applicability of text-guided diffusion models beyond single images to collections of related images, as in storybooks. Directed Diffusion provides easy high-level positional control over multiple objects, while making use of an existing pre-trained model and maintaining a coherent blend between the positioned objects and the background. Moreover, it requires only a few lines to implement.",Directed Diffusion: Direct Control of Object Placement through Attention Guidance,https://www.semanticscholar.org/paper/beaa3130f2657bbfe784b2313f20ba66495926ba,"Natural language instructions are a powerful interface for editing the outputs of text-to-image diffusion models. However, several challenges need to be addressed: 1) underspecification (the need to model the implicit meaning of instructions) 2) grounding (the need to localize where the edit has to be performed), 3) faithfulness (the need to preserve the elements of the image not affected by the edit instruction). Current approaches focusing on image editing with natural language instructions rely on automatically generated paired data, which, as shown in our investigation, is noisy and sometimes nonsensical, exacerbating the above issues. Building on recent advances in segmentation, Chain-of-Thought prompting, and visual question answering, we significantly improve the quality of the paired data. In addition, we enhance the supervision signal by highlighting parts of the image that need to be changed by the instruction. The model fine-tuned on the improved data is capable of performing fine-grained object-centric edits better than state-of-the-art baselines, mitigating the problems outlined above, as shown by automatic and human evaluations. Moreover, our model is capable of generalizing to domains unseen during training, such as visual metaphors.",Learning to Follow Object-Centric Image Editing Instructions Faithfully,https://www.semanticscholar.org/paper/d406b90a31297f853ab45099f6ed5d4a44703abb,"Recent text-to-image generative models have demonstrated an unparalleled ability to generate diverse and creative imagery guided by a target text prompt. While revolutionary, current state-of-the-art diffusion models may still fail in generating images that fully convey the semantics in the given text prompt. We analyze the publicly available Stable Diffusion model and assess the existence of catastrophic neglect, where the model fails to generate one or more of the subjects from the input prompt. Moreover, we find that in some cases the model also fails to correctly bind attributes (e.g., colors) to their corresponding subjects. To help mitigate these failure cases, we introduce the concept of Generative Semantic Nursing (GSN), where we seek to intervene in the generative process on the fly during inference time to improve the faithfulness of the generated images. Using an attention-based formulation of GSN, dubbed Attend-and-Excite, we guide the model to refine the cross-attention units to attend to all subject tokens in the text prompt and strengthen --- or excite --- their activations, encouraging the model to generate all subjects described in the text prompt. We compare our approach to alternative approaches and demonstrate that it conveys the desired concepts more faithfully across a range of text prompts. Code is available at our project page: https://attendandexcite.github.io/Attend-and-Excite/.",Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models,https://www.semanticscholar.org/paper/c3c7464acb90049c5f520b0732dc7435ba3690bd,"Recent text-to-image (T2I) diffusion models have achieved remarkable progress in generating high-quality images given text-prompts as input. However, these models fail to convey appropriate spatial composition specified by a layout instruction. In this work, we probe into zero-shot grounded T2I generation with diffusion models, that is, generating images corresponding to the input layout information without training auxiliary modules or finetuning diffusion models. We propose a Region and Boundary (R&B) aware cross-attention guidance approach that gradually modulates the attention maps of diffusion model during generative process, and assists the model to synthesize images (1) with high fidelity, (2) highly compatible with textual input, and (3) interpreting layout instructions accurately. Specifically, we leverage the discrete sampling to bridge the gap between consecutive attention maps and discrete layout constraints, and design a region-aware loss to refine the generative layout during diffusion process. We further propose a boundary-aware loss to strengthen object discriminability within the corresponding regions. Experimental results show that our method outperforms existing state-of-the-art zero-shot grounded T2I generation methods by a large margin both qualitatively and quantitatively on several benchmarks.",R&B: Region and Boundary Aware Zero-shot Grounded Text-to-image Generation,https://www.semanticscholar.org/paper/54b49617a447dd38b079725b74e3f0753f2b782f,novel
"Develop a hybrid gamification intervention that combines digital and tabletop game elements to enhance health education among adolescents. This intervention could be designed using a framework that incorporates game mechanics, dynamics, aesthetics, and emotional engagement tailored to the adolescent demographic. Pilot testing in schools could assess the impact on health parameters such as food consumption, sleep quality, and physical activity. The study would compare the efficacy of hybrid games versus purely digital or tabletop games in promoting sustained health behavior changes.","['Health Behavior Change', 'Gamification', 'Game Design', 'Tabletop Games', 'Digital Games', 'Health Education', 'Behavioral Science', 'Adolescent Health']","Background Games, when used as interventional tools, can influence behavior change by incentivizing, reinforcing, educating, providing feedback loops, prompting, persuading, or providing meaning, fun, and community. However, not all game elements will appeal to all consumers equally, and different elements might work for different people and in different contexts. Objective The aim of this study was to conduct a realist review of tabletop games targeting behavior change and to propose a framework for designing effective behavior change games. Methods A realist review was conducted to inform program theory in the development of tabletop games for health behavior change. The context, mechanisms used to change behavior, and outcomes of included studies were reviewed through a realist lens. Results Thirty-one papers met the eligibility criteria and were included in the review. Several design methods were identified that enhanced the efficacy of the games to change behavior. These included design by local teams, pilot testing, clearly defined targets of behavior change, conscious attention to all aspects of game design, including game mechanics, dynamics, aesthetics, and the elicitation of emotions. Delivery with other mediums, leveraging behavioral insights, prior training for delivery, and repeated play were also important. Some design elements that were found to reduce efficacy included limited replayability or lack of fun for immersive engagement. Conclusions Game designers need to consider all aspects of the context and the mechanisms to achieve the desired behavior change outcomes. Careful design thinking should include consideration of the game mechanics, dynamics, aesthetics, emotions, and contexts of the game and the players. People who know the players and the contexts well should design the games or have significant input. Testing in real-world settings is likely to lead to better outcomes. Careful selection and purposeful design of the behavior change mechanisms at play is essential. Fun and enjoyment of the player should be considered, as without engagement, there will be no desired intervention effect.",Tabletop Board Game Elements and Gamification Interventions for Health Behavior Change: Realist Review and Proposal of a Game Design Framework,https://www.semanticscholar.org/paper/5c3fb96b75a79b7d15ae568e75a9f4e51925f7cf,"
 BACKGROUND
 Games, when used as interventional tools, can influence behavior change by incentivizing, reinforcing, educating, providing feedback loops, prompting, persuading, or providing meaning, fun, and community. However, not all game elements will appeal to all consumers equally, and different elements might work for different people and in different contexts.
 
 
 OBJECTIVE
 The aim of this study was to conduct a realist review of tabletop games targeting behavior change and to propose a framework for designing effective behavior change games.
 
 
 METHODS
 A realist review was conducted to inform program theory in the development of tabletop games for health behavior change. The context, mechanisms used to change behavior, and outcomes of included studies were reviewed through a realist lens.
 
 
 RESULTS
 Thirty-one papers met the eligibility criteria and were included in the review. Several design methods were identified that enhanced the efficacy of the games to change behavior. These included design by local teams, pilot testing, clearly defined targets of behavior change, conscious attention to all aspects of game design, including game mechanics, dynamics, aesthetics, and the elicitation of emotions. Delivery with other mediums, leveraging behavioral insights, prior training for delivery, and repeated play were also important. Some design elements that were found to reduce efficacy included limited replayability or lack of fun for immersive engagement.
 
 
 CONCLUSIONS
 Game designers need to consider all aspects of the context and the mechanisms to achieve the desired behavior change outcomes. Careful design thinking should include consideration of the game mechanics, dynamics, aesthetics, emotions, and contexts of the game and the players. People who know the players and the contexts well should design the games or have significant input. Testing in real-world settings is likely to lead to better outcomes. Careful selection and purposeful design of the behavior change mechanisms at play is essential. Fun and enjoyment of the player should be considered, as without engagement, there will be no desired intervention effect.
",Tabletop Board Game Elements and Gamification Interventions for Health Behavior Change: Realist Review and Proposal of a Game Design Framework (Preprint),https://www.semanticscholar.org/paper/8d052de2d798755f62c4fef0e74ad1098f340d95,"Physical activity (PA), body composition and sedentary behavior may affect the health of children. Therefore, this study examined the effect of an educational hybrid physical education (PE) program on physical fitness (PF), body composition and sedentary and PA times in adolescents. A 9-month group-randomized controlled trial was conducted in 150 participants (age: 14.63 ± 1.38 years) allocated into the control group (CG, n = 37) and experimental group (EG, n = 113). Cardiorespiratory fitness, speed, strength, agility, flexibility and body mass index (BMI) were assessed through previously validated field tests. Sedentary time, PA at school and afterschool were evaluated with the Youth Activity Profile-Spain questionnaire. Significant differences were observed concerning to the CG in APA-weekend (p = 0.044), speed-agility (p = 0.005) and agility (p = 0.008). Regarding the intervention, cardiorespiratory fitness (p = 0.000), speed-agility (p = 0.000), strength (p = 0.000), flexibility (p = 0.000), agility (p = 0.000), PA in school (p = 0.011), APA-weekday (p = 0.001), APA-weekend (p = 0.000), APA-week (p = 0.000), and sedentary time (p = 0.000) increased significantly in the EG. The use of a hybrid program based on teaching personal and social responsibility and gamification strategies produced enhancements in cardiorespiratory fitness, agility, speed, APA-weekdays and APA-weekends, reducing the sedentary time.","Effects of an Educational Hybrid Physical Education Program on Physical Fitness, Body Composition and Sedentary and Physical Activity Times in Adolescents: The Seneb’s Enigma",https://www.semanticscholar.org/paper/0c8145ee85f9c2c1d636d9aa80dd324da01d8ab2,"Gamification is a relatively new approach that allows the use of videogame design techniques in contexts that are originally not game related, including for the promotion and education of health outcomes. Gamification has been used in many contexts, but healthcare practices, which include often boring, frustrating, or painful tasks, can especially benefit from the fun enjoyable games people play for entertainment purposes. Games can be helpful both promoting an increase in health knowledge and behaviors, as well as the positive emotions elicited by health-related contents and behaviors. This chapter begins by discussing the concept of gamification, the gamification toolbox, and gamer taxonomies and the different uses of gamification and game-based approaches in the healthcare context are explored, to figure out what the key success elements are and why this promising approach has yet to achieve its wide-spread potential use.",Fun and Games,https://www.semanticscholar.org/paper/99b364c79b0d09d1668d16de0494fcfb5f8fef82,"Every month at Google Campus in London, dozens of software developers, clinicians, behavioural scientists and investors get together to discuss new strategies to influence health behaviours. The collective aim of these networking events is to develop digital ‘games with purpose’ that can improve health by integrating software design and game mechanics with public health theory and behavioural insights. Gamification is a purposely-broad umbrella term used to encompass the process of using ‘gaming’ elements to motivate and engage people in non-game contexts.1 Enhanced opportunities now exist to deliver behaviour change interventions through game platforms on new smartphone devices. 
 
Defying traditional stereotypes, people across demographic boundaries now play video games on a wide range of digital devices.2 Whilst such games continue to be primarily used for entertainment purposes, there is increasing interest in their potential to influence positive changes in health behaviours.3 This has been encouraged by the finding that rather than spending hours being sedentary and chasing intangible outcomes, players of active video games (e.g. Nintendo Wii Fit) are motivated to exert themselves to achieve activity goals through game mechanics.4,5 
 
Whilst still in its infancy, we predict that gamification will become an increasingly familiar concept in healthcare as a consequence of two trends. The first builds on the consumer's appetite for new smartphone devices that provide games designers with a wider audience to target and more attractive tools to use in designing interactive health interventions. The second factor is the enthusiasm and willingness of developers to incorporate the latest behavioural insights into electronic interventions.",‘Gamification’: Influencing health behaviours with games,https://www.semanticscholar.org/paper/9a4d3da2afbfb9359e8538837953aec17fab3eaa,"
 
 
 Ample evidence exists that wellbeing is critical during teenage years. Therefore, options for implementing wellbeing among adolescents are needed. To fulfill this need, gamification promises to be a suitable method. Gamification uses game design elements from a game context (points, leaderboards) and game elements (randomness, tension) in a real life context to reach life goals such as behavioral changes. Recent studies indicate that gamification only produces long-term effects when filled with stimulating content. To categorize the content of the adventures and other game design elements within the gamification concept, the “5 ways to wellbeing” have been a suitable evidence-based approach among adult interventions: connect, give, take notice, keep learning and be active. This research aims at filling the framework of gamification with the most opportune content to foster wellbeing among adolescents.
 
 
 
 71 interventions of fostering wellbeing among adolescents could be identified in a scoping review from Pubmed. Eligible interventions had to focus on adolescents and had to have outcomes on wellbeing. Interventions that covered a more specific target group, such as cancer survivors, were excluded from this research.
 
 
 
 Adolescent interventions' contents, which had a positive outcome on wellbeing, mostly dealt with physical activity (13) and health promotion (11), followed by psychological training interventions (9). 11 interventions did not have a positive outcome on wellbeing.
 
 
 
 The many positive outcomes in above categories result from the amount of interventions. To further investigate the effect of the five ways to wellbeing within a gamification framework for adolescents, we develop an intervention application (ONYA) which will be applied among adolescents in German middle schools.
 ONYA is funded by the German Federal Ministry of Health Education.
 
 
 
 Research on suitable content for gamification frameworks on wellbeing is rare. Wellbeing interventions are underrepresented in the categories of the 5 ways to wellbeing.
",Reviewing Effective Contents for a Gamification Approach to Foster Wellbeing among Adolescents,https://www.semanticscholar.org/paper/d419901304773563282691ebb473e9d25dd1c383,"Games are a successful pedagogical tool to change attitudes and behaviors. This chapter will examine how games facilitate change, discuss common pitfalls, and outline best practices for making serious games for clinical practice. Sustained engagement and motivation are key to lasting clinical interventions. When developing a game for clinical practice, the designer should avoid “punishing by rewards” (Kohn, 1993), damaging motivation towards the desired goal. Understanding game design principles is crucial to creating intrinsically engaging experiences that lead to lasting motivation. The Mechanics, Dynamics, and Aesthetics (MDA) framework is widely accepted by game designers as a framework to make compelling games. Using MDA as a base for understanding how to create engaging experiences, this chapter proposes a new framework for serious games called Mechanics, Dynamics, Aesthetics, and Outcomes. MDAO describes how to design a game that is intrinsically motivating and effective by focusing on the interplay between outcomes and other vectors of design.","Guidelines for Designing Effective Games as Clinical Interventions: Mechanics, Dynamics, Aesthetics, and Outcomes (MDAO) Framework",https://www.semanticscholar.org/paper/61b886482091631d93e1790ca813b6ae13f0024c,"This paper presents a list of principles that could be used to conceptualize games for behavior change. These principles are derived from lessons learned after teaching two design-centered courses around Gaming and Narrative Technologies for Health Behavior Change. Course sessions were designed to create many rapid prototypes based on specific topics from behavior change theory coupled with iterative human-centered and games design techniques. The design task was composed of two broad goals: 1) designing efficacious technologies, with an emphasis on short-term behavior change and 2) using metaphors, dramatic arcs and game dynamics as vehicles for increased engagement and long-term sustained change. Some example prototypes resulting from this design approach are presented. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. CHI’13, April 27 – May 2, 2013, Paris, France. Copyright 2012 ACM 978-1-4503-1952-2...$10.00. Pablo Paredes Berkeley Institute of Design",Design Principles for the Conceptualization of Games for Health Behavior Change,https://www.semanticscholar.org/paper/1458ce36372b7e05180ea09f4f59a6651df96517,"Serious video games for health are designed to entertain while changing a specific health behavior. This article identifies behavioral principles that can guide the development of serious video games focused on changing a variety of health behaviors, including those attempting to decrease risk of obesity and type 2 diabetes. Guidelines discussed include how to develop video games that provide a solid foundation for behavior change by enhancing a player's knowledge and skill, ways in which personal mastery experiences can be incorporated into a video game environment, using game characters and avatars to promote observational learning, creating personalized experiences through tailoring, and the importance of achieving a balance between “fun-ness” and “seriousness.” The article concludes with suggestions for future research needed to inform this rapidly growing field.",Designing Serious Video Games for Health Behavior Change: Current Status and Future Directions,https://www.semanticscholar.org/paper/6363cf00a0e53cf8fcf004fcd9748e72060d9ae4,"Effective chronic disease management is essential to improve positive health outcomes, and incentive strategies are useful in promoting self-care with longevity. Gamification, applied with mHealth (mobile health) applications, has the potential to better facilitate patient self-management. This review article addresses a knowledge gap around the effective use of gamification design principles, or mechanics, in developing mHealth applications. Badges, leaderboards, points and levels, challenges and quests, social engagement loops, and onboarding are mechanics that comprise gamification. These mechanics are defined and explained from a design and development perspective. Health and fitness applications with gamification mechanics include: bant which uses points, levels, and social engagement, mySugr which uses challenges and quests, RunKeeper which uses leaderboards as well as social engagement loops and onboarding, Fitocracy which uses badges, and Mango Health, which uses points and levels. Specific design considerations are explored, an example of the efficacy of a gamified mHealth implementation in facilitating improved self-management is provided, limitations to this work are discussed, a link between the principles of gaming and gamification in health and wellness technologies is provided, and suggestions for future work are made. We conclude that gamification could be leveraged in developing applications with the potential to better facilitate self-management in persons with chronic conditions.",A game plan: Gamification design principles in mHealth applications for chronic disease management,https://www.semanticscholar.org/paper/f9be2234cbcb02cadb45c2994463cfecfab63a0c,novel
"Implement **alternative assessment techniques** to **boost scientific innovation** by creating a gamified platform where researchers can earn points and badges for exploring novel research areas and collaborating with diverse scholars. This platform would use alternative metrics such as peer evaluations, impact scores from interdisciplinary initiatives, and innovative practice implementations to measure success. **User studies with researchers** would be conducted to assess the platform's effectiveness in encouraging researchers to step out of their comfort zones and engage in interdisciplinary and innovative projects.","['Computer Science', 'Human-Computer Interaction', 'Information Retrieval', 'Recommender Systems', 'Scientific Innovation', 'Interdisciplinary Research']","Collaboration between research scientists, particularly those with diverse backgrounds, is a driver of scientific innovation. However, finding the right collaborator is often an unscientific process that is subject to chance. This paper explores recommending collaborators based on repeating patterns of previous successful collaboration experiences, what we term prototypical collaborations. We investigate a method for discovering such prototypes to use them as a basis to guide the recommendation of new collaborations. To this end, we also examine two methods for matching collaboration seekers to these prototypical collaborations. Our initial studies reveal that though promising, improving collaborations through recommendation is a complex goal.",Discovering Patterns of Collaboration for Recommendation,https://www.semanticscholar.org/paper/d14bf8dc82ec9715fd08505924876c9863b62e7d,"Scientific evaluation is a determinant of how scientists, institutions and funders behave, and as such is a key element in the making of science. In this article, we propose an alternative to the current norm of evaluating research with journal rank. Following a well-defined notion of scientific value, we introduce qualitative processes that can also be quantified and give rise to meaningful and easy-to-use article-level metrics. In our approach, the goal of a scientist is transformed from convincing an editorial board through a vertical process to convincing peers through an horizontal one. We argue that such an evaluation system naturally provides the incentives and logic needed to constantly promote quality, reproducibility, openness and collaboration in science. The system is legally and technically feasible and can gradually lead to the self-organized reappropriation of the scientific process by the scholarly community and its institutions. We propose an implementation of our evaluation system with the platform ""the Self-Journals of Science"" (www.sjscience.org).",Novel processes and metrics for a scientific evaluation rooted in the principles of science - Version 1,https://www.semanticscholar.org/paper/72e914d5d4b89e32099afe46d73ac3dc9baf7dff,"Citations allow quickly identifying related research. If multiple publications are selected as seeds, specific suggestions for related literature can be made based on the number of incoming and outgoing citation links to this selection. Interactively adding recommended publications to the selection refines the next suggestion and incrementally builds a relevant collection of publications. Following this approach, the paper presents a search and foraging approach, PUREsuggest, which combines citation-based suggestions with augmented visualizations of the citation network. The focus and novelty of the approach is, first, the transparency of how the rankings are explained visually and, second, that the process can be steered through user-defined keywords, which reflect topics of interests. The system can be used to build new literature collections, to update and assess existing ones, as well as to use the collected literature for identifying relevant experts in the field. We evaluated the recommendation approach through simulated sessions and performed a user study investigating search strategies and usage patterns supported by the interface.",PUREsuggest: Citation-based Literature Search and Visual Exploration with Keyword-controlled Rankings,https://www.semanticscholar.org/paper/813fccf0b9f37dfc7bb6df0690502d72468b76c7,"One of the most challenging aspects of research is choosing a suitable topic. It is hard not to be intimidated by unlimited subjects in every field. Formulating effective research questions is one way to ease the stress of choosing a topic. Regrettably, some problems often arise as students attempt to formulate questions and topics. Researchers can avoid painful and frustrating wasted hours by recognising potential problems. This research employs bibliometric analysis with the R software to examine data obtained from Scopus database. The results show that there are limited number of papers available on this topic in the Scopus database. There is undoubtedly a gap in this area of research and therefore more research needs to be conducted on identifying a researchable topic. This paper provided guidelines to identify a potential research topic with a constructive approach. Researchers can reduce frustration and wasting of time by avoiding topic apathy, selecting a broad topic, not reading enough, and not being inquisitive enough. Researchers should focus on three significant features: capability, appropriateness, and relevance. The paper suggested beginning the process of identifying the research topic by developing a depth of knowledge in the area of interest. The depth can be developed from the literature, beginning from the core to the latest research papers that can be tracked from websites like Google Scholar, Research Gate etc.",Unveiling Promising Research Avenues: A Systematic Guide for Selecting Researchable Topics,https://www.semanticscholar.org/paper/c183acca429552f072a654111628ee9c8e6e54ab,"Abstract: The proliferation of digital platforms and the exponential growth of data have led to an overwhelming abundance of information, necessitating innovative solutions to assist individuals in navigating this landscape. In response to this challenge, advice generator applications have emerged as a promising tool for delivering personalized recommendations and guidance tailored to the unique needs and preferences of users. Leveraging artificial intelligence and data analytics, these applications analyze user inputs and contextual factors to generate actionable insights across various domains. This abstract provides an overview of the landscape of advice generator applications, exploring their functionalities, significance, and implications within the realm of research. Additionally, it highlights the role of these applications in facilitating individual empowerment, enhancing decision-making processes, and informing future studies in fields such as psychology, behavioral economics, and artificial intelligence. Through a comprehensive analysis of advice generator applications, this research paper seeks to contribute to a deeper understanding of their potential impact on society and the broader research landscape. Keywords : Advice generator app gives the advice what to do or not by using this app.",WonderWoods Online Furniture Website,https://www.semanticscholar.org/paper/18cd3959f1ea75ad713f22091a08b110bd3b71f4,"ResearchRabbit is a scholarly publication discovery tool supported by artificial intelligence (AI). It was developed in 2021 by a team of three in Seattle [1]. This tool lets users discover publications related to one or more seed publications with the help of visualization maps and lists of earlier, later, and similar publications. ResearchRabbit is designed to support the workflow of unstructured searching while providing a left-to-right trail from the original publication(s) through any selected authors or publications. These trails, which can run as deep as rabbit holes, suggest the origin of the tool’s name. To start using ResearchRabbit, users first need to create an account. Then they need to create a collection and add at least one publication. The more publications that are added, the better ResearchRabbit can understand users’ interests and generate recommendations similar to the contents of the collection. Publications can be added either by uploading a RIS or BibTeX file or by using ResearchRabbit’s search, powered by PubMed, if users are searching the medical sciences, or Semantic Scholar, for any other subject area. While ResearchRabbit uses PubMed’s and Semantic Scholar’s search engines, the company claims its unique database of “100s of millions of academic articles” is second in size only to Google Scholar [2]. Once publications are in a collection, ResearchRabbit’s algorithm will begin generating recommendations. These recommendations can be explored through two modes: 1) by Papers that are Similar work, Earlier work, or Later work or 2) by People that provide additional publications that These authors or Suggested authors have published (Figure 1). These recommendations are depicted using visualization maps. Fig. 1 Different exploration modes",ResearchRabbit,https://www.semanticscholar.org/paper/6740b72b134f194553d9d3d2b23b0dba5eb9667c,"Understanding the evolution of scholarly impact is essential for many real-life decision-making processes in academia, such as research planning, frontier exploration, and award selection. Popular platforms like Google Scholar and Web of Science rely on numerical indicators that are too abstract to convey the context and content of scientific impact, while most existing visualization approaches on mapping science do not consider the presentation of individual scholars' impact evolution using curated self-citation data. This paper builds on our previous work and proposes an integrated pipeline to visualize a scholar's impact evolution from multiple topic facets. A novel 3D prism-shaped visual metaphor is introduced as the overview of a scholar's impact, whilst their scientific evolution on each topic is displayed in a more structured manner. Additional designs by topic chord diagram, streamgraph visualization, and inter-topic flow map, optimized by an elaborate layout algorithm, assist in perceiving the scholar's scientific evolution across topics. A new six-degree-impact glyph metaphor highlights key interdisciplinary works driving the evolution. The proposed visualization methods are evaluated through case studies analyzing the careers of prestigious Turing award laureates and a major visualization venue.",GeneticPrism: Multifaceted Visualization of Scientific Impact Evolutions,https://www.semanticscholar.org/paper/8e62faec9f93c94e74a4bf2e2ed1438eab05de90,"In the age of information abundance, discovering personalized content is paramount to engaging users on a variety of topics. The engine goes beyond traditional approaches by not only dynamically adjusting recommendations to the users' historical preferences, but also actively encouraging exploration of interdisciplinary areas by referencing them with other genre or domains. The recommendation engine uses a multi-layer strategy that includes collaborative and content-based filtering, hybrid models and advanced diversity metrices. It transcends the general boundaries of user preferences and fosters randomness and novelty through cross-domain recommendations. The system adapts to changing user interests by suggesting content in various formats such as articles, podcasts, videos and simulations. Community integration plays a key role, leveraging collaborative filtering to discover community-curated lists and hidden gems across various categories. Real-time trend research allows users to stay informed and engaged with the latest developments, maintaining a dynamic and up-to-date content ecosystem. Ethical considerations, transparency and user-friendly interfaces are essential elements to address bias and provide explanations for recommendations. Through continuous user feedback and iterative improvements, the novel algorithmic recommendation engine for diverse content discovery aims to redefine content discovery and foster a sense of exploration, curiosity and learning across a wide range of knowledge domains.",Novel Algorithmic Recommendation Engine for Diverse Content Discovery,https://www.semanticscholar.org/paper/60e54892d78a056b957508feab0072cca4b5fac8,"Isolated silos of scientific research and the growing challenge of information overload limit awareness across the literature and hinder innovation. Algorithmic curation and recommendation, which often prioritize relevance, can further reinforce these informational “filter bubbles.” In response, we describe Bridger, a system for facilitating discovery of scholars and their work. We construct a faceted representation of authors with information gleaned from their papers and inferred author personas, and use it to develop an approach that locates commonalities and contrasts between scientists to balance relevance and novelty. In studies with computer science researchers, this approach helps users discover authors considered useful for generating novel research directions. We also demonstrate an approach for displaying information about authors, boosting the ability to understand the work of new, unfamiliar scholars. Our analysis reveals that Bridger connects authors who have different citation profiles and publish in different venues, raising the prospect of bridging diverse scientific communities.",Bursting Scientific Filter Bubbles: Boosting Innovation via Novel Author Discovery,https://www.semanticscholar.org/paper/bea1c05b0584a4463cbfbcf4917d6afacaef0bde,"Open user models provide affordance for a transparent user control over recommendations based on shared symbolic representation within the system. Users must build their user profile by adding these symbols and tuning their importance to get meaningful recommendations. Since the link between these symbols and the reference explanation is often unavailable, it can be difficult for users to understand them. These symbols are often referred to as concepts, tags, areas, topics, labels, features, or keyphrases. This study showcases an information exploration system that helps students identify potential faculty members to collaborate with. The system works by matching user and faculty profiles that contain keywords or phrases representing topics/areas of interest. Students must develop their understanding of research topics while building their profiles, which can become challenging as they add more keywords. To support students in controlling the recommendation, we introduce post hoc explanations with three levels of detail: no explanations, individual explanation for topics, and explanation of the relationships between topics. This study explores how explanation is associated with the user context / tasks and the exploration process. Our observation suggests that expertise in the field is linked to exploring fewer novel topics and seeking fewer explanations but engaging more with explanations of relationships. In addition, we found that the engagement with faculty information is moderately correlated with the use of more advanced explanations.",Explanations in Open User Models for Personalized Information Exploration,https://www.semanticscholar.org/paper/ff4e3f399b8db5615b067f39a416f2055f9b285e,novel
"Create a **combined audit, feedback, and education intervention system** designed **to improve scientific paper revisions**. This system will use **periodic audits of manuscript drafts**, followed by **detailed feedback sessions** and **educational resources** on high-quality writing and common revision challenges. The effectiveness of this system will be **evaluated using multilevel modeling and inductive analysis** to measure improvements in manuscript quality and alignment with reviewer comments.","['Computer Science', 'Natural Language Processing', 'Scientific Writing', 'Peer Review Systems', 'Machine Learning', 'Human-Computer Interaction']","Communication is a crucial skillset for engineers, yet graduates ​[1]–[3]​ and their employers ​[4]–[8]​ continue to report their lack of preparation for effective communication upon completion of their undergraduate or graduate programs. Thus, technical communication training merits deeper investigation and creative solutions. At the 2017 ASEE Meeting, we introduced the MIT School of Engineering Communication Lab, a discipline-specific technical communication service that is akin to a writing center, but embedded within engineering departments ​[9]​. By using the expertise of graduate student and postdoctoral peer coaches within a given discipline, the Communication Lab provides a scalable, content-aware solution with the benefits of just-in-time, one-on-one ​[10]​, and peer ​[11]​ training. When we first introduced this model, we offered easy-to-record metrics for the Communication Lab’s effectiveness (such as usage statistics and student and faculty opinion surveys), as are commonly used to assess writing centers ​[12], [13]​. Here we present a formal quantitative study of the effectiveness of Communication Lab coaching. We designed a pre-post test study for two related tasks: personal statements for applications to graduate school and graduate fellowships. We designed an analytic rubric with seven categories (strategic alignment, audience awareness, context, evidence, organization/flow, language mechanics, and visual impact) and tested it to ensure inter-rater reliability. Over one semester, we collected and anonymized 119 personal statement drafts from 47 unique Communication Lab clients across four different engineering departments. Peer coaches rubric-scored the drafts, and we developed a statistical model based on maximum likelihood to identify significant score changes in individual rubric categories across trajectories of sequential drafts. In addition, post-session surveys of clients and their peer coaches provided insight into clients’ qualitative experiences during coaching sessions. Taken together, our quantitative and qualitative findings suggest that our peer coaches are most effective in supporting the skills of organization/flow, strategic alignment, and providing appropriate evidence; this aligns with our program’s emphasis on supporting high-level communication skills. Our results also suggest that a major factor in coaching efficacy is coach-client discussion of major takeaways from a session: rubric category scores were more likely to improve across a drafting trajectory when a category had been identified as a takeaway. Hence, we show quantitative evidence that through collaborative conversations, technical peer coaches can guide clients to identify and effectively revise key areas for improvement. Finally, since we have gathered a sizable dataset and developed analytical tools, we have laid the groundwork for future quantitative writing assessments by both our program and others. We argue that although inter-rater variability poses a challenge, statistical methods and skill-based assessments of authentic communication tasks can provide both insights into student writing/revision ability and direction for improvement of communication resources. Introduction One of the greatest gaps in engineering education is the development of communication skills: degree accreditation agencies and employers alike identify communication as one of the most crucial skills ​[14]–[18]​, yet most graduates feel unprepared for the demands of professional communication ​[3], [18]​. To fill this gap, educational programs have often adopted curricular interventions such as technical communication courses or embedded communication tasks within technical courses ​[19]–[21]​. However, writing centers -co-curricular interventions that provide students with just-in-time support throughout their training -have been both underused and much less studied ​[9]​. We previously introduced the Communication Lab (Comm Lab), an adaptation of the writing center model specifically for STEM contexts, which originated in 2012 in a single department at the Massachusetts Institute of Technology (MIT) ​[9], [22]​. By training STEM graduate students and postdocs as peer coaches, the model leverages the educational benefits of peers’ first-hand experience with communication in the discipline ​[23]–[26]​, learning through authentic tasks ​[27]–[29]​, and just-in-time support. We described the Comm Lab’s original implementation within several MIT engineering departments in ​[9]​. Subsequently, we compared its adaptations across several different technical and liberal-arts institutions in ​[22]​. Our first publication underlined the affordability and flexibility of a peer coaching model, in contrast to a one-time curricular intervention. Likewise, our second publication highlighted the adaptability of the Comm Lab model to different institutional constraints and needs (e.g., service to undergraduates only ​versus​ both undergraduate and graduate students). Indeed, adaptation to local conditions is a core tenet of the model, and its success is demonstrated by the Comm Lab’s continued growth across both MIT departments and external institutions. The Communication Lab’s core pedagogical approach The Comm Lab’s coaching model emphasizes self-analysis and incorporation of feedback through revision. An appointment with a Comm Lab coach encourages the client to take an active role in analyzing their work and proposing solutions; the coach facilitates by asking open-ended questions and acting as a proxy for the client’s eventual, technical audience. A typical appointment of 30-60 minutes proceeds as follows: 1. The client and coach discuss the intended audience for the communication task and the client’s own strategic goals. 2. The coach suggests an activity that will help the client analyze their own work (such as distilling the three most important points they wish to convey), while the coach reviews the work. 3. The coach focuses first on reviewing high-level communication choices like argument and structure, but also assesses the client’s success in executing these according to field-specific expectations: e.g., is the logical flow of an argument technically sound? 4. Following assessment, the coach and client discuss the communication issues identified, compare examples from the field (which may include the coach’s own experiences), and model/practice strategies for revision. 5. The coach ensures that the client identifies priorities for revision on their own. In short, during a session, the coach models for the client a process for both high-level analysis and practical revision. Research on writing centers confirms numerous benefits of such peer learning experiences, including increased writer satisfaction, improved writing and revision processes, and improved course outcomes ​[30]​. Empirical research likewise highlights the advantage of peers with disciplinary knowledge who can address both rhetoric and content by, for example, challenging students’ technical claims and evidence ​[23]​. In other words, a “knowledgeable peer” ​[31]​ offers a combination of social-emotional, communication, and technical support. Our aims in designing a quantitative and qualitative study of the Communication Lab In this study, our primary research question was: is the Comm Lab succeeding in improving clients’ work according to our own metrics of success? I.e., do sessions bring clients closer to our standards for a given communication task, which are informed by both rhetorical principles and real-world field standards? To do so, we designed a quantitative, rubric-based, pre-post evaluation of authentic writing products: drafts for graduate school and graduate fellowship applications, assessed by authentic evaluators -a team of our own peer coaches. In order to build a broader picture of the client’s analytical and reflective experience, we complemented the quantitative core of the study by collecting qualitative reflections about the content of the coaching session. Overall, we argue that our study design provides useful qualitative and quantitative information about the effectiveness of the Comm Lab, despite the many limitations inherent in writing assessments. Writing studies experts agree that writing assessments are challenging: whether quantitative or qualitative, of writing centers in particular or the writing process more broadly, it is difficult to design direct, authentic assessments that concretely demonstrate student success or growth ​[12], [32]​. Our past publications ​[9], [22]​ offered typical indirect measures used by writing centers, such as repeat visits, client self-assessment, and faculty testimonials. While useful for program justification, such indirect metrics are clearly limited in their ability to concretely evaluate student growth ​[12], [13], [33]​. Direct assessments are complicated by three considerations: validity, reliability, and ethical limitations on truly scientific study design. Validity asks: does the assessment measure what it is supposed to measure? Reliability asks: can writing be consistently and quantitatively evaluated by different evaluators? Finally, ethics forbid writing centers from executing the classic “treatment/no treatment” experimental design: true negative controls would require denial of writing center access to students who want it. Due to these three constraints, “the typical evaluation of writing programs...usually fails to obtain statistically significant results” ​[34]​. For this reason, since roughly the 1990s, research on writing assessment and especially writing center assessment has focused on qualitative studies, despite the advantages of quantitative pre-post test design [26]. Nonetheless, we designed our study to maximize validity and reliability within these constraints by addressing the most important concerns and recommendations about both: First, most concerns about validity revolve ar",Quantitative Assessment of Students’ Revision Processes,https://www.semanticscholar.org/paper/db29afcd100888f6f7aa1b03ec6d427fc3fff1d2,"Background Nonmuscle invasive bladder cancer (NMIBC) accounts for 75% of bladder cancers. It is common and costly. Cost and detriment to patient outcomes and quality of life are driven by high recurrence rates and the need for regular invasive surveillance and repeat treatments. There is evidence that the quality of the initial surgical procedure (transurethral resection of bladder tumor [TURBT]) and administration of postoperative bladder chemotherapy significantly reduce cancer recurrence rates and improve outcomes (cancer progression and mortality). There is surgeon-reported evidence that TURBT practice varies significantly across surgeons and sites. There is limited evidence from clinical trials of intravesical chemotherapy that NMIBC recurrence rate varies significantly between sites and that this cannot be accounted for by differences in patient, tumor, or adjuvant treatment factors, suggesting that how the surgery is performed may be a reason for the variation. Objective This study primarily aims to determine if feedback on and education about surgical quality indicators can improve performance and secondarily if this can reduce cancer recurrence rates. Planned secondary analyses aim to determine what surgeon, operative, perioperative, institutional, and patient factors are associated with better achievement of TURBT quality indicators and NMIBC recurrence rates. Methods This is an observational, international, multicenter study with an embedded cluster randomized trial of audit, feedback, and education. Sites will be included if they perform TURBT for NMIBC. The study has four phases: (1) site registration and usual practice survey; (2) retrospective audit; (3) randomization to audit, feedback, and education intervention or to no intervention; and (4) prospective audit. Local and national ethical and institutional approvals or exemptions will be obtained at each participating site. Results The study has 4 coprimary outcomes, which are 4 evidence-based TURBT quality indicators: a surgical performance factor (detrusor muscle resection); an adjuvant treatment factor (intravesical chemotherapy administration); and 2 documentation factors (resection completeness and tumor features). A key secondary outcome is the early cancer recurrence rate. The intervention is a web-based surgical performance feedback dashboard with educational and practical resources for TURBT quality improvement. It will include anonymous site and surgeon-level peer comparison, a performance summary, and targets. The coprimary outcomes will be analyzed at the site level while recurrence rate will be analyzed at the patient level. The study was funded in October 2020 and began data collection in April 2021. As of January 2023, there were 220 hospitals participating and over 15,000 patient records. Projected data collection end date is June 30, 2023. Conclusions This study aims to use a distributed collaborative model to deliver a site-level web-based performance feedback intervention to improve the quality of endoscopic bladder cancer surgery. The study is funded and projects to complete data collection in June 2023. Trial Registration ClinicalTrials.org NCT05154084; https://clinicaltrials.gov/ct2/show/NCT05154084 International Registered Report Identifier (IRRID) DERR1-10.2196/42254","Audit, Feedback, and Education to Improve Quality and Outcomes in Transurethral Resection and Single-Instillation Intravesical Chemotherapy for Nonmuscle Invasive Bladder Cancer Treatment: Protocol for a Multicenter International Observational Study With an Embedded Cluster Randomized Trial",https://www.semanticscholar.org/paper/1f31a9f08d235f3d0892d3936091aa9ebc15da14,"Studies of writing revisions rarely focus on revision quality. To address this issue, we introduce a corpus of between-draft revisions of student argumentative essays, annotated as to whether each revision improves essay quality. We demonstrate a potential usage of our annotations by developing a machine learning model to predict revision improvement. With the goal of expanding training data, we also extract revisions from a dataset edited by expert proofreaders. Our results indicate that blending expert and non-expert revisions increases model performance, with expert data particularly important for predicting low-quality revisions.",Annotation and Classification of Sentence-level Revision Improvement,https://www.semanticscholar.org/paper/7ee7e580798e9177dc8ff9bb94c895cd3d4ab780,"Scientific publications are the primary means to communicate research discoveries, where the writing quality is of crucial importance. However, prior work studying the human editing process in this domain mainly focused on the abstract or introduction sections, resulting in an incomplete picture. In this work, we provide a complete computational framework for studying text revision in scientific writing. We first introduce arXivEdits, a new annotated corpus of 751 full papers from arXiv with gold sentence alignment across their multiple versions of revision, as well as fine-grained span-level edits and their underlying intentions for 1,000 sentence pairs. It supports our data-driven analysis to unveil the common strategies practiced by researchers for revising their papers. To scale up the analysis, we also develop automatic methods to extract revision at document-, sentence-, and word-levels. A neural CRF sentence alignment model trained on our corpus achieves 93.8 F1, enabling the reliable matching of sentences between different versions. We formulate the edit extraction task as a span alignment problem, and our proposed method extracts more fine-grained and explainable edits, compared to the commonly used diff algorithm. An intention classifier trained on our dataset achieves 78.9 F1 on the fine-grained intent classification task. Our data and system are released at tiny.one/arxivedits.",arXivEdits: Understanding the Human Revision Process in Scientific Writing,https://www.semanticscholar.org/paper/e53b0d9c061a1a108d87d79826c47cf77bac85d6,"Peer review constitutes a core component of scholarly publishing; yet it demands substantial expertise and training, and is susceptible to errors and biases. Various applications of NLP for peer reviewing assistance aim to support reviewers in this complex process, but the lack of clearly licensed datasets and multi-domain corpora prevent the systematic study of NLP for peer review. To remedy this, we introduce NLPeer– the first ethically sourced multidomain corpus of more than 5k papers and 11k review reports from five different venues. In addition to the new datasets of paper drafts, camera-ready versions and peer reviews from the NLP community, we establish a unified data representation and augment previous peer review datasets to include parsed and structured paper representations, rich metadata and versioning information. We complement our resource with implementations and analysis of three reviewing assistance tasks, including a novel guided skimming task.Our work paves the path towards systematic, multi-faceted, evidence-based study of peer review in NLP and beyond. The data and code are publicly available.",NLPeer: A Unified Resource for the Computational Study of Peer Review,https://www.semanticscholar.org/paper/54a316ecfb97352e55c5e85c06ccf7d013c4993b,"M a r t í n C A , A l o n s o J C , A l o n s o J A , P a l a c í n C , M a g a ñ a M , M a r t í n B . 2 0 0 7 . S e x b i a s e d j u v e n i l e s u r v i v a l i n a b i r d w i t h e x t r e m e s i z e d i m o r p h i s m , t h e G r e a t B u s t a r d O t i s t a r d a . J o u r n a l o f A v i a n B i o l o g y 3 8 : 3 3 5 – 3 4 6 . M a r t í n B . 2 0 0 8 . D i n á m i c a d e p o b l a c i ó n y v i a b i l i d a d d e l a a v u t a r d a c o m ú n e n l a C o m u n i d a d d e M a d r i d . P h D t h e s i s , U n i v e r s i d a d C o m p l u t e n s e d e M a d r i d , M a d r i d M a r t i n , G . R . , & S h a w , J . M . ( 2 0 1 0 ) . B i r d c o l l i s i o n s w i t h p o w e r l i n e s : F a i l i n g t o s e e t h e w a y a h e a d ? B i o l o g i c a l C o n s e r v a t i o n , 1 4 3 ( 1 1 ) , 2 6 9 5 – 2 7 0 2 . h t t p s : / / d o i . o r g / 1 0 . 1 0 1 6 / j . b i o c o n . 2 0 1 0 . 0 7 . 0 1 4 P a l a c i n , C . , A l o n s o , J . C . , M a r t i n , C . A . a n d A l o n s o , J . A . 2 0 1 2 . T h e i m p o r t a n c e o f t r a d i t i o n a l f a r m l a n d a r e a s f o r s t e p p e b i r d s : a c a s e s t u d y o f m i g r a n t f e m a l e g r e a t b u s t a r d s O t i s t a r d a i n S p a i n . I b i s , 1 5 4 : 8 5 9 5 . S h a w , J . M . , 2 0 0 9 . T h e E n d o f t h e L i n e f o r S o u t h A f r i c a ’ s N a t i o n a l B i r d ? M o d e l l i n g P o w e r   L i n e C o l l i s i o n R i s k f o r t h e B l u e C r a n e . M a s t e r s T h e s i s P e r c y F i t z p a t r i c k I n s t i t u t e o f A f r i c a n O r n i t h o l o g y , U n i v e r s i t y o f C a p e T o w n .",Evidence of Impact,https://www.semanticscholar.org/paper/f5d005a525caf22a4b20326d14d9d403978893f5,"Wikis have become a popular online collaboration platform. Their open nature can, and indeed does, lead to a large number of editors of their articles, who create a large number of revisions. These editors make various types of edits on an article, from minor ones such as spelling correction and text formatting, to major revisions such as new content introduction, whole article re-structuring, etc. Given the enormous number of revisions, it is difficult to identify the type of contributions made in these revisions through human observation alone. Moreover, different types of edits imply different edit significance. A revision that introduces new content is arguably more significant than a revision making a few spelling corrections. By taking edit types into account, better measurements of edit significance can be produced. This paper proposes a method for categorizing and presenting edits in an intuitive way and with a flexible measure of significance of each individual editor's contributions.",What did they do? Deriving high-level edit histories in Wikis,https://www.semanticscholar.org/paper/404062ad4b4b38341517276357d6539fa591c6c7,"Peer review is under pressure. Without fair, transparent and efficient peer review we cannot ensure the right proposals get funded and the correct manuscripts get published. In the era of Open Access, which is driving an exponential increase in the number of submitted publications, how we carry out peer review is becoming increasingly important and how we find reviewers is coming under scrutiny. The current methods are slow and produce bias pools of reviewers. As such we need an improved way. At Prophy we have developed a state-of-the-art referee finder that can find experts to review any manuscript from any scientific field in seconds. Then through post-processing filters we can find appropriate candidate referees who are most likely to review a paper, whilst highlighting important conflicts of interest through our complex citation networks. These methods can ensure fair and independent experts who can review interdisciplinary papers from any discipline. These methods are being delivered through APIs and the editorial workflow of editors ensure the right people get access to these tools. Finally, as large-language models improve, so does Prophy and as such we will be looking to drive real innovation in this area in years to come.","Prophy: An automated reviewer finder to improve the efficiency, diversity and quality of reviews",https://www.semanticscholar.org/paper/617a078bc2310a2e9814981f58c713a4a6342340,"Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR machine learning conference (1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature journals, 39.23% for ICLR) is comparable to the overlap between two human reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The overlap between GPT-4 and human reviewers is larger for the weaker papers. We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half (57.4%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations.",Can large language models provide useful feedback on research papers? A large-scale empirical analysis,https://www.semanticscholar.org/paper/f2209eb5ac6747319a29b87dedabb97770be3243,,,,novel
"Develop a system that employs **decision tree sampling** to create a structured representation of scientific contributions with the aim **to boost scientific innovation**. By using **evaluation metrics** to assess the impact and relevance of different research contributions, this system would help researchers identify novel and impactful works. This approach would not only enhance the understanding of key features driving innovation but also facilitate the discovery of underexplored research areas, promoting a more comprehensive and effective innovation ecosystem.","['Computer Science', 'Information Retrieval', 'Recommender Systems', 'Scientific Innovation', 'Data Mining', 'Human-Computer Interaction']","This paper questions the evaluation of innovation systems and innovation measurements and the effectiveness of innovation policies applied at the territorial level by assessing whether the existing European regional scoreboard is effective in providing accurate inputs for decision-makers in mountainous regions. The aim of the research is to provide, through comparative analysis by using statistical multi-methods of two mountainous macro-regions (the Alps and the Carpathians), a possible and available path to develop novel perspectives and alternative views on innovation systems’ performance for informed and territorial-based policy making by using the indicators of the Regional Innovation Scoreboard. The methodology used includes descriptive statistics, chi-square bivariate test, Student’s t test, one-way ANOVA with Bonferroni post hoc multiple comparisons, multilinear regression analysis, and decision tree with CRT (classification and regression trees) algorithm. Our results emphasize the similarities and differences between the Alpine and Carpathian mountain regions, find the best predictors for each mountain region, and provide a scientific basis for the development of a holistic approach linking measurement theory, innovation systems, innovation policies, and their territorial approach toward sustainable development of mountain areas. The paper’s contribution is relevant in the context of remote, rural, and mountain areas, which are usually left behind in terms of innovation chances and in the context of the COVID-19 aftermath with budget constraints. The present results are pertinent for designing effective smart specialization strategies in these regions due to the difficulties that most remote areas and less developed regions are facing in developing innovation policies.",Do Innovation Metrics Reflect Sustainable Policy Making in Europe? A Comparative Study Case on the Carpathian and Alpine Mountain Regions,https://www.semanticscholar.org/paper/56d512c66bc4e25d3b8b8bb986977ec833e5ba3e,"The use of artificial intelligence (AI) in working environments with individuals, known as Human-AI Collaboration (HAIC), has become essential in a variety of domains, boosting decision-making, efficiency, and innovation. Despite HAIC's wide potential, evaluating its effectiveness remains challenging due to the complex interaction of components involved. This paper provides a detailed analysis of existing HAIC evaluation approaches and develops a fresh paradigm for more effectively evaluating these systems. Our framework includes a structured decision tree which assists to select relevant metrics based on distinct HAIC modes (AI-Centric, Human-Centric, and Symbiotic). By including both quantitative and qualitative metrics, the framework seeks to represent HAIC's dynamic and reciprocal nature, enabling the assessment of its impact and success. This framework's practicality can be examined by its application in an array of domains, including manufacturing, healthcare, finance, and education, each of which has unique challenges and requirements. Our hope is that this study will facilitate further research on the systematic evaluation of HAIC in real-world applications.",Evaluating Human-AI Collaboration: A Review and Methodological Framework,https://www.semanticscholar.org/paper/00779a37dc55a6dc1e3fee00baf65714a80f7a98,"ResearchRabbit is a scholarly publication discovery tool supported by artificial intelligence (AI). It was developed in 2021 by a team of three in Seattle [1]. This tool lets users discover publications related to one or more seed publications with the help of visualization maps and lists of earlier, later, and similar publications. ResearchRabbit is designed to support the workflow of unstructured searching while providing a left-to-right trail from the original publication(s) through any selected authors or publications. These trails, which can run as deep as rabbit holes, suggest the origin of the tool’s name. To start using ResearchRabbit, users first need to create an account. Then they need to create a collection and add at least one publication. The more publications that are added, the better ResearchRabbit can understand users’ interests and generate recommendations similar to the contents of the collection. Publications can be added either by uploading a RIS or BibTeX file or by using ResearchRabbit’s search, powered by PubMed, if users are searching the medical sciences, or Semantic Scholar, for any other subject area. While ResearchRabbit uses PubMed’s and Semantic Scholar’s search engines, the company claims its unique database of “100s of millions of academic articles” is second in size only to Google Scholar [2]. Once publications are in a collection, ResearchRabbit’s algorithm will begin generating recommendations. These recommendations can be explored through two modes: 1) by Papers that are Similar work, Earlier work, or Later work or 2) by People that provide additional publications that These authors or Suggested authors have published (Figure 1). These recommendations are depicted using visualization maps. Fig. 1 Different exploration modes",ResearchRabbit,https://www.semanticscholar.org/paper/6740b72b134f194553d9d3d2b23b0dba5eb9667c,"The strategic relevance of innovation and scientific research has amplified the attention towards the definition of quality in research practice. However, despite the proliferation of evaluation metrics and procedures, there is a need to go beyond bibliometric approaches and to identify, more explicitly, what constitutes good research and which are its driving factors or determinants. This article reviews specialized research policy, science policy and scientometrics literature to extract critical dimensions associated with research quality as presented in a vast although fragmented theory background. A literature-derived framework of research quality attributes is, thus, obtained, which is subject to an expert feedback process, involving scholars and practitioners in the fields of research policy and evaluation. The results are represented by a structured taxonomy of 66 quality attributes providing a systemic definition of research quality. The attributes are aggregated into a three-dimensional framework encompassing research design (ex ante), research process (in-process) and research impact (ex post) perspectives. The main value of the study is to propose a literature-derived and comprehensive inventory of quality attributes and perspectives of evaluation. The findings can support further theoretical developments and research policy discussions on the ultimate drivers of quality and impact of scientific research. The framework can be also useful to design new exercises or procedures of research evaluation based on a multidimensional view of quality.","What Is Quality in Research? Building a Framework of Design, Process and Impact Attributes and Evaluation Perspectives",https://www.semanticscholar.org/paper/c052b687db6fe509914ace4dfd4d81d81d7a296a,"Citations allow quickly identifying related research. If multiple publications are selected as seeds, specific suggestions for related literature can be made based on the number of incoming and outgoing citation links to this selection. Interactively adding recommended publications to the selection refines the next suggestion and incrementally builds a relevant collection of publications. Following this approach, the paper presents a search and foraging approach, PUREsuggest, which combines citation-based suggestions with augmented visualizations of the citation network. The focus and novelty of the approach is, first, the transparency of how the rankings are explained visually and, second, that the process can be steered through user-defined keywords, which reflect topics of interests. The system can be used to build new literature collections, to update and assess existing ones, as well as to use the collected literature for identifying relevant experts in the field. We evaluated the recommendation approach through simulated sessions and performed a user study investigating search strategies and usage patterns supported by the interface.",PUREsuggest: Citation-based Literature Search and Visual Exploration with Keyword-controlled Rankings,https://www.semanticscholar.org/paper/813fccf0b9f37dfc7bb6df0690502d72468b76c7,"The article aimed to develop a systematic review of the scientific literature about indicators for the evaluation of science, technology and innovation activities. For this, the Web of Science, Scopus and Google Scholar databases were used. Through the application of the SysteRe-HSS methodology, 96 publications were selected that formed the basis for a descriptive model of the science, technology and innovation indicators. The results of the research showed that there is a predominance of indicators related to the evaluation of innovation activities, human resources allocated to the activity of science, technology and innovation, financial resources and investments in research plus development, and indicators related to bibliometrics and scientometrics. However, challenges are faced related to measuring indicators of social innovation, linking insights from existing innovation measurement approaches with the essential features of social innovation, measuring the impact of social appropriation practices of science and technology, and the next generation metrics, responsible metrics and evaluation for open science, as well as alternative indicators for the evaluation of the social impact of research in web 2.0.","Indicators for the Evaluation of Science, Technology and Innovation Activities: A Systematized Review",https://www.semanticscholar.org/paper/dddb8c1e30c08d4b04bccc180380880fb89a459d,"Research impact of a scientist can be defined using these dimensions: level of scientific results; level of participation in the organization of sciences; level of  knowledge transfer; and level of technology innovations. We skip the last item because of difficulties in finding data on technology innovations. We present an approach to measuring the level of research results, for the first time, to the best of our  knowledge. The approach involves a taxonomy of the research domain. The level of results is evaluated according to the taxonomy ranks of the subjects that have emerged or  have been crucially transformed due to the results by the scientist under consideration. Two conventional dimensions, (a) citation metrics and (b)  merit metrics, are taken too. To aggregate individual criteria we develop an in-house  criteria-weighting method Linstrat. The method's criterion is that the strata are as tight as possible. We take a sample of thirty scientists in data  analysis and machine learning, and the ACM Computing Classification System 2012. Empirical results: (a) Hirsch citation index gets a zero weight; (b) when combining  the scales for Citation, Merit, and Taxonomic rank, the latter gets the weight of 80%; (c) the three dimensions are almost  uncorrelated.",Using taxonomies and aggregate rankings for measuring research impact.,https://www.semanticscholar.org/paper/11ce2b37e411fcf9fe702ad3a9749ac8b6ae2db7,"Finding potential collaborators has become a challenge due to the growing number of scientists in organizations such as universities, research institutes, or companies. Collaboration Recommendation Systems (CRSs) have been developed to help researchers identify possible collaboration partners, but they often rely on citation graphs or paper abstracts which may not be readily available in organizational databases or online sources. However, scientific publication titles provide consistent bibliometric data that can provide insights into research areas. TOMOSCO is a topic modelling framework that uses transformer-based methods to extract research area information from small amounts of text, such as publication titles or brief project descriptions. TOMOSCO can classify, cluster, and match research topics across different disciplines, uncovering relationships among scientists and suggesting potential interdisciplinary collaborations. In experiments, TOMOSCO was able to identify existing collaborations with over 90% accuracy based solely on publication titles and propose new collaborations based on previously unseen publications and project descriptions.",Exploiting Topic Modelling for the Identification of Untapped Scientific Collaborations,https://www.semanticscholar.org/paper/ed12a2ed110c00c70a57b78fc476fda98b1231ca,"There has been a proliferation of new research discovery tools that aid scientists in finding relevant publications. To obtain a general overview of this development, this article generates a conceptual typology of all possible research discovery tools by drawing from the information-theoretical concepts of redundancy/variety. Bibliometric links between scholarly publications can thus exhibit ‘redundancy’ (i.e. expectable linkages between academic works) or ‘variety’ (i.e. original co-occurrence patterns). On the redundancy-reproducing end of the typology are machines that harness extant co-citations or keyword queries, such as academic search engines and paper recommender systems. The variety end of the spectrum harbours services that enable categorial browsing or that suggest publications randomly, such as journals’ tables of contents or random paper bots. The typology has implications for understanding how the design of research discovery platforms may ultimately shape aggregate citational networks of science.",A typology of research discovery tools,https://www.semanticscholar.org/paper/760745c9357117c0dc7d28f21566eccec9d2beed,"In the age of information abundance, discovering personalized content is paramount to engaging users on a variety of topics. The engine goes beyond traditional approaches by not only dynamically adjusting recommendations to the users' historical preferences, but also actively encouraging exploration of interdisciplinary areas by referencing them with other genre or domains. The recommendation engine uses a multi-layer strategy that includes collaborative and content-based filtering, hybrid models and advanced diversity metrices. It transcends the general boundaries of user preferences and fosters randomness and novelty through cross-domain recommendations. The system adapts to changing user interests by suggesting content in various formats such as articles, podcasts, videos and simulations. Community integration plays a key role, leveraging collaborative filtering to discover community-curated lists and hidden gems across various categories. Real-time trend research allows users to stay informed and engaged with the latest developments, maintaining a dynamic and up-to-date content ecosystem. Ethical considerations, transparency and user-friendly interfaces are essential elements to address bias and provide explanations for recommendations. Through continuous user feedback and iterative improvements, the novel algorithmic recommendation engine for diverse content discovery aims to redefine content discovery and foster a sense of exploration, curiosity and learning across a wide range of knowledge domains.",Novel Algorithmic Recommendation Engine for Diverse Content Discovery,https://www.semanticscholar.org/paper/60e54892d78a056b957508feab0072cca4b5fac8,novel
"To <b style=""background-color:#F0FFF0;"">improve human-ai team decision-making</b>, researchers can use <b style=""background-color:#FFFACD;"">selective explanations</b> that align with individual user preferences. By conducting <b style=""background-color:#E6E6FA;"">mixed-method user studies</b>, researchers can gather qualitative and quantitative data on how these personalized explanations affect user trust, satisfaction, and decision-making performance. Participants will interact with an AI system that provides explanations tailored to their input, allowing for a more nuanced understanding of how personalized explanations can enhance collaborative decision-making.","['Human-Computer Interaction', 'Explainable AI', 'Human-AI Collaboration', 'User Studies', 'Decision-Making Systems']","While a vast collection of explainable AI (XAI) algorithms has been developed in recent years, they have been criticized for significant gaps with how humans produce and consume explanations. As a result, current XAI techniques are often found to be hard to use and lack effectiveness. In this work, we attempt to close these gaps by making AI explanations selective ---a fundamental property of human explanations---by selectively presenting a subset of model reasoning based on what aligns with the recipient's preferences. We propose a general framework for generating selective explanations by leveraging human input on a small dataset. This framework opens up a rich design space that accounts for different selectivity goals, types of input, and more. As a showcase, we use a decision-support task to explore selective explanations based on what the decision-maker would consider relevant to the decision task. We conducted two experimental studies to examine three paradigms based on our proposed framework: in Study 1, we ask the participants to provide critique-based or open-ended input to generate selective explanations (self-input). In Study 2, we show the participants selective explanations based on input from a panel of similar users (annotator input). Our experiments demonstrate the promise of selective explanations in reducing over-reliance on AI and improving collaborative decision making and subjective perceptions of the AI system, but also paint a nuanced picture that attributes some of these positive effects to the opportunity to provide one's own input to augment AI explanations. Overall, our work proposes a novel XAI framework inspired by human communication behaviors and demonstrates its potential to encourage future work to make AI explanations more human-compatible.",Selective Explanations: Leveraging Human Input to Align Explainable AI,https://www.semanticscholar.org/paper/e2de545750c859b9f7f62919e2b94f9e92ad9147,"Machine learning systems are often hard to investigate and intransparent in their decision making . Explainable Artiﬁcial Intelligence (XAI) tries to make these systems more transparent. However, most work in the ﬁeld focuses on technical aspects like maximizing metrics. The human aspects of explainability are often neglected. In this work, we present personalized explanations, which instead focus on the user. Personalized explanations can be adapted to individual users to be as useful and relevant as possible. They can be interacted with to give users the ability to engage in an explanatory dialog with the system. Finally, they should also protect user data to increase the trust in the explanation system.",Personalized Explanations,https://www.semanticscholar.org/paper/81bc1b91d74e84555a5897a80842beb5a426aa37,"AI explanations have been increasingly used to help people better utilize AI recommendations in AI-assisted decision making. While numerous technical transparency approaches have been established, a human-centered perspective is needed for understanding how human decision makers use and process AI explanations. In my thesis, I start with an empirical exploration of how AI explanations shape the way people understand and utilize AI decision aids. Next, I move to the time‑evolving nature of AI explanations, exploring how explanation changes due to AI model updates affect human decision makers’ perception and usage of AI models. Lastly, I construct computational human behavior models to gain a more quantitative understandings of human decision makers’ cognitive interactions with AI explanations. I conclude with future work on carefully identifying user needs for explainable AI in an era when AI models are becoming more complex and human-AI collaboration scenarios are increasingly diversified.",Human-Centered Evaluation of Explanations in AI-Assisted Decision-Making,https://www.semanticscholar.org/paper/af46821116b0b90f9043c50ee153d234d13b2533,"Explainability (XAI) has matured in recent years to provide more human-centered explanations of AI-based decision systems. While static explanations remain predominant, interactive XAI has gathered momentum to support the human cognitive process of explaining. However, the evidence regarding the benefits of interactive explanations is unclear. In this paper, we map existing findings by conducting a detailed scoping review of 48 empirical studies in which interactive explanations are evaluated with human users. We also create a classification of interactive techniques specific to XAI and group the resulting categories according to their role in the cognitive process of explanation: ""selective"", ""mutable"" or ""dialogic"". We identify the effects of interactivity on several user-based metrics. We find that interactive explanations improve perceived usefulness and performance of the human+AI team but take longer. We highlight conflicting results regarding cognitive load and overconfidence. Lastly, we describe underexplored areas including measuring curiosity or learning or perturbing outcomes.","On Selective, Mutable and Dialogic XAI: a Review of What Users Say about Different Types of Interactive Explanations",https://www.semanticscholar.org/paper/5694c7a8759847a4f13a5c0ba7ee37297372f3ca,"Explanation of artificial intelligence (AI) decision-making has become an important research area in human–computer interaction (HCI) and computer-supported teamwork research. While plenty of research has investigated AI explanations with an intent to improve AI transparency and human trust in AI, how AI explanations function in teaming environments remains unclear. Given that a major benefit of AI giving explanations is to increase human trust understanding how AI explanations impact human trust is crucial to effective human-AI teamwork. An online experiment was conducted with 156 participants to explore this question by examining how a teammate’s explanations impact the perceived trust of the teammate and the effectiveness of the team and how these impacts vary based on whether the teammate is a human or an AI. This study shows that explanations facilitate trust in AI teammates when explaining why AI disobeyed humans’ orders but hindered trust when explaining why an AI lied to humans. In addition, participants’ personal characteristics (e.g., their gender and the individual’s ethical framework) impacted their perceptions of AI teammates both directly and indirectly in different scenarios. Our study contributes to interactive intelligent systems and HCI by shedding light on how an AI teammate’s actions and corresponding explanations are perceived by humans while identifying factors that impact trust and perceived effectiveness. This work provides an initial understanding of AI explanations in human-AI teams, which can be used for future research to build upon in exploring AI explanation implementation in collaborative environments.","I Know This Looks Bad, But I Can Explain: Understanding When AI Should Explain Actions In Human-AI Teams",https://www.semanticscholar.org/paper/6e45a9162a3010d198c9740e9e744f7f42317461,"Explainability techniques are rapidly being developed to improve human-AI decision-making across various cooperative work settings. Consequently, previous research has evaluated how decision-makers collaborate with imperfect AI by investigating appropriate reliance and task performance with the aim of designing more human-centered computer-supported collaborative tools. Several human-centered explainable AI (XAI) techniques have been proposed in hopes of improving decision-makers' collaboration with AI; however, these techniques are grounded in findings from previous studies that primarily focus on the impact of incorrect AI advice. Few studies acknowledge the possibility of the explanations being incorrect even if the AI advice is correct. Thus, it is crucial to understand how imperfect XAI affects human-AI decision-making. In this work, we contribute a robust, mixed-methods user study with 136 participants to evaluate how incorrect explanations influence humans' decision-making behavior in a bird species identification task, taking into account their level of expertise and an explanation's level of assertiveness. Our findings reveal the influence of imperfect XAI and humans' level of expertise on their reliance on AI and human-AI team performance. We also discuss how explanations can deceive decision-makers during human-AI collaboration. Hence, we shed light on the impacts of imperfect XAI in the field of computer-supported cooperative work and provide guidelines for designers of human-AI collaboration systems.",The Impact of Imperfect XAI on Human-AI Decision-Making,https://www.semanticscholar.org/paper/1f83050a22da19fe4c0e7b0ac04768c92ecd6338,"Recommender systems are ubiquitous and shape the way users access information and make decisions. As these systems become more complex, there is a growing need for transparency and interpretability. In this article, we study the problem of generating and visualizing personalized explanations for recommender systems that incorporate signals from many different data sources. We use a flexible, extendable probabilistic programming approach and show how we can generate real-time personalized recommendations. We then turn these personalized recommendations into explanations. We perform an extensive user study to evaluate the benefits of explanations for hybrid recommender systems. We conduct a crowd-sourced user study where our system generates personalized recommendations and explanations for real users of the last.fm music platform. First, we evaluate the performance of the recommendations in terms of perceived accuracy and novelty. Next, we experiment with (1) different explanation styles (e.g., user-based, item-based), (2) manipulating the number of explanation styles presented, and (3) manipulating the presentation format (e.g., textual vs. visual). We also apply a mixed-model statistical analysis to consider user personality traits as a control variable and demonstrate the usefulness of our approach in creating personalized hybrid explanations with different style, number, and format. Finally, we perform a post analysis that shows different preferences for explanation styles between experienced and novice last.fm users.",Generating and Understanding Personalized Explanations in Hybrid Recommender Systems,https://www.semanticscholar.org/paper/b4193993a9efaaec661e7be0f25bc7e1e8f4dd79,"Explainable AI (XAI) is widely viewed as a sine qua non for ever-expanding AI research. A better understanding of the needs of XAI users, as well as human-centered evaluations of explainable models are both a necessity and a challenge. In this paper, we explore how HCI and AI researchers conduct user studies in XAI applications based on a systematic literature review. After identifying and thoroughly analyzing 85 core papers with human-based XAI evaluations over the past five years, we categorize them along the measured characteristics of explanatory methods, namely trust, understanding, fairness, usability , and human-AI team performance . Our research shows that XAI is spreading more rapidly in certain application domains, such as recommender systems than in others, but that user evaluations are still rather sparse and incorporate hardly any insights from cognitive or social sciences. Based on a comprehensive discussion of best practices, i.e., common models, design choices, and measures in user studies, we propose practical guidelines on designing and conducting user studies for XAI researchers and practitioners. Lastly, this survey also highlights several open research directions, particularly linking psychological science and human-centered XAI.",Towards Human-centered Explainable AI: User Studies for Model Explanations,https://www.semanticscholar.org/paper/1b82af66272e978d160cc95ef37363de2f2fb0b0,"Explainable AI (XAI) is widely viewed as a sine qua non for ever-expanding AI research. A better understanding of the needs of XAI users, as well as human-centered evaluations of explainable models are both a necessity and a challenge. In this paper, we explore how human-computer interaction (HCI) and AI researchers conduct user studies in XAI applications based on a systematic literature review. After identifying and thoroughly analyzing 97 core papers with human-based XAI evaluations over the past five years, we categorize them along the measured characteristics of explanatory methods, namely trust, understanding, usability, and human-AI collaboration performance. Our research shows that XAI is spreading more rapidly in certain application domains, such as recommender systems than in others, but that user evaluations are still rather sparse and incorporate hardly any insights from cognitive or social sciences. Based on a comprehensive discussion of best practices, i.e., common models, design choices, and measures in user studies, we propose practical guidelines on designing and conducting user studies for XAI researchers and practitioners. Lastly, this survey also highlights several open research directions, particularly linking psychological science and human-centered XAI.",Towards Human-Centered Explainable AI: A Survey of User Studies for Model Explanations,https://www.semanticscholar.org/paper/5b60cfa5ada16c22566e1eea48b166a861391afd,"Recent years have witnessed the growing literature in empirical evaluation of explainable AI (XAI) methods. This study contributes to this ongoing conversation by presenting a comparison on the effects of a set of established XAI methods in AI-assisted decision making. Based on our review of previous literature, we highlight three desirable properties that ideal AI explanations should satisfy — improve people’s understanding of the AI model, help people recognize the model uncertainty, and support people’s calibrated trust in the model. Through three randomized controlled experiments, we evaluate whether four types of common model-agnostic explainable AI methods satisfy these properties on two types of AI models of varying levels of complexity, and in two kinds of decision making contexts where people perceive themselves as having different levels of domain expertise. Our results demonstrate that many AI explanations do not satisfy any of the desirable properties when used on decision making tasks that people have little domain expertise in. On decision making tasks that people are more knowledgeable, the feature contribution explanation is shown to satisfy more desiderata of AI explanations, even when the AI model is inherently complex. We conclude by discussing the implications of our study for improving the design of XAI methods to better support human decision making, and for advancing more rigorous empirical evaluation of XAI methods.",Effects of Explanations in AI-Assisted Decision Making: Principles and Comparisons,https://www.semanticscholar.org/paper/be80390dca18a9972815e58c7912a3cece4ab86a,not novel
Integrate **machine learning-based anomaly detection** techniques into AI systems to **improve human-ai team decision-making**. This system will monitor decision-making processes in real-time to detect and explain anomalies or inconsistencies in AI recommendations. **Integration into real-world settings** will ensure the system's practical applicability and effectiveness in various decision-making scenarios. The evaluation will focus on real-world use cases to validate the system's ability to enhance trust and accuracy in human-AI collaborations.,"['Artificial Intelligence', 'Explainable AI', 'Human-AI Collaboration', 'Decision-Making', 'Machine Learning', 'Anomaly Detection']","Time series anomaly detection is a critical machine learning task for numerous applications, such as finance, healthcare, and industrial systems. However, even high-performed models may exhibit potential issues such as biases, leading to unreliable outcomes and misplaced confidence. While model explanation techniques, particularly visual explanations, offer valuable insights to detect such issues by elucidating model attributions of their decision, many limitations still exist -- They are primarily instance-based and not scalable across dataset, and they provide one-directional information from the model to the human side, lacking a mechanism for users to address detected issues. To fulfill these gaps, we introduce HILAD, a novel framework designed to foster a dynamic and bidirectional collaboration between humans and AI for enhancing anomaly detection models in time series. Through our visual interface, HILAD empowers domain experts to detect, interpret, and correct unexpected model behaviors at scale. Our evaluation with two time series datasets and user studies demonstrates the effectiveness of HILAD in fostering a deeper human understanding, immediate corrective actions, and the reliability enhancement of models.",A Reliable Framework for Human-in-the-Loop Anomaly Detection in Time Series,https://www.semanticscholar.org/paper/41a094747f24e8296ed5b836b4d9f2bc019b0ae6,"Real-time Anomaly Detection is of great importance in industrial applications in order to have high-quality production and avoid downtime or failure of the system. In this paper, we study the application of anomaly detection over the multivariate data collected from Glass Production Industry. Our experiments utilize and compare different Unsupervised multivariate time series Anomaly Detection and Localization algorithms that have already demonstrated significant results on the state-of-the-art data sets. We propose a two-level multivariate anomaly detection approach that not only detects anomalous events in the production line but also categorize the different type of anomalies based on statistical pattern recognition. Furthermore, we localize the anomalous sensors by utilizing Explainable-AI approaches to help better decision-making in glass production monitoring. In this work, we propose an efficient pipeline for Anomaly Detection, Categorization and Localization which the experiments show promising results.",Explainable Unsupervised Multi-Sensor Industrial Anomaly Detection and Categorization,https://www.semanticscholar.org/paper/11b68b28e99cf710203340de069c66fbbcdb2990,"Adaptive machine learning models are revolutionizing real-time financial fraud prevention in dynamic environments, offering unparalleled accuracy and responsiveness to evolving fraud patterns. Financial institutions face constant threats from increasingly sophisticated fraud schemes that adapt and change over time. Traditional static models often fall short in addressing these rapidly shifting threats, necessitating the adoption of adaptive machine learning techniques. Adaptive machine learning models are designed to evolve continuously by learning from new data and adjusting to emerging fraud patterns. These models employ advanced algorithms, such as reinforcement learning, online learning, and deep learning, to maintain their effectiveness in detecting and preventing fraud. Reinforcement learning algorithms optimize detection strategies by receiving feedback from their actions, continually improving their decision-making processes. Online learning algorithms update models incrementally as new transaction data becomes available, ensuring that the models remain current and responsive. One of the key strengths of adaptive machine learning models is their ability to process vast amounts of data in real time. By leveraging technologies such as neural networks and ensemble learning, these models can analyze complex datasets, identify subtle anomalies, and detect fraudulent activities with high precision. Real-time data processing capabilities enable immediate detection and response to suspicious transactions, significantly reducing the risk of financial losses. Adaptive models also incorporate anomaly detection techniques to identify deviations from normal transaction behavior. By constantly learning from the latest data, these models can recognize previously unseen fraud patterns, providing a robust defense against novel threats. Additionally, the integration of explainable AI (XAI) techniques ensures that the decision-making processes of these models are transparent and interpretable, fostering trust and compliance with regulatory requirements. Implementing adaptive machine learning models for real-time fraud prevention involves addressing challenges such as data quality, computational efficiency, and model interpretability. Financial institutions must ensure the availability of high-quality data and invest in robust computational infrastructure to support real-time processing. Furthermore, adopting explainable AI techniques enhances model transparency and regulatory compliance. In conclusion, adaptive machine learning models offer a dynamic and effective solution for real-time financial fraud prevention. By continuously learning and adapting to new data, these models provide a resilient defense against evolving fraud schemes, enhancing the security and integrity of financial transactions. This adaptive approach not only mitigates financial risks but also strengthens the overall trustworthiness of financial systems.",Adaptive machine learning models: Concepts for real-time financial fraud prevention in dynamic environments,https://www.semanticscholar.org/paper/1903ef02b963217f1096835c67029c6e655df6ce,"With the rapid development of decision aids that are driven by AI models, the practice of AI-assisted decision making has become increasingly prevalent. To improve the human-AI team performance in decision making, earlier studies mostly focus on enhancing humans' capability in better utilizing a given AI-driven decision aid. In this paper, we tackle this challenge through a complementary approach—we aim to train ""behavior-aware AI"" by adjusting the AI model underlying the decision aid to account for humans' behavior in adopting AI advice. In particular, as humans are observed to accept AI advice more when their confidence in their own judgement is low, we propose to train AI models with a human-confidence-based instance weighting strategy, instead of solving the standard empirical risk minimization problem. Under an assumed, threshold-based model characterizing when humans will adopt the AI advice, we first derive the optimal instance weighting strategy for training AI models. We then validate the efficacy and robustness of our proposed method in improving the human-AI joint decision making performance through systematic experimentation on synthetic datasets. Finally, via randomized experiments with real human subjects along with their actual behavior in adopting the AI advice, we demonstrate that our method can significantly improve the decision making performance of the human-AI team in practice.",Designing Behavior-Aware AI to Improve the Human-AI Team Performance in AI-Assisted Decision Making,https://www.semanticscholar.org/paper/65edbeeb2f943030833cfdb3c4a1f1d837040a6c,"In a world where the use of AI is growing and evolving, where will we be in 5 years? 10 years? 20 years? What role will AI play in our society, and how will humans and AI interact? While there will undoubtedly be scenarios where AI systems will be able to outperform humans, there will also continue to be instances where humans will be a critical part of the process. As researchers explore improvements to AI systems, we also need to explore the interplay between humans and AI, and continue to evolve our understanding of how humans and AI systems can work together, effectively harnessing the benefits of both systems [3]. Designing effective interaction between the human and the AI systems is critical for future use of Human-AI systems [1]. Merely building an AI system that blindly sends recommendations to users has been shown in some cases to decrease human performance [2]. Different models can also have differential impact on user's trust of the model, adherence to the recommendation, and can impact bias in decision making tasks. This talk will highlight important directions for Human-AI research.",Does My AI Help or Hurt? Exploring Human-AI Complementarity,https://www.semanticscholar.org/paper/b8531b916bbb73590310c020c0d9b2e28642d52c,"Artificial Intelligence (AI) has transformed the way human-computer interaction (HCI) teams are able to collaborate and coordinate in various domains, including aviation. AI’s transformative capabilities can enhance teamwork, efficiency, and safety, particularly in risk management. AI’s ability to process vast amounts of data and provide real-time insights enables informed decision-making and automation of repetitive tasks in aviation. By combining the strengths of AI and humans, outlined in our modified version of the ‘HABA-MABA’ framework, a dynamic teamwork relationship emerges, provided roles are successfully allocated. AI systems are able to act as intelligent assistants, offering timely recommendations, fostering effective communication, and facilitating coordination among crew members. Its adaptability and capacity for learning improve collaboration abilities, tailoring strategies to meet the team’s specific needs. This paper explores the theories, considerations, and implications of human-AI teams in aviation, highlighting potential benefits, training recommendations, and future research directions. While human-AI teams offer numerous benefits, addressing the risks, limitations, and ethical considerations is crucial to ensuring safe and efficient operations. Future research must prioritize transparency, explainability, adaptability, and real-world testing to unlock the full potential of human-AI teams and foster successful integration across diverse domains.",Human-AI Teams in Aviation: Considerations from Human Factors Human-AI Teams in Aviation: Considerations from Human Factors and Team Science and Team Science,https://www.semanticscholar.org/paper/63fb5aef1f0c2547d893b59e4c7d582809dda066,"The true potential of human-AI collaboration lies in exploiting the complementary capabilities of humans and AI to achieve a joint performance superior to that of the individual AI or human, i.e., to achieve complementary team performance (CTP). To realize this complementarity potential, humans need to exercise discretion in following AI 's advice, i.e., appropriately relying on the AI's advice. While previous work has focused on building a mental model of the AI to assess AI recommendations, recent research has shown that the mental model alone cannot explain appropriate reliance. We hypothesize that, in addition to the mental model, human learning is a key mediator of appropriate reliance and, thus, CTP. In this study, we demonstrate the relationship between learning and appropriate reliance in an experiment with 100 participants. This work provides fundamental concepts for analyzing reliance and derives implications for the effective design of human-AI decision-making.",Towards Effective Human-AI Decision-Making: The Role of Human Learning in Appropriate Reliance on AI Advice,https://www.semanticscholar.org/paper/655da0ed37f335d5045a750b56ec9c2ad025b432,"Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.",Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making,https://www.semanticscholar.org/paper/5cc4100a67fd6f2ce3c760655ba7a12f358c7950,"Recent organizations have started to adopt AI-based decision support tools to optimize human resource development practices, while facing various challenges of using AIs in highly contextual and sensitive domains. We present our case study that aims to help professional assessors make decisions in human assessment, in which they conduct interviews with assessees and evaluate their suitability for certain job roles. Our workshop with two industrial assessors elucidated troubles they face (i.e., maintaining stable and non-subjective observation of assessees’ behaviors) and derived requirements of AI systems (i.e., extracting their nonverbal cues from interview videos in an interpretable manner). In response, we employed an unsupervised anomaly detection algorithm using multimodal behavioral features such as facial keypoints, body and head pose, and gaze. The algorithm extracts outlier scenes from the video based on behavioral features as well as informing which feature contributes to the outlierness. We first evaluated how the assessors would perceive the extracted cues and discovered that the algorithm is useful in suggesting scenes to which assessors would pay attention, thanks to its interpretability. Then, we developed an interface prototype incorporating the algorithm and had six assessors use it for their actual assessment. Their comments revealed the effectiveness of introducing unsupervised anomaly detection to enhance their feeling of confidence and objectivity of the assessment along with potential use scenarios of such AI-based systems in human assessment. Our approach, which builds on top of the idea of separating observation and interpretation in human-AI collaboration, will facilitate human decision making in highly contextual domains, such as human assessment, while keeping their trust in the system.",AI for human assessment: What do professional assessors need?,https://www.semanticscholar.org/paper/55f0a0b130bce6e82ed5f2b9e9742183345d01c5,"We have developed a novel ’Teacher-Student with human feedback’ model for Human-Artificial Intelligence (AI) collaborations in cybersecurity tasks. In our model, AI furnishes sufficient information about its decision-making process to enable human agents to provide feedback to improve the model. Our key innovations include: enhancing the interpretability of AI models by analyzing falsely detected samples using LIME and SHAP values; developing a novel posthoc explanation-based dynamic teacher-student model to address concept drift or concept shift; integrating human experts’ feedback on falsely detected samples to increase accuracy, precision, and recall values, without retraining the entire model; establishing a list of attack-based feature values for human experts to promote reproducibility. We show in experiments with real data and threat detection tasks that our model significantly improves the accuracy of existing AI algorithms for these tasks.",POSTER: A Teacher-Student with Human Feedback Model for Human-AI Collaboration in Cybersecurity,https://www.semanticscholar.org/paper/9c6f2d5bb25aac7ec9b0704dbb492b2ae5730bff,not novel
