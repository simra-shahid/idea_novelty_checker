"Examples Reviews that assess novelty of an idea using related papers:\n                - Not Novel: \n                    Idea: Create a **benchmarking approach** designed **to evaluate analogical reasoning in language models** by utilizing a **novel analogical reasoning corpus aggregated through environmental dna metabarcoding** techniques applied to text data. This corpus would be constructed by identifying and extracting analogical expressions from a vast array of textual sources, similar to how eDNA captures biodiversity. The agent will be evaluated through **performance comparison** with existing datasets to measure its ability to identify and reason with analogies.\n    Class: not novel\n    Review: The idea is not novel because it closely replicates existing work. For instance, the concept of creating a corpus by aggregating analogical expressions from textual sources is similar to the approach in ANALOGYKB[0]. Additionally, the benchmarking method for evaluating analogical reasoning is akin to the one used in AnaloBench[1].\n    Related Papers:\n    [0] Title: ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base. Abstract: Analogical reasoning is a fundamental cognitive ability of humans. However, current language models (LMs) still struggle to achieve human-like performance in analogical reasoning tasks due to a lack of resources for model training. In this work, we address this gap by proposing ANALOGYKB, a million-scale analogy knowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB identifies two types of analogies from the KGs: 1) analogies of the same relations, which can be directly extracted from the KGs, and 2) analogies of analogous relations, which are identified with a selection and filtering pipeline enabled by large language models (LLMs), followed by minor human efforts for data quality control. Evaluations on a series of datasets of two analogical reasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB successfully enables both smaller LMs and LLMs to gain better analogical reasoning capabilities.\n    [1] Title: AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies. Abstract: Humans regularly engage in analogical thinking, relating personal experiences to current situations ($X$ is analogous to $Y$ because of $Z$). Analogical thinking allows humans to solve problems in creative ways, grasp difficult concepts, and articulate ideas more effectively. Can language models (LMs) do the same? To answer this question, we propose ANALOBENCH, a benchmark to determine analogical reasoning ability in LMs. Our benchmarking approach focuses on aspects of this ability that are common among humans: (i) recalling related experiences from a large amount of information, and (ii) applying analogical reasoning to complex and lengthy scenarios. We test a broad collection of proprietary models (e.g., GPT family, Claude V2) and open source models such as LLaMA2. As in prior results, scaling up LMs results in some performance boosts. Surprisingly, scale offers minimal gains when, (i) analogies involve lengthy scenarios, or (ii) recalling relevant scenarios from a large pool of information, a process analogous to finding a needle in a haystack. We hope these observations encourage further research in this field.\n    [2] Title: Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction. Abstract: The vital role of analogical reasoning in human cognition allows us to grasp novel concepts by linking them with familiar ones through shared relational structures. Despite the attention previous research has given to word analogies, this work suggests that Large Language Models (LLMs) often overlook the structures that underpin these analogies, raising questions about the efficacy of word analogies as a measure of analogical reasoning skills akin to human cognition. In response to this, our paper introduces a task of analogical structure abduction, grounded in cognitive psychology, designed to abduce structures that form an analogy between two systems. In support of this task, we establish a benchmark called SCAR, containing 400 scientific analogies from 13 distinct fields, tailored for evaluating analogical reasoning with structure abduction. The empirical evidence underlines the continued challenges faced by LLMs, including ChatGPT and GPT-4, in mastering this task, signifying the need for future exploration to enhance their abilities.\n    [3] Title: Relevant or Random: Can LLMs Truly Perform Analogical Reasoning?. Abstract: Analogical reasoning is a unique ability of humans to address unfamiliar challenges by transferring strategies from relevant past experiences. One key finding in psychology is that compared with irrelevant past experiences, recalling relevant ones can help humans better handle new tasks. Coincidentally, the NLP community has also recently found that self-generating relevant examples in the context can help large language models (LLMs) better solve a given problem than hand-crafted prompts. However, it is yet not clear whether relevance is the key factor eliciting such capability, i.e., can LLMs benefit more from self-generated relevant examples than irrelevant ones? In this work, we systematically explore whether LLMs can truly perform analogical reasoning on a diverse set of reasoning tasks. With extensive experiments and analysis, we show that self-generated random examples can surprisingly achieve comparable or even better performance, e.g., 4% performance boost on GSM8K with random biological examples. We find that the accuracy of self-generated examples is the key factor and subsequently design two improved methods with significantly reduced inference costs. Overall, we aim to advance a deeper understanding of LLM analogical reasoning and hope this work stimulates further research in the design of self-generated contexts.\n    [4] Title: Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models. Abstract: Large language models (LLMs) have performed well on several reasoning benchmarks, including ones that test analogical reasoning abilities. However, it has been debated whether they are actually performing humanlike abstract reasoning or instead employing less general processes that rely on similarity to what has been seen in their training data. Here we investigate the generality of analogy-making abilities previously claimed for LLMs (Webb, Holyoak,&Lu, 2023). We take one set of analogy problems used to evaluate LLMs and create a set of\"counterfactual\"variants-versions that test the same abstract reasoning abilities but that are likely dissimilar from any pre-training data. We test humans and three GPT models on both the original and counterfactual problems, and show that, while the performance of humans remains high for all the problems, the GPT models' performance declines sharply on the counterfactual set. This work provides evidence that, despite previously reported successes of LLMs on analogical reasoning, these models lack the robustness and generality of human analogy-making.\n    [5] Title: Life is a Circus and We are the Clowns: Automatically Finding Analogies between Situations and Processes. Abstract: Analogy-making gives rise to reasoning, abstraction, flexible categorization and counterfactual inference \u2013 abilities lacking in even the best AI systems today. Much research has suggested that analogies are key to non-brittle systems that can adapt to new domains. Despite their importance, analogies received little attention in the NLP community, with most research focusing on simple word analogies. Work that tackled more complex analogies relied heavily on manually constructed, hard-to-scale input representations.In this work, we explore a more realistic, challenging setup: our input is a pair of natural language procedural texts, describing a situation or a process (e.g., how the heart works/how a pump works). Our goal is to automatically extract entities and their relations from the text and find a mapping between the different domains based on relational similarity (e.g., blood is mapped to water). We develop an interpretable, scalable algorithm and demonstrate that it identifies the correct mappings 87% of the time for procedural texts and 94% for stories from cognitive-psychology literature. We show it can extract analogies from a large dataset of procedural texts, achieving 79% precision (analogy prevalence in data: 3%). Lastly, we demonstrate that our algorithm is robust to paraphrasing the input texts\n    [6] Title: Why Do We Need Neurosymbolic AI to Model Pragmatic Analogies?. Abstract: A hallmark of intelligence is the ability to use a familiar domain to make inferences about a less familiar domain, known as analogical reasoning. In this article, we delve into the performance of large language models (LLMs) in dealing with progressively complex analogies expressed in unstructured text. We discuss analogies at four distinct levels of complexity: lexical, syntactic, semantic, and pragmatic. As the analogies become more complex, they require increasingly extensive, diverse knowledge beyond the textual content, unlikely to be found in the lexical co-occurrence statistics that power LLMs. We discuss neurosymbolic AI techniques that combine statistical and symbolic AI, informing the representation of unstructured text to highlight and augment relevant content, provide abstraction, and guide the mapping process. This maintains the efficiency of LLMs while preserving the ability to explain analogies for pedagogical applications.\n    [7] Title: ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies. Abstract: Analogy-making is central to human cognition, allowing us to adapt to novel situations \u2013 an ability that current AI systems still lack. Most analogy datasets today focus on simple analogies (e.g., word analogies); datasets including complex types of analogies are typically manually curated and very small. We believe that this holds back progress in computational analogy.In this work, we design a data generation pipeline, ParallelPARC (Parallel Paragraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to create complex, paragraph-based analogies, as well as distractors, both simple and challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset of analogies between scientific processes. We publish a gold-set, validated by humans, and a silver-set, generated automatically. We test LLMs\u2019 and humans\u2019 analogy recognition in binary and multiple-choice settings, and found that humans outperform the best models (\u223c13% gap) after a light supervision. We demonstrate that our silver-set is useful for training models. Lastly, we show challenging distractors confuse LLMs, but not humans. We hope our pipeline will encourage research in this emerging field.\n    [8] Title: ARN: Analogical Reasoning on Narratives. Abstract: As a core cognitive skill that enables the transferability of information across domains, analogical reasoning has been extensively studied for both humans and computational models. However, while cognitive theories of analogy often focus on narratives and study the distinction between surface, relational, and system similarities, existing work in natural language processing has a narrower focus as far as relational analogies between word pairs. This gap brings a natural question: can state-of-the-art large language models (LLMs) detect system analogies between narratives? To gain insight into this question and extend word-based relational analogies to relational system analogies, we devise a comprehensive computational framework that operationalizes dominant theories of analogy, using narrative elements to create surface and system mappings. Leveraging the interplay between these mappings, we create a binary task and benchmark for Analogical Reasoning on Narratives (ARN), covering four categories of far (cross-domain)/near (within-domain) analogies and disanalogies. We show that while all LLMs can largely recognize near analogies, even the largest ones struggle with far analogies in a zero-shot setting, with GPT4.0 scoring below random. Guiding the models through solved examples and chain-of-thought reasoning enhances their analogical reasoning ability. Yet, since even in the few-shot setting, the best model only performs halfway between random and humans, ARN opens exciting directions for computational analogical reasoners.\n    [9] Title: Large Language Models as Analogical Reasoners. Abstract: Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, analogical prompting, designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench.\n\n\n    Idea: Develop a **systematic review-based framework** designed **to align llm evaluation with human preferences**, ensuring that evaluation criteria are continuously refined based on comprehensive reviews of user feedback and emerging model behaviors. This framework will utilize **content analysis of user interactions and feedback** to identify patterns and areas of improvement. The effectiveness of this framework will be assessed through a **qualitative study** involving iterative cycles of user feedback and criteria refinement.\n    Class: not novel\n    Review: The idea is not novel because it closely resembles existing frameworks like EvalLM[0] and HumanELY[1], which already align LLM evaluations with human preferences using user-defined criteria and human feedback.\n    Related Papers:\n    [0] Title: EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria. Abstract: By simply composing prompts, developers can prototype novel generative applications with Large Language Models (LLMs). To refine prototypes into products, however, developers must iteratively revise prompts by evaluating outputs to diagnose weaknesses. Formative interviews (N=8) revealed that developers invest significant effort in manually evaluating outputs as they assess context-specific and subjective criteria. We present EvalLM, an interactive system for iteratively refining prompts by evaluating multiple outputs on user-defined criteria. By describing criteria in natural language, users can employ the system\u2019s LLM-based evaluator to get an overview of where prompts excel or fail, and improve these based on the evaluator\u2019s feedback. A comparative study (N=12) showed that EvalLM, when compared to manual evaluation, helped participants compose more diverse criteria, examine twice as many outputs, and reach satisfactory prompts with 59% fewer revisions. Beyond prompts, our work can be extended to augment model evaluation and alignment in specific application contexts.\n    [1] Title: HUMANELY: HUMAN EVALUATION OF LLM YIELD, USING A NOVEL WEB BASED EVALUATION TOOL. Abstract: Large language models (LLMs) have caught the imagination of researchers,developers and public in general the world over with their potential for transformation. Vast amounts of research and development resources are being provided to implement these models in all facets of life. Trained using billions of parameters, various measures of their accuracy and performance have been proposed and used in recent times. While many of the automated natural language assessment parameters measure LLM output performance for use of language, contextual outputs are still hard to measure and quantify. Hence, human evaluation is still an important measure of LLM performance,even though it has been applied variably and inconsistently due to lack of guidance and resource limitations. To provide a structured way to perform comprehensive human evaluation of LLM output, we propose the first guidance and tool called HumanELY. Our approach and tool built using prior knowledge helps perform evaluation of LLM outputs in a comprehensive, consistent, measurable and comparable manner. HumanELY comprises of five key evaluation metrics: relevance, coverage, coherence, harm and comparison. Additional submetrics within these five key metrics provide for Likert scale based human evaluation of LLM outputs. Our related webtool uses this HumanELY guidance to enable LLM evaluation and provide data for comparison against different users performing human evaluation. While all metrics may not be relevant and pertinent to all outputs, it is important to assess and address their use. Lastly, we demonstrate comparison of metrics used in HumanELY against some of the recent publications in the healthcare domain. We focused on the healthcare domain due to the need to demonstrate highest levels of accuracy and lowest levels of harm in a comprehensive manner. We anticipate our guidance and tool to be used for any domain where LLMs find an use case.\n    [2] Title: Evaluation of Code Generation for Simulating Participant Behavior in Experience Sampling Method by Iterative In-Context Learning of a Large Language Model. Abstract: The Experience Sampling Method (ESM) is commonly used to understand behaviors, thoughts, and feelings in the wild by collecting self-reports. Sustaining sufficient response rates, especially in long-running studies remains challenging. To avoid low response rates and dropouts, experimenters rely on their experience, proposed methodologies from earlier studies, trial and error, or the scarcely available participant behavior data from previous ESM protocols. This approach often fails in finding the acceptable study parameters, resulting in redesigning the protocol and repeating the experiment. Research has shown the potential of machine learning to personalize ESM protocols such that ESM prompts are delivered at opportune moments, leading to higher response rates. The corresponding training process is hindered due to the scarcity of open data in the ESM domain, causing a cold start, which could be mitigated by simulating participant behavior. Such simulations provide training data and insights for the experimenters to update their study design choices. Creating this simulation requires behavioral science, psychology, and programming expertise. Large language models (LLMs) have emerged as facilitators for information inquiry and programming, albeit random and occasionally unreliable. We aspire to assess the readiness of LLMs in an ESM use case. We conducted research using GPT-3.5 turbo-16k to tackle an ESM simulation problem. We explored several prompt design alternatives to generate ESM simulation programs, evaluated the output code in terms of semantics and syntax, and interviewed ESM practitioners. We found that engineering LLM-enabled ESM simulations have the potential to facilitate data generation, but they perpetuate trust and reliability challenges.\n    [3] Title: Human-Centered Evaluation and Auditing of Language Models. Abstract: The recent advancements in Large Language Models (LLMs) have significantly impacted numerous, and will impact more, real-world applications. However, these models also pose significant risks to individuals and society. To mitigate these issues and guide future model development, responsible evaluation and auditing of LLMs are essential. This workshop aims to address the current \u201cevaluation crisis\u201d in LLM research and practice by bringing together HCI and AI researchers and practitioners to rethink LLM evaluation and auditing from a human-centered perspective. The workshop will explore topics around understanding stakeholders\u2019 needs and goals with evaluation and auditing LLMs, establishing human-centered evaluation and auditing methods, developing tools and resources to support these methods, building community and fostering collaboration. By soliciting papers, organizing invited keynote and panel, and facilitating group discussions, this workshop aims to develop a future research agenda for addressing the challenges in LLM evaluation and auditing.\n    [4] Title: Aligning Model Evaluations with Human Preferences: Mitigating Token Count Bias in Language Model Assessments. Abstract: The SLAM paper demonstrated that on-device Small Language Models (SLMs) are a viable and cost-effective alternative to API-based Large Language Models (LLMs), such as OpenAI's GPT-4, offering comparable performance and stability. However, SLAM also identified discrepancies between human preferences and traditional auto-evaluators. This follow-up paper explores methods to align LLM evaluator preferences with human evaluations by addressing biases, particularly toward higher token counts. We employed Bayesian statistics and a t-test to quantify this bias and developed a recalibration procedure to adjust the GPTScorer. Our findings significantly improve aligning the recalibrated LLM evaluator with human evaluations across multiple use cases. For instance, spearman's ranking correlation score in the Recommendation use case improved from -27.27 to 44.55. These results highlight the importance of accounting for biases in automated evaluations to ensure fair and accurate model assessments. The recalibration process enhances the reliability of automated evaluators, leading to better AI models that align with human values and expectations. This study provides a robust methodology for future research into bias correction and emphasizes the feasibility and benefits of developing human-aligned AI evaluation systems.\n    [5] Title: Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences. Abstract: Due to the cumbersome nature of human evaluation and limitations of code-based evaluation, Large Language Models (LLMs) are increasingly being used to assist humans in evaluating LLM outputs. Yet LLM-generated evaluators simply inherit all the problems of the LLMs they evaluate, requiring further human validation. We present a mixed-initiative approach to ``validate the validators'' -- aligning LLM-generated evaluation functions (be it prompts or code) with human requirements. Our interface, EvalGen, provides automated assistance to users in generating evaluation criteria and implementing assertions. While generating candidate implementations (Python functions, LLM grader prompts), EvalGen asks humans to grade a subset of LLM outputs; this feedback is used to select implementations that better align with user grades. A qualitative study finds overall support for EvalGen but underscores the subjectivity and iterative process of alignment. In particular, we identify a phenomenon we dub \\emph{criteria drift}: users need criteria to grade outputs, but grading outputs helps users define criteria. What is more, some criteria appears \\emph{dependent} on the specific LLM outputs observed (rather than independent criteria that can be defined \\emph{a priori}), raising serious questions for approaches that assume the independence of evaluation from observation of model outputs. We present our interface and implementation details, a comparison of our algorithm with a baseline approach, and implications for the design of future LLM evaluation assistants.\n    [6] Title: Human-Centered Design Recommendations for LLM-as-a-judge. Abstract: Traditional reference-based metrics, such as BLEU and ROUGE, are less effective for assessing outputs from Large Language Models (LLMs) that produce highly creative or superior-quality text, or in situations where reference outputs are unavailable. While human evaluation remains an option, it is costly and difficult to scale. Recent work using LLMs as evaluators (LLM-as-a-judge) is promising, but trust and reliability remain a significant concern. Integrating human input is crucial to ensure criteria used to evaluate are aligned with the human\u2019s intent, and evaluations are robust and consistent. This paper presents a user study of a design exploration called EvaluLLM, that enables users to leverage LLMs as customizable judges, promoting human involvement to balance trust and cost-saving potential with caution. Through interviews with eight domain experts, we identified the need for assistance in developing effective evaluation criteria aligning the LLM-as-a-judge with practitioners\u2019 preferences and expectations. We offer findings and design recommendations to optimize human-assisted LLM-as-judge systems.\n    [7] Title: CheckEval: Robust Evaluation Framework using Large Language Model via Checklist. Abstract: We introduce CheckEval, a novel evaluation framework using Large Language Models, addressing the challenges of ambiguity and inconsistency in current evaluation methods. CheckEval addresses these challenges by dividing evaluation criteria into detailed sub-aspects and constructing a checklist of Boolean questions for each, simplifying the evaluation. This approach not only renders the process more interpretable but also significantly enhances the robustness and reliability of results by focusing on specific evaluation dimensions. Validated through a focused case study using the SummEval benchmark, CheckEval indicates a strong correlation with human judgments. Furthermore, it demonstrates a highly consistent Inter-Annotator Agreement. These findings highlight the effectiveness of CheckEval for objective, flexible, and precise evaluations. By offering a customizable and interactive framework, CheckEval sets a new standard for the use of LLMs in evaluation, responding to the evolving needs of the field and establishing a clear method for future LLM-based evaluation.\n    [8] Title: Discovering Language Model Behaviors with Model-Written Evaluations. Abstract: As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer (\"sycophancy\") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors.\n    [9] Title: Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models. Abstract: Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment. Additionally, they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. To address these issues, we introduce Prometheus 2, a more powerful evaluator LM than its predecessor that closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, Prometheus 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Our models, code, and data are all publicly available at https://github.com/prometheus-eval/prometheus-eval.\n\n                - Novel:\n                    Idea: Create a gamified health promotion platform that combines digital and tabletop game elements to support caregiver psychological adjustment. The platform will include stress-relief games, emotional support activities, and community-building features tailored to caregivers' needs.\n    Class: novel\n    Review: The idea is novel because it uniquely combines digital and tabletop game elements to support caregiver psychological adjustment, a focus not seen in related works\n    Related Papers:\n    [0] Title: Tabletop Board Game Elements and Gamification Interventions for Health Behavior Change: Realist Review and Proposal of a Game Design Framework. Abstract: Background Games, when used as interventional tools, can influence behavior change by incentivizing, reinforcing, educating, providing feedback loops, prompting, persuading, or providing meaning, fun, and community. However, not all game elements will appeal to all consumers equally, and different elements might work for different people and in different contexts. Objective The aim of this study was to conduct a realist review of tabletop games targeting behavior change and to propose a framework for designing effective behavior change games. Methods A realist review was conducted to inform program theory in the development of tabletop games for health behavior change. The context, mechanisms used to change behavior, and outcomes of included studies were reviewed through a realist lens. Results Thirty-one papers met the eligibility criteria and were included in the review. Several design methods were identified that enhanced the efficacy of the games to change behavior. These included design by local teams, pilot testing, clearly defined targets of behavior change, conscious attention to all aspects of game design, including game mechanics, dynamics, aesthetics, and the elicitation of emotions. Delivery with other mediums, leveraging behavioral insights, prior training for delivery, and repeated play were also important. Some design elements that were found to reduce efficacy included limited replayability or lack of fun for immersive engagement. Conclusions Game designers need to consider all aspects of the context and the mechanisms to achieve the desired behavior change outcomes. Careful design thinking should include consideration of the game mechanics, dynamics, aesthetics, emotions, and contexts of the game and the players. People who know the players and the contexts well should design the games or have significant input. Testing in real-world settings is likely to lead to better outcomes. Careful selection and purposeful design of the behavior change mechanisms at play is essential. Fun and enjoyment of the player should be considered, as without engagement, there will be no desired intervention effect.\n    [1] Title: Tabletop Board Game Elements and Gamification Interventions for Health Behavior Change: Realist Review and Proposal of a Game Design Framework (Preprint). Abstract: \n BACKGROUND\n Games, when used as interventional tools, can influence behavior change by incentivizing, reinforcing, educating, providing feedback loops, prompting, persuading, or providing meaning, fun, and community. However, not all game elements will appeal to all consumers equally, and different elements might work for different people and in different contexts.\n \n \n OBJECTIVE\n The aim of this study was to conduct a realist review of tabletop games targeting behavior change and to propose a framework for designing effective behavior change games.\n \n \n METHODS\n A realist review was conducted to inform program theory in the development of tabletop games for health behavior change. The context, mechanisms used to change behavior, and outcomes of included studies were reviewed through a realist lens.\n \n \n RESULTS\n Thirty-one papers met the eligibility criteria and were included in the review. Several design methods were identified that enhanced the efficacy of the games to change behavior. These included design by local teams, pilot testing, clearly defined targets of behavior change, conscious attention to all aspects of game design, including game mechanics, dynamics, aesthetics, and the elicitation of emotions. Delivery with other mediums, leveraging behavioral insights, prior training for delivery, and repeated play were also important. Some design elements that were found to reduce efficacy included limited replayability or lack of fun for immersive engagement.\n \n \n CONCLUSIONS\n Game designers need to consider all aspects of the context and the mechanisms to achieve the desired behavior change outcomes. Careful design thinking should include consideration of the game mechanics, dynamics, aesthetics, emotions, and contexts of the game and the players. People who know the players and the contexts well should design the games or have significant input. Testing in real-world settings is likely to lead to better outcomes. Careful selection and purposeful design of the behavior change mechanisms at play is essential. Fun and enjoyment of the player should be considered, as without engagement, there will be no desired intervention effect.\n\n    [2] Title: A game plan: Gamification design principles in mHealth applications for chronic disease management. Abstract: Effective chronic disease management is essential to improve positive health outcomes, and incentive strategies are useful in promoting self-care with longevity. Gamification, applied with mHealth (mobile health) applications, has the potential to better facilitate patient self-management. This review article addresses a knowledge gap around the effective use of gamification design principles, or mechanics, in developing mHealth applications. Badges, leaderboards, points and levels, challenges and quests, social engagement loops, and onboarding are mechanics that comprise gamification. These mechanics are defined and explained from a design and development perspective. Health and fitness applications with gamification mechanics include: bant which uses points, levels, and social engagement, mySugr which uses challenges and quests, RunKeeper which uses leaderboards as well as social engagement loops and onboarding, Fitocracy which uses badges, and Mango Health, which uses points and levels. Specific design considerations are explored, an example of the efficacy of a gamified mHealth implementation in facilitating improved self-management is provided, limitations to this work are discussed, a link between the principles of gaming and gamification in health and wellness technologies is provided, and suggestions for future work are made. We conclude that gamification could be leveraged in developing applications with the potential to better facilitate self-management in persons with chronic conditions.\n    [3] Title: Playing alone: can game design elements satisfy user needs in gamified mHealth services?. Abstract: Chronic health conditions have necessitated the need for behavioral interventions (such as exercise programs) outside of clinical contexts, increasingly managed through technology such as mobile health (mHealth) services. Gamification has emerged as a promising tool to facilitate greater engagement in these services; however, no studies investigate the links between specific game design elements (GDEs) and psychological or behavioral outcomes within a health context. This domain is motivationally complex and has shown resistance to the satisfaction of social (relatedness) needs, presenting a challenge to the design of gamification products for health promotion. Drawing on self-determination theory, this research demonstrates the strengths of a taxonomy based upon structural features of GDEs (such as social, narrative or reward elements) rather than the design intent definitions of these elements used in previous studies. This taxonomy is then used to assess the relationship between GDEs and psychological needs satisfaction in a survey (N\u2009=\u2009236) of gamified exercise/fitness application users. Further qualitative interviews (N\u2009=\u200920) were conducted to clarify survey findings. This research demonstrates the positive association between control and presentation elements and autonomy satisfaction, and control and reward elements and competency satisfaction. However, it also suggests that player type and context may limit the ability for GDEs alone to support relatedness satisfaction in mHealth services. Implications for managers and researchers are discussed, particularly the strengths and weaknesses of using structural taxonomies in gamification assessment.\n    [4] Title: Design Features in Games for Health: Disciplinary and Interdisciplinary Expert Perspectives. Abstract: Games for health (G4H) aim to improve health outcomes and encourage behavior change. While existing theoretical frameworks describe features of both games and health interventions, there has been limited systematic investigation into how disciplinary and interdisciplinary stakeholders understand design features in G4H. We recruited 18 experts from the fields of game design, behavioral health, and games for health, and prompted them with 16 sample games. Applying methods including open card sorting and triading, we elicited themes and features (e.g., real-world interaction, game mechanics) around G4H. We found evidence of conceptual differences suggesting that a G4H perspective is not simply the sum of game and health perspectives. At the same time, we found evidence of convergence in stakeholder views, including areas where game experts provided insights about health and vice versa. We discuss how this work can be applied to provide conceptual tools, improve the G4H design process, and guide approaches to encoding G4H-related data for large-scale empirical analysis.\n    [5] Title: Guidelines for Designing Effective Games as Clinical Interventions: Mechanics, Dynamics, Aesthetics, and Outcomes (MDAO) Framework. Abstract: Games are a successful pedagogical tool to change attitudes and behaviors. This chapter will examine how games facilitate change, discuss common pitfalls, and outline best practices for making serious games for clinical practice. Sustained engagement and motivation are key to lasting clinical interventions. When developing a game for clinical practice, the designer should avoid \u201cpunishing by rewards\u201d (Kohn, 1993), damaging motivation towards the desired goal. Understanding game design principles is crucial to creating intrinsically engaging experiences that lead to lasting motivation. The Mechanics, Dynamics, and Aesthetics (MDA) framework is widely accepted by game designers as a framework to make compelling games. Using MDA as a base for understanding how to create engaging experiences, this chapter proposes a new framework for serious games called Mechanics, Dynamics, Aesthetics, and Outcomes. MDAO describes how to design a game that is intrinsically motivating and effective by focusing on the interplay between outcomes and other vectors of design.\n    [6] Title: Quittr: The Design of a Video Game to Support Smoking Cessation. Abstract: Background Smoking is recognized as the largest, single, preventable cause of death and disease in the developed world. While the majority of smokers report wanting to quit, and many try each year, smokers find it difficult to maintain long-term abstinence. Behavioral support, such as education, advice, goal-setting, and encouragement, is known to be beneficial in improving the likelihood of succeeding in a quit attempt, but it remains difficult to effectively deliver this behavioral support and keep the patient engaged with the process for a sufficient duration. In an attempt to solve this, there have been numerous mobile apps developed, yet engagement and retention have remained key challenges that limit the potential effectiveness of these interventions. Video games have been clearly linked with the effective delivery of health interventions, due to their capacity to increase motivation and engagement of players. Objective The objective of this study is to describe the design and development of a smartphone app that is theory-driven, and which incorporates gaming characteristics in order to promote engagement with content, and thereby help smokers to quit. Methods Game design and development was informed by a taxonomy of motivational affordances for meaningful gamified and persuasive technologies. This taxonomy describes a set of design components that is grounded in well-established psychological theories on motivation. Results This paper reports on the design and development process of Quittr, a mobile app, describing how game design principles, game mechanics, and game elements can be used to embed education and support content, such that the app actually requires the user to access and engage with relevant educational content. The next stage of this research is to conduct a randomized controlled trial to determine whether the additional incentivization game features offer any value in terms of the key metrics of engagement\u2013how much content users are consuming, how many days users are persisting with using the app, and what proportion of users successfully abstain from smoking for 28 days, based on user-reported data and verified against a biochemical baseline using cotinine tests. Conclusions We describe a novel, and theoretically-informed mobile app design approach that has a broad range of potential applications. By using the virtual currency approach, we remove the need for the game to comprehensively integrate the healthy activity as part of its actual play mechanics. This opens up the potential for a wide variety of health problems to be tackled through games where no obvious play mechanic presents itself. The implications of this app are that similar approaches may be of benefit in areas such as managing chronic conditions (diabetes, heart disease, etc), treating substance abuse (alcohol, illicit drugs, etc), diet and exercise, eating disorders (anorexia, bulimia, and binge eating), and various phobias.\n    [7] Title: Developing Theory-Driven, Evidence-Based Serious Games for Health: Framework Based on Research Community Insights. Abstract: Background The idea of using serious games to effectuate better outcomes in health care has gained significant traction among a growing community of researchers, developers, and health care professionals. Many now recognize the importance of creating evidence-based games that are purposefully designed to address physical and mental health challenges faced by end users. To date, no regulatory resources have been established to guide the development of serious games for health (SGH). Developers must therefore look elsewhere for guidance. Although a more robust level of evidence exists in the research literature, it is neither structured nor is there any clear consensus. Developers currently use a variety of approaches and methodologies. The establishment of a well-defined framework that represents the consensus views of the SGH research community would help developers improve the efficiency of internal development processes, as well as chances of success. A consensus framework would also enhance the credibility of SGH and help provide quality evidence of their effectiveness. Objective This research aimed to (1) identify and evaluate the requirements, recommendations, and guidelines proposed by the SGH community in the research literature, and; (2) develop a consensus framework to guide developers, designers, researchers, and health care professionals in the development of evidence-based SGH. Methods A critical review of the literature was performed in October to November 2018. A 3-step search strategy and a predefined set of inclusion criteria were used to identify relevant articles in PubMed, ScienceDirect, Institute of Electrical and Electronics Engineers Xplore, CiteSeerX, and Google Scholar. A supplemental search of publications from regulatory authorities was conducted to capture their specific requirements. Three researchers independently evaluated the identified articles. The evidence was coded and categorized for analysis. Results This review identified 5 categories of high-level requirements and 20 low-level requirements suggested by the SGH community. These advocate a methodological approach that is multidisciplinary, iterative, and participatory. On the basis of the requirements identified, we propose a framework for developing theory-driven, evidence-based SGH. It comprises 5 stages that are informed by various stakeholders. It focuses on building strong scientific and design foundations that guide the creative and technical development. It includes quantitative trials to evaluate whether the SGH achieve the intended outcomes, as well as efforts to disseminate trial findings and follow-up monitoring after the SGH are rolled out for use. Conclusions This review resulted in the formulation of a framework for developing theory-driven, evidence-based SGH that represents many of the requirements set out by SGH stakeholders in the literature. It covers all aspects of the development process (scientific, technological, and design) and is transparently described in sufficient detail to allow SGH stakeholders to implement it in a wide variety of projects, irrespective of discipline, health care segments, or focus.\n    [8] Title: The Construction and Evaluation of a Design Framework for Narrative Games for Health. Abstract: A larger number of games for health were developed over the past two decades to provide an engaging way of health care and behavior change intervention. However, many problems with the design of these games, as well as with the methodologies used to evaluate them emerged: the games were generally designed without the consultation or direct involvement of a professional game designer, and created without the guidance of a proper game design framework; the health messages delivered in the games were mostly simple and knowledge-oriented and not crafted based on theories from behavioral medicine; and the evaluation studies were also poorly designed. To solve these problems, I define the DraGuNa (Drama-Guided Narrative Health Game) framework, a methodology that uses drama theory and sound principles from behavioral medicine to guide games for health design to solve the current problems in games for health. The dissertation introduces a methodology of game design, specifically developed for games for health, which addresses two key constructs: engagement \u2013 ensuring users stick with the game for the duration of the intervention; and adherence \u2013 ensuring users perform those actions in the game hypothesized by behavioral medicine theories to lead to health behavior change. The dissertation also provides a methodology to develop interactive narrative-based games based on existing story media, which also suggests a new path of research for the intelligent narrative community. Finally, the dissertation describes an experimental framework for testing the effects of a game on the two fundamental dimensions of player involvement in the intervention \u2013 engagement and adherence \u2013 and tests the relative contributions of each on health outcomes.\n    [9] Title: The Model of Gamification Principles for Digital Health Interventions: Evaluation of Validity and Potential Utility. Abstract: Background Although gamification continues to be a popular approach to increase engagement, motivation, and adherence to behavioral interventions, empirical studies have rarely focused on this topic. There is a need to empirically evaluate gamification models to increase the understanding of how to integrate gamification into interventions. Objective The model of gamification principles for digital health interventions proposes a set of five independent yet interrelated gamification principles. This study aimed to examine the validity and reliability of this model to inform its use in Web- and mobile-based apps. Methods A total of 17 digital health interventions were selected from a curated website of mobile- and Web-based apps (PsyberGuide), which makes independent and unbiased ratings on various metrics. A total of 133 independent raters trained in gamification evaluation techniques were instructed to evaluate the apps and rate the degree to which gamification principles are present. Multiple ratings (n\u226520) were collected for each of the five gamification principles within each app. Existing measures, including the PsyberGuide credibility score, mobile app rating scale (MARS), and the app store rating of each app were collected, and their relationship with the gamification principle scores was investigated. Results Apps varied widely in the degree of gamification implemented (ie, the mean gamification rating ranged from 0.17\u2264m\u22644.65 out of 5). Inter-rater reliability of gamification scores for each app was acceptable (\u03ba\u22650.5). There was no significant correlation between any of the five gamification principles and the PsyberGuide credibility score (P\u2265.49 in all cases). Three gamification principles (supporting player archetypes, feedback, and visibility) were significantly correlated with the MARS score, whereas three principles (meaningful purpose, meaningful choice, and supporting player archetypes) were significantly correlated with the app store rating. One gamification principle was statistically significant with both the MARS and the app store rating (supporting player archetypes). Conclusions Overall, the results support the validity and potential utility of the model of gamification principles for digital health interventions. As expected, there was some overlap between several gamification principles and existing app measures (eg, MARS). However, the results indicate that the gamification principles are not redundant with existing measures and highlight the potential utility of a 5-factor gamification model structure in digital behavioral health interventions. These gamification principles may be used to improve user experience and enhance engagement with digital health programs.\n\n\n    Idea: Develop a research initiative to investigate social bias in language models by applying discourse analysis techniques. This approach will analyze narrative outputs from language models to uncover the implicit and explicit power dynamics and biases they reproduce. By focusing on presuppositions and entailments within generated texts, researchers can identify how language models reinforce hegemonic norms and marginalize minority groups. This initiative will be evaluated through a series of case studies, using qualitative analyses of generated content in multiple languages and socio-cultural contexts to measure the extent and nature of biases.\n    Class: novel\n    Review: The idea is novel because it introduces discourse analysis techniques to investigate social bias in language models, differing from the NLP mechanisms used in EFL discourse analysis[0] and Social Bias Frames[2].\n    Related Papers:\n    [0] Title: EFL discourse as cultural practice. Abstract: ABSTRACT This paper presents a culturally based discussion of the role English as a foreign language (EFL) plays in shaping the ideological positions of those involved in teaching, learning, and planning English in Israel. The English curriculum is uniform for speakers of Hebrew (the majority) and speakers of Arabic (the minority). Furthermore, the English curriculum states that EFL pedagogy should take a global rather than an Anglo-centric approach. This situation constitutes the paper\u2019s main assumption regarding the cultural content of EFL materials being ideologically oriented toward: (1) reproducing and perpetuating dominant Western hegemonic ideologies; (2) maintaining social misrepresentations and inequalities regarding the Other. By employing critical discourse analysis (CDA) to uncover Westcentric cultural biases in EFL discourse and cultural discourse studies (CDS) to reconstruct the role of Eastern and local Arab discourses in EFL pedagogies, I examine a new culturally critical way of observing EFL discourse and linking it with social change. I suggest a holistic, ecological EFL pedagogy that constructs EFL discourse as cultural practice. Promoting an EFL \u2018ecological discourse\u2019 prioritizes narratives emphasizing positive multicultural values, respect, and acceptance of the Other and encourages learners to consider cultural differences as the source for reading EFL texts about the Other.\n    [1] Title: Gender stereotypes and biases in Iranian EFL textbooks / Amir Biglarbeigi Ghajarieh. Abstract: Gender biases and ideologies are manifested through many avenues, including the representations of gendered social actors in school textbooks (Lee & Collins, 2008; Blumberg, 2008). This study examines the representations of male and female social actors within gendered discourses in Iranian EFL textbooks. These prescribed English textbooks, which enjoy a nation-wide audience, cater for the needs of Iranian EFL students at the secondary level, high school, and pre-university levels and serve as the students\u2018 initial \nintroduction to the English Language and culture. \nThe theoretical background of this study is grounded in Critical Discourse Analysis (CDA). To examine the depictions of male and female social actors within the identified gendered discourses in these textbooks, this study fused van Leeuwen\u2018s (2003) \u2017Social Actor Network Model\u2018 and Sunderland\u2018s (2004) \u2017Gendered Discourses Model\u2018. I studied a focus group of students who use Iranian EFL textbooks at different educational levels to triangulate my data. \nThis study employed Sunderland\u2018s (2006) framework together with Hall\u2018s (1980) idea of text readers\u2018 interpretations; i.e., hegemonic, oppositional and negotiated readings, to analyse the focus group\u2018s readings of sample gendered \ntexts in Iranian EFL textbooks. \nThe conservative gendered discourses provisionally identified in this study were supported to a great extend through the representations of male and female social actors in the texts and images closely related to the texts of Iranian EFL textbooks at different educational levels. Conversely, the subversive gendered discourses were resisted through such representations to a great extent. Grounded on CDA, one can argue that, the support for \nconservative gendered discourses and the resistance against subversive gendered discourses in Iranian EFL textbooks underpin gender norms and religious ideologies existing in Iran. This could serve the interests of powerful groups in Iran, maintain the status quo and ensure the dominance of powerful groups\u2018 ideologies on gender (Fairclough, 1992a, 1995, 2001, 2003; Fairclough & Wodak, 1997; van Dijk, 1993a, 1998, 2008, Wodak, 1995, 1997, 2005, 2012). \nWith regard to interview results one can argue that, the existence of hegemonic readings in the focus group of this study indicates that the hegemonic messages disseminated through Iranian EFL textbooks could produce the intended reaction in some audiences of such textbooks (Hall, 1990). The existence of oppositional readings indicates that some audiences of Iranian EFL textbooks constructed themselves different from the representations of male and female social actors depicted by their textbooks. According to van \nLeeuwen (2003), one can assert that these social actors were excluded from the representations of male and female social actors in Iranian EFL textbooks. \nSuch exclusions could obscure the reality regarding the existence of such gender identities (van Leeuwen, 2003; Sunderland, 2004). \nThe findings suggest that a truly balanced treatment of gender equality has yet to be achieved in the analysed textbooks. CDA could underlie the existence of hegemonic ideologies in educational materials and the results of studies such as the one at hand should be shared with both the academia and men and women in society at large.\n    [2] Title: Social Bias Frames: Reasoning about Social and Power Implications of Language. Abstract: Warning: this paper contains content that may be offensive or upsetting. Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people\u2019s judgments about others. For example, given a statement that \u201cwe shouldn\u2019t lower our standards to hire more women,\u201d most listeners will infer the implicature intended by the speaker - that \u201cwomen (candidates) are less qualified.\u201d Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text. We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80% F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames. Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.\n    [3] Title: A cosy consensus on deviant discourse: How the refugee and asylum seeker meta-narrative has endorsed an interpretative crisis in relation to the transnational politics of the world's displaced persons.. Abstract: Immigration is a key feature in late capitalist societies, with some 20,000,000 displaced persons worldwide. This paper reports on coverage of refugees and asylum seekers in English-language newspapers worldwide, drawing on media content between 2003 and 2004. It analyses media discourse on refugees and asylum seekers across the world, with a particular focus on deconstructing negative coverage. Five dominant negative frames in international media discourses are identified. These themes are examined in the context of theories of racism and xenophobia to highlight their negative potential for displaced persons and attitudes towards them in their host countries. Theory is also employed to explore the potential utility of such negative narratives for the media and social elites. The work being presented here is part of a much larger research project being undertaken by the authors at the University of Limerick. (For preliminary findings see Devereux and Breen, 2003 and 2004). Introduction The role of the media in perpetuating the \u2018otherness\u2019 of refugees and asylum seekers is the focus of this paper. The concept of the other and our understanding of its construction and function are applied to the empirical example of international press coverage of refugee and asylum seekers. Using this data source, we explore the role of the media in excluding refugees and asylum seekers through the production and reproduction of othering discourses. Specifically, we deconstruct negative media discourses to demonstrate their basis in notions of otherness and threat. We also acknowledge the existence of more positive coverage and the potential of the media to promote inclusion. This paper begins with a discussion of key concepts in theories of racism and xenophobia, which inform our analysis. In section two, the linkages between these concepts and media theory are outlined. The methodology informing this paper and the wider research project of which it is part are then elaborated. The five key negative frames identified within international media discourse are subsequently outlined; asylum seekers are represented as \u2018an economic threat\u2019; \u2018a threat to national and local integrity\u2019; \u2018a criminal element\u2019; \u2018social deviants\u2019 and as \u2018illegal aliens\u2019. Finally, our nascent conclusions regarding media constructions of asylum seekers and refugees are presented. The Social Function of Fear Fear is a powerful emotion. At a primeval level, fear can draw people together, seeking safety in numbers. The group that faces a common threat often benefits in terms of cohesion. Thus, historically a common fear \u2013 of flood, of a hard winter, of wild animals has often had positive ramifications for communities, strengthening bonds that might otherwise have been weak, creating interdependence where there might instead have been anomie, establishing a community where instead there would only have been individuals. Our innate understanding of its binding power, is reflected in manufactured fears emerging from the common consciousness \u2013 religion and the paranormal providing many examples which are experienced in parallel with genuine threats, filling a void should one exist or perhaps providing a more manageable source of fear in place of the too frightening uncontrollability of external and natural forces. Those in leadership roles have been particularly cognisant of the benefits of a common threat. They have long recognised its potential not only for creating cohesion, but also for preventing dissent and in-fighting \u2013 phenomena which not only have a negative impact upon community, but also upon a leader\u2019s hold over power. Thus, fears have also been manufactured by elites to generate necessary or desired cohesion. Closely related to the common threat is scapegoating. Cohesion is maintained through the creation of a scapegoat, which can be blamed for negative happenings. Scapegoats can prevent a group having to face its own flaws \u2013 in The Burning of Bridget Cleary, for example, Angela Bourke (1999) identifies fairies as a device used by the community to apportion blame for socially unacceptable events. A communities\u2019 complicit acceptance of a supernatural explanation for a spouse\u2019s disappearance could prevent the necessity of addressing the existence of illicit activity in their midst and the flawed nature of their members. Bourke\u2019s (1999) analysis also evokes an understanding of the function of scapegoating for disempowered groups. For the dependent and powerless, blaming the fairies for domestic violence may have been easier than challenging a seemingly immutable patriarchal structure. In contemporary society also, it often seems preferable for the powerless to misapportion blame rather than face an immutable opponent. False class-consciousness is a powerful example of this. Hegemony is achieved and sustained by an ideology that emphasises that everybody can \u2018make it\u2019 in material terms if they choose to. Barry Glassner (2003), in his seminal text Culture of Fear, emphasises that misinformation rather than consciously misplaced blame may, however, be the key to understanding widespread scapegoating. Elites who which to divert attention from their corruption and errors and beneficiaries of inequality who wish to maintain the status quo all benefit from, if not generating, at least leaving unchallenged, misinformation and myths. Glassner\u2019s (2003) text provides clear examples of the reciprocal relationship between the common threat and the scapegoat. The construction of an external threat often employs the devise of scapegoating to demonstrate the immediacy and relevance of the danger, or to manufacture a threat where there exist only uncharted waters, while the scapegoat as a source of blame is inherently viewed as threatening. Both require the creation of a narrative, which elaborates to the in-group members whom they should fear and why. Prejudice, viewed as a social norm to which individuals conform, has been theorised as having its roots not in individual personality disorder, but in a societal need to scapegoat. According to this theory hostility towards minorities is a result of a social system in which: \"the achievement level of large sections of the population falls short of the normatively sanctioned aspiration level.\" (Hartman and Husband, 1974: 45) Thus someone who has high ethnic status (i.e. is white) and a high educational status, but has a low occupational status, may use a minority group as a scapegoat to explain their status inconsistency for instance they might claim that immigrants were 'taking all the jobs'. Discrimination, the manifest expression of prejudice, is also not simply a number of isolated actions, which favour one group over another. It is a system of social relations, both individual and institutional that places and maintains power in the hands of the majority. Discrimination may mean denial of access to wealth, education, legal and social protection. It can also mean forced assimilation or segregation (Hartmann and Husband, 1974). In this paper we are focused upon a situation in which the subject of the narrative, the source of the manufactured or artificially inflated threat, the scapegoat, is not a figment of fairytale, but human beings, in this case asylum seekers and refugees. We identify and deconstruct media coverage which create this group as a threat. Our aim is to highlight the congruence between the various elements of these narratives and scapegoating, in order to add to our understanding of how such threats are made palpable and acute for the public. Through this undertaking, we also aim to highlight the enormous negative potential of such discourses for the human beings who are their subject. Bad News Sells ... General Panics Run and Run The \u2018knowledge gap\u2019 between host populations and displaced persons is key to understanding the significance of media coverage in formulating public attitudes towards asylum seekers and refugees. In addition to the general climate of social distance, which exists between the majority and minorities, asylum seekers often live lives very distant from those of host populations. Opportunities to interact in the same social space are often limited by restrictions on the right to work, where to live and financial deprivation resulting from limited welfare. In other cases, the opportunity to occupy even the same physical space are limited by detention, placement in remote locations and restriction to communal living centres. In Ireland, for example, it was suggested that asylum seekers be housed on specially converted ships on the river Suir in County Waterford. Foucault\u2019s \u2018Ship of Fools\u2019 was to become a \u2018Ship of Asylum Seekers\u2019. While refugees are not subject to such spatial segregation, they may experience social segregation as a consequences of racial prejudice and discrimination. Whether rooted in spatial or social segregation, the resultant social distance between host populations and asylum seekers and refugees leaves the former with few (respected) routes of learning about the latter, beyond the media. Although the public may not unquestioningly accept the perspective of the media, lacking an alternative, the power of the media\u2019s message is indisputable. According to Benjamin D. Singer: \"..it may be argued that the potential for identifiable symbolic content through the media is probably greater than direct contact with the native people (themselves)..\" (Singer, 1983: 234) In Racism and the Mass Media: A study of the role of the mass media in the formation of white beliefs and attitudes in Britain, Hartmann and Husband (1974) as a prelude to a content analysis of press coverage similar to that elaborated here, examined people\u2019s attitudes to and definitions of the racial situation in Britain. They concluded that people\u2019s situations, the attitudes prevailing in their local area and the numbers of non-white people living there, determined affective and evaluative attitudes in combi\n    [4] Title: Lexicalisation in News Stories of Some Nigerian National Newspapers. Abstract: There has not been much research on unmasking the i deological bias embedded in the language of the seemingly objective representations f people, events, institutions, and policies in Nigerian newspapers. Thus, this study e xplores how lexical choices are used to simultaneously convey information and judgment on p eople, events, and policies in Nigerian newspaper news reports. The study applies critical discourse analysis to analyse selected news stories relating to politics, economy, securit y, sports, health, and religion, from four Nigerian national daily newspapers ( The Guardian, The Nation, Nigerian Tribune, and Daily Trust). Findings of the study reveal that Nigerian newsp aper reporters\u2019 choice of lexical patterns produces differential judgmental stances w hich have some control on the attitudes and actions of readers towards the people, events a nd policies represented. The findings also depict that Nigerian newspapers often represent peo ple, events, and policies through sensational but very blatantly biased, inflammatory and pejorative lexical items, which undoubtedly reveal the reporters\u2019 desperation to pr opagandise, sensationalise, binarise, or impose certain meanings upon the webs of complexity in news discourse in Nigerian newspapers. Thus, the study recommends that Nigeria n journalists, who are products of a nation-state that is confronted with many problems, should strive to be fair, balanced, and restrained in their deployment of representational resources. LUMINA, Vol. 23, No.2, ISSN 2094-1188 Introduction In Nigerian media discourse, the reporter uses a wi de variety of representational resources in doing his or her job. Some of the ling uistic structures, pragmatic codings, styles of presentation, and visual imagery are his or her own choosing while others are re-enacted or reproduced to appropriately represent the ideas, in tentions, power, and emotion of the agents or news makers (who do not hesitate to express thei r pr judice and discrimination on some issues, societies or groups) in the news discourse. Thus, the reporter\u2019s language, style, and visual images at times affect, shape and control th e attitude, opinion and thinking of the audience about some people, policies, events and si tuations. The problem of the re-enactment of power, prejudice and discrimination in Nigerian print media discourse can be said to be capable of promoting social, political and economic discrimination in the country because newspapers have enormous and lasting influence, lan guage being the main tool of this influence (Smirnova, 2009: 80). Newspapers can be r ead and re-read by audience; they may be shared with friends, family members and workmate s; and they may be photocopied and archived in order to have permanent access to them. This is why, according to Liu (2009: 60), the last few decades have witnessed a great deal of research on print media representations from different perspectives. The newspaper is a text that exemplifies language use in a social context. It is a multimodal or multisemiotic text. It is polythemati c and involves a lot of societal problems and issues. It uses linguistic, discourse and visua l resources in representing (or misrepresenting) people, events, opinions, and poli cies. The choice and use of these linguistic resources are not in all cases the journalists\u2019 cre ation but are dictated by the values and norms of their institution (Pan, 2002: 51). In other word s, lexicalisation and imaging in media discourse are rooted in some specific cultures and ideologies as discourse itself is ideology (van Dijk, 1991). Media representations through lan guage and images are often found to be discrepant from so-called objective reality (Teo, 2 000). Such representations, however, determine public perceptions (or misperception) of s me people, sects, events, and policies are capable of distorting audience\u2019s attitudes, opi ni ns, and actions towards other people, societies, and events. Media researchers claim that media representations influence people\u2019s perceptions of reality. In other words, individuals learn about events, people, and issues from the media and they react based on this knowledge (S otirovic, 2001). LUMINA, Vol. 23, No.2, ISSN 2094-1188 Although there have been several critical studies o n media discourse in the Western countries, there are very few of such in Nigeria. T he few ones available focused primarily on interesting devices in Nigerian newspaper reporting , using methodologies and analytical insights of error analysis, text linguistics, genre analysis, stylistics, sociolinguistics, pragmatics, and semiotics. Duncan (2006) employs di scourse analysis in his assessment of language and discourse trends and themes which mani fest in print media representation of Black South Africans in the fading days of aparthei d. His findings reveal that the overriding discourse was that of Blacks as violent, untrustwor thy, subhuman, racist, and unreasonable. He also reports the near-complete absence of simila r emotive descriptors in news stories depicting Whites as perpetrators of violence. Oloru nnisola (2006) is also a study that uses discourse analysis to unravel the language and disc our e of racism in some local and international newspapers in South Africa. The study clearly confirms the existence of a typical separation of powers along racial lines typ ified. He observes that there are a few instances of language of conflict and discriminatio n n some news stories in a way that readers are left with little doubt that there are d ivisions along racial lines. Nevertheless, Duncan (2006) and Olorunnisola (2006) address only e social problem racism. Yin (2007) combines the analysis of media texts and u ience interpretations to explore the extent towhich the media narratives con strain audience interpretations and the extent to which audience members can resistthe pref rred meanings presented by the media. His study affirms that media texts serve as mate-na rratives foraudience interpretations. Even when audience members are able to resist the ideolo gy c nveyed by one type ofmedia, they rely on the dominant meanings of another. Caldas-Co ulthard and Moon (2010) use corpus methodology as a research tool to investigate how s cial actors are classified in the public discourse of the media with lexis as a point of ent ry. Their findings indicate that uses of premodification associated with the two newspapers in Br tain and their lexical choices produce differential judgmental stances that have some soci al effects. They also observe that the media categorize people through very specific point f view and values not always apparent to the uncritical audience. This indicates that med ia all over the world craftily often take advantage of narrative as a powerful tool to influe nce the audience or determine their thoughts about people. Ismail (2010) investigates m ainstream U.S. newspaper discourses concerning the dividing wall that Israel built as a eparation barrier from the West Bank. He LUMINA, Vol. 23, No.2, ISSN 2094-1188 explores news media\u2019s role as agents of social cont rol and influence and observes that while the media\u2019s role in social control and influence ma y be significant, the adequacy with which they perform this role is questionable. Closely rel at d to the above, Pounds (2010) uses critical linguistic analysis to investigate authori al stance in English news reporting and the nature of authorial voice itself. He observes that expressive resources and reporting style in Italian news reporting are often loaded with prejud ice.\n    [5] Title: Progressive planning, biased discourse, and critique: An agentic perspective. Abstract: Progressive theorists and reflective practitioners have exhorted agents to renounce exploitative planning discourses. This repudiation can, however, only be successful if the agent\u2019s own investment in biased discourses is accounted for. Reflecting on the work of Jon Elster, John Thompson, and Raymond Geuss, it is argued that the progressive planner should direct agents toward an acknowledgement of the motives compelling them to become invested in biased discourse, and how these discourses satisfy these motives by capitalizing on the trappings of habitual modes of reasoning and thriving on the ambiguity of referents. The value of this perspective is illustrated in an analysis of neoliberalism as a contemporary hegemonic discourse. It is discussed how popular anxieties about social mobility and community power motivate the investment in discourses characterized by a thesis of the inevitability of neoliberal restructuring and associations with narratives on entrepreneurialism, multiculturalism, and self-help.\n    [6] Title: Carnivalising Postcolonial Zimbabwe: The Vulgar and Grotesque Logic of Postcolonial Protest in NoViolet Bulawayo's We Need New Names (2013). Abstract: Summary This article set out to explore NoViolet Bulawayo's We need new names from the perspective of carnivalised writing. The objectives of the article were to unpack how the vulgar and the grotesque were used to create carnival moments in the narrative and to examine how marginal subjects gain voice and some degree of power to live an alternative life, even if this is momentary. It sought to examine how Bulawayo derives her aesthetics from the vulgar and the grotesque to create a carnivalesque logic that informs postcolonial protest in the novel. The analysis made use of the theoretical concepts of carnival propounded by Mikhail Bakhtin. This article argues that the text is constituted by a regime of the vulgar, which the child characters deploy for transgressing hegemonic practices and authoritative discourses. Social norms are suspended and the children have a subversive agency, courtesy of parody and satire. The article reveals that apart from speaking back to power, the children harness the image of kaka (human excrement) as a discursive resource to satirise the failures of the Zimbabwean postcolony and to degrade all forms of authority. It is concluded that while the scatological in the novel suggests social indictment, the images of kaka and dirt fail to transcend protest to see the realisation of a desired postcolonial condition.\n    [7] Title: Comunicaci\u00f3n y representaci\u00f3n: an\u00e1lisis de la construcci\u00f3n sociocultural de los modelos de feminidad en la revista Hogar. Abstract: This study makes a discursive analysis around the social construction of the femininity in the contents of the Hogar magazine, in order to recognize and highlight a hegemonic model of woman. The categories wich make and essential part of development of this work are: representation, gender, culture, communication and narrative; the review of each one of them represents a theorical contribution to work on the discourses that are produced and exposed to everybody in an Ecuadorian women's magazine. The discursive content selected for analysis in this study are the sing of biased view of the woman, wich has not changed significantly over time, but it also presents women who are \u201cfully incuded in the life of the country\u201d only through reduced numbers of labor inclusion, educational and of growth of female-headed households in families.\n    [8] Title: nan. Abstract: nan\n    [9] Title: nan. Abstract: nan\n             "