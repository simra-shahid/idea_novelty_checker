"Examples Reviews that assess novelty of an idea using related papers:\n                - Not Novel: \n                    Idea: Create a **benchmarking approach** designed **to evaluate analogical reasoning in language models** by utilizing a **novel analogical reasoning corpus aggregated through environmental dna metabarcoding** techniques applied to text data. This corpus would be constructed by identifying and extracting analogical expressions from a vast array of textual sources, similar to how eDNA captures biodiversity. The agent will be evaluated through **performance comparison** with existing datasets to measure its ability to identify and reason with analogies.\n    Class: not novel\n    Review: The idea is not novel because it closely replicates existing work. For instance, the concept of creating a corpus by aggregating analogical expressions from textual sources is similar to the approach in ANALOGYKB[0]. Additionally, the benchmarking method for evaluating analogical reasoning is akin to the one used in AnaloBench[1].\n    Related Papers:\n    [0] Title: ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base. Abstract: Analogical reasoning is a fundamental cognitive ability of humans. However, current language models (LMs) still struggle to achieve human-like performance in analogical reasoning tasks due to a lack of resources for model training. In this work, we address this gap by proposing ANALOGYKB, a million-scale analogy knowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB identifies two types of analogies from the KGs: 1) analogies of the same relations, which can be directly extracted from the KGs, and 2) analogies of analogous relations, which are identified with a selection and filtering pipeline enabled by large language models (LLMs), followed by minor human efforts for data quality control. Evaluations on a series of datasets of two analogical reasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB successfully enables both smaller LMs and LLMs to gain better analogical reasoning capabilities.\n    [1] Title: AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies. Abstract: Humans regularly engage in analogical thinking, relating personal experiences to current situations ($X$ is analogous to $Y$ because of $Z$). Analogical thinking allows humans to solve problems in creative ways, grasp difficult concepts, and articulate ideas more effectively. Can language models (LMs) do the same? To answer this question, we propose ANALOBENCH, a benchmark to determine analogical reasoning ability in LMs. Our benchmarking approach focuses on aspects of this ability that are common among humans: (i) recalling related experiences from a large amount of information, and (ii) applying analogical reasoning to complex and lengthy scenarios. We test a broad collection of proprietary models (e.g., GPT family, Claude V2) and open source models such as LLaMA2. As in prior results, scaling up LMs results in some performance boosts. Surprisingly, scale offers minimal gains when, (i) analogies involve lengthy scenarios, or (ii) recalling relevant scenarios from a large pool of information, a process analogous to finding a needle in a haystack. We hope these observations encourage further research in this field.\n    [2] Title: Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction. Abstract: The vital role of analogical reasoning in human cognition allows us to grasp novel concepts by linking them with familiar ones through shared relational structures. Despite the attention previous research has given to word analogies, this work suggests that Large Language Models (LLMs) often overlook the structures that underpin these analogies, raising questions about the efficacy of word analogies as a measure of analogical reasoning skills akin to human cognition. In response to this, our paper introduces a task of analogical structure abduction, grounded in cognitive psychology, designed to abduce structures that form an analogy between two systems. In support of this task, we establish a benchmark called SCAR, containing 400 scientific analogies from 13 distinct fields, tailored for evaluating analogical reasoning with structure abduction. The empirical evidence underlines the continued challenges faced by LLMs, including ChatGPT and GPT-4, in mastering this task, signifying the need for future exploration to enhance their abilities.\n    [3] Title: Relevant or Random: Can LLMs Truly Perform Analogical Reasoning?. Abstract: Analogical reasoning is a unique ability of humans to address unfamiliar challenges by transferring strategies from relevant past experiences. One key finding in psychology is that compared with irrelevant past experiences, recalling relevant ones can help humans better handle new tasks. Coincidentally, the NLP community has also recently found that self-generating relevant examples in the context can help large language models (LLMs) better solve a given problem than hand-crafted prompts. However, it is yet not clear whether relevance is the key factor eliciting such capability, i.e., can LLMs benefit more from self-generated relevant examples than irrelevant ones? In this work, we systematically explore whether LLMs can truly perform analogical reasoning on a diverse set of reasoning tasks. With extensive experiments and analysis, we show that self-generated random examples can surprisingly achieve comparable or even better performance, e.g., 4% performance boost on GSM8K with random biological examples. We find that the accuracy of self-generated examples is the key factor and subsequently design two improved methods with significantly reduced inference costs. Overall, we aim to advance a deeper understanding of LLM analogical reasoning and hope this work stimulates further research in the design of self-generated contexts.\n    [4] Title: Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models. Abstract: Large language models (LLMs) have performed well on several reasoning benchmarks, including ones that test analogical reasoning abilities. However, it has been debated whether they are actually performing humanlike abstract reasoning or instead employing less general processes that rely on similarity to what has been seen in their training data. Here we investigate the generality of analogy-making abilities previously claimed for LLMs (Webb, Holyoak,&Lu, 2023). We take one set of analogy problems used to evaluate LLMs and create a set of\"counterfactual\"variants-versions that test the same abstract reasoning abilities but that are likely dissimilar from any pre-training data. We test humans and three GPT models on both the original and counterfactual problems, and show that, while the performance of humans remains high for all the problems, the GPT models' performance declines sharply on the counterfactual set. This work provides evidence that, despite previously reported successes of LLMs on analogical reasoning, these models lack the robustness and generality of human analogy-making.\n    [5] Title: Life is a Circus and We are the Clowns: Automatically Finding Analogies between Situations and Processes. Abstract: Analogy-making gives rise to reasoning, abstraction, flexible categorization and counterfactual inference \u2013 abilities lacking in even the best AI systems today. Much research has suggested that analogies are key to non-brittle systems that can adapt to new domains. Despite their importance, analogies received little attention in the NLP community, with most research focusing on simple word analogies. Work that tackled more complex analogies relied heavily on manually constructed, hard-to-scale input representations.In this work, we explore a more realistic, challenging setup: our input is a pair of natural language procedural texts, describing a situation or a process (e.g., how the heart works/how a pump works). Our goal is to automatically extract entities and their relations from the text and find a mapping between the different domains based on relational similarity (e.g., blood is mapped to water). We develop an interpretable, scalable algorithm and demonstrate that it identifies the correct mappings 87% of the time for procedural texts and 94% for stories from cognitive-psychology literature. We show it can extract analogies from a large dataset of procedural texts, achieving 79% precision (analogy prevalence in data: 3%). Lastly, we demonstrate that our algorithm is robust to paraphrasing the input texts\n    [6] Title: Why Do We Need Neurosymbolic AI to Model Pragmatic Analogies?. Abstract: A hallmark of intelligence is the ability to use a familiar domain to make inferences about a less familiar domain, known as analogical reasoning. In this article, we delve into the performance of large language models (LLMs) in dealing with progressively complex analogies expressed in unstructured text. We discuss analogies at four distinct levels of complexity: lexical, syntactic, semantic, and pragmatic. As the analogies become more complex, they require increasingly extensive, diverse knowledge beyond the textual content, unlikely to be found in the lexical co-occurrence statistics that power LLMs. We discuss neurosymbolic AI techniques that combine statistical and symbolic AI, informing the representation of unstructured text to highlight and augment relevant content, provide abstraction, and guide the mapping process. This maintains the efficiency of LLMs while preserving the ability to explain analogies for pedagogical applications.\n    [7] Title: ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies. Abstract: Analogy-making is central to human cognition, allowing us to adapt to novel situations \u2013 an ability that current AI systems still lack. Most analogy datasets today focus on simple analogies (e.g., word analogies); datasets including complex types of analogies are typically manually curated and very small. We believe that this holds back progress in computational analogy.In this work, we design a data generation pipeline, ParallelPARC (Parallel Paragraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to create complex, paragraph-based analogies, as well as distractors, both simple and challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset of analogies between scientific processes. We publish a gold-set, validated by humans, and a silver-set, generated automatically. We test LLMs\u2019 and humans\u2019 analogy recognition in binary and multiple-choice settings, and found that humans outperform the best models (\u223c13% gap) after a light supervision. We demonstrate that our silver-set is useful for training models. Lastly, we show challenging distractors confuse LLMs, but not humans. We hope our pipeline will encourage research in this emerging field.\n    [8] Title: ARN: Analogical Reasoning on Narratives. Abstract: As a core cognitive skill that enables the transferability of information across domains, analogical reasoning has been extensively studied for both humans and computational models. However, while cognitive theories of analogy often focus on narratives and study the distinction between surface, relational, and system similarities, existing work in natural language processing has a narrower focus as far as relational analogies between word pairs. This gap brings a natural question: can state-of-the-art large language models (LLMs) detect system analogies between narratives? To gain insight into this question and extend word-based relational analogies to relational system analogies, we devise a comprehensive computational framework that operationalizes dominant theories of analogy, using narrative elements to create surface and system mappings. Leveraging the interplay between these mappings, we create a binary task and benchmark for Analogical Reasoning on Narratives (ARN), covering four categories of far (cross-domain)/near (within-domain) analogies and disanalogies. We show that while all LLMs can largely recognize near analogies, even the largest ones struggle with far analogies in a zero-shot setting, with GPT4.0 scoring below random. Guiding the models through solved examples and chain-of-thought reasoning enhances their analogical reasoning ability. Yet, since even in the few-shot setting, the best model only performs halfway between random and humans, ARN opens exciting directions for computational analogical reasoners.\n    [9] Title: Large Language Models as Analogical Reasoners. Abstract: Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, analogical prompting, designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench.\n\n\n    Idea: Develop a **mobile application** that allows users to **interactively provide feedback** on outputs generated by **language models**. This application would feature intuitive interfaces for users to **annotate and edit generated content directly on their devices**. The feedback gathered will be processed in real-time to **align language models with user preferences** without the need for extensive fine-tuning. The effectiveness of this approach will be evaluated using a **simulated user evaluation** where a GPT-4 model simulates diverse user interactions and preferences. This approach leverages the **ease of mobile input** to continuously update and refine the model's understanding of user preferences.\n    Class: not novel\n    Review: The idea is novel because the use of a mobile application for interactive feedback collection is unique compared to related work.\n    Related Papers:\n    [0] Title: Aligning LLM Agents by Learning Latent Preference from User Edits. Abstract: We study interactive learning of LLM-based language agents based on user edits made to the agent's output. In a typical setting such as writing assistants, the user interacts with a language agent to generate a response given a context, and may optionally edit the agent response to personalize it based on their latent preference, in addition to improving the correctness. The edit feedback is naturally generated, making it a suitable candidate for improving the agent's alignment with the user's preference, and for reducing the cost of user edits over time. We propose a learning framework, PRELUDE that infers a description of the user's latent preference based on historic edit data. The inferred user preference descriptions are used to define prompts for generating responses in the future. This avoids fine-tuning the agent, which is costly, challenging to scale with the number of users, and may even degrade its performance on other tasks. Furthermore, learning descriptive preference improves interpretability, allowing the user to view and modify the learned preference. However, user preference can be complex, subtle, and vary based on context, making it challenging to learn. To address this, we propose a simple yet effective algorithm named CIPHER that leverages the LLM to infer the user preference for a given context based on user edits. In the future, CIPHER retrieves inferred preferences from the k-closest contexts in the history, and forms an aggregate preference for response generation. We introduce two interactive environments -- summarization and email writing, and use a GPT-4 simulated user for evaluation. On both tasks, CIPHER outperforms several baselines by achieving the lowest edit distance cost while only having a small overhead in LLM query cost. Our analysis reports that user preferences learned by CIPHER show significant similarity to the ground truth latent preferences.\n    [1] Title: Data-Efficient Alignment of Large Language Models with Human Feedback Through Natural Language. Abstract: Learning from human feedback is a prominent technique to align the output of large language models (LLMs) with human expectations. Reinforcement learning from human feedback (RLHF) leverages human preference signals that are in the form of ranking of response pairs to perform this alignment. However, human preference on LLM outputs can come in much richer forms including natural language, which may provide detailed feedback on strengths and weaknesses of a given response. In this work we investigate data efficiency of modeling human feedback that is in natural language. Specifically, we fine-tune an open-source LLM, e.g., Falcon-40B-Instruct, on a relatively small amount (1000 records or even less) of human feedback in natural language in the form of critiques and revisions of responses. We show that this model is able to improve the quality of responses from even some of the strongest LLMs such as ChatGPT, BARD, and Vicuna, through critique and revision of those responses. For instance, through one iteration of revision of ChatGPT responses, the revised responses have 56.6% win rate over the original ones, and this win rate can be further improved to 65.9% after applying the revision for five iterations.\n    [2] Title: A Survey on Human Preference Learning for Large Language Models. Abstract: The recent surge of versatile large language models (LLMs) largely depends on aligning increasingly capable foundation models with human intentions by preference learning, enhancing LLMs with excellent applicability and effectiveness in a wide range of contexts. Despite the numerous related studies conducted, a perspective on how human preferences are introduced into LLMs remains limited, which may prevent a deeper comprehension of the relationships between human preferences and LLMs as well as the realization of their limitations. In this survey, we review the progress in exploring human preference learning for LLMs from a preference-centered perspective, covering the sources and formats of preference feedback, the modeling and usage of preference signals, as well as the evaluation of the aligned LLMs. We first categorize the human feedback according to data sources and formats. We then summarize techniques for human preferences modeling and compare the advantages and disadvantages of different schools of models. Moreover, we present various preference usage methods sorted by the objectives to utilize human preference signals. Finally, we summarize some prevailing approaches to evaluate LLMs in terms of alignment with human intentions and discuss our outlooks on the human intention alignment for LLMs.\n    [3] Title: PMG : Personalized Multimodal Generation with Large Language Models. Abstract: The emergence of large language models (LLMs) has revolutionized the capabilities of text comprehension and generation. Multi-modal generation attracts great attention from both the industry and academia, but there is little work on personalized generation, which has important applications such as recommender systems. This paper proposes the first method for personalized multimodal generation using LLMs, showcases its applications and validates its performance via an extensive experimental study on two datasets. The proposed method, Personalized Multimodal Generation (PMG for short) first converts user behaviors (e.g., clicks in recommender systems or conversations with a virtual assistant) into natural language to facilitate LLM understanding and extract user preference descriptions. Such user preferences are then fed into a generator, such as a multimodal LLM or diffusion model, to produce personalized content. To capture user preferences comprehensively and accurately, we propose to let the LLM output a combination of explicit keywords and implicit embeddings to represent user preferences. Then the combination of keywords and embeddings are used as prompts to condition the generator. We optimize a weighted sum of the accuracy and preference scores so that the generated content has a good balance between them. Compared to a baseline method without personalization, PMG has a significant improvement on personalization for up to 8% in terms of LPIPS while retaining the accuracy of generation.\n    [4] Title: Improving Context-Aware Preference Modeling for Language Models. Abstract: While finetuning language models from pairwise preferences has proven remarkably effective, the underspecified nature of natural language presents critical challenges. Direct preference feedback is uninterpretable, difficult to provide where multidimensional criteria may apply, and often inconsistent, either because it is based on incomplete instructions or provided by diverse principals. To address these challenges, we consider the two-step preference modeling procedure that first resolves the under-specification by selecting a context, and then evaluates preference with respect to the chosen context. We decompose reward modeling error according to these two steps, which suggests that supervising context in addition to context-specific preference may be a viable approach to aligning models with diverse human preferences. For this to work, the ability of models to evaluate context-specific preference is critical. To this end, we contribute context-conditioned preference datasets and accompanying experiments that investigate the ability of language models to evaluate context-specific preference. We use our datasets to (1) show that existing preference models benefit from, but fail to fully consider, added context, (2) finetune a context-aware reward model with context-specific performance exceeding that of GPT-4 and Llama 3 70B on tested datasets, and (3) investigate the value of context-aware preference modeling.\n    [5] Title: Towards Aligning Language Models with Textual Feedback. Abstract: We present ALT (ALignment with Textual feedback), an approach that aligns language models with user preferences expressed in text. We argue that text offers greater expressiveness, enabling users to provide richer feedback than simple comparative preferences and this richer feedback can lead to more efficient and effective alignment. ALT aligns the model by conditioning its generation on the textual feedback. Our method relies solely on language modeling techniques and requires minimal hyper-parameter tuning, though it still presents the main benefits of RL-based alignment algorithms and can effectively learn from textual feedback. We explore the efficacy and efficiency of textual feedback across different tasks such as toxicity reduction, summarization, and dialog response generation. We find that ALT outperforms PPO for the task of toxicity reduction while being able to match its performance on summarization with only 20% of the samples. We also explore how ALT can be used with feedback provided by an existing LLM where we explore an LLM providing constrained and unconstrained textual feedback. We also outline future directions to align models with natural language feedback.\n    [6] Title: Training Language Models with Language Feedback. Abstract: Pretrained language models often do not perform tasks in ways that are in line with our preferences, e.g., generating offensive text or factually incorrect summaries. Recent work approaches the above issue by learning from a simple form of human evaluation: comparisons between pairs of model-generated task outputs. Comparison feedback conveys limited information about human preferences per human evaluation. Here, we propose to learn from natural language feedback, which conveys more information per human evaluation. We learn from language feedback on model outputs using a three-step learning algorithm. First, we condition the language model on the initial output and feedback to generate many refinements. Second, we choose the refinement with the highest similarity to the feedback. Third, we finetune a language model to maximize the likelihood of the chosen refinement given the input. In synthetic experiments, we first evaluate whether language models accurately incorporate feedback to produce refinements, finding that only large language models (175B parameters) do so. Using only 100 samples of human-written feedback, our learning algorithm finetunes a GPT-3 model to roughly human-level summarization ability.\n    [7] Title: Training Language Models with Natural Language Feedback. Abstract: Pretrained language models often do not perform tasks in ways that are in line with our preferences, e.g., generating offensive text or factually incorrect summaries. Recent work approaches the above issue by learning from a simple form of human evaluation: comparisons between pairs of model-generated task outputs. Comparison feedback conveys limited information about human preferences per human evaluation. Here, we propose to learn from natural language feedback, which conveys more information per human evaluation. We learn from language feedback on model outputs using a three-step learning algorithm. First, we condition the language model on the initial output and feedback to generate many re\ufb01nements. Second, we choose the re\ufb01ne-ment with the highest similarity to the feed-back. Third, we \ufb01netune a language model to maximize the likelihood of the chosen re\ufb01ne-ment given the input. In synthetic experiments, we \ufb01rst evaluate whether language models accurately incorporate feedback to produce re-\ufb01nements, \ufb01nding that only large language models (175B parameters) do so. Using only 100 samples of human-written feedback, our learning algorithm \ufb01netunes a GPT-3 model to roughly human-level summarization ability.\n    [8] Title: Chain of Hindsight Aligns Language Models with Feedback. Abstract: Learning from human preferences is important for language models to match human needs and to align with human and social values. Prior works have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them inefficient in terms of data utilization and challenging to apply in general, or they depend on reinforcement learning, which often suffers from imperfect reward functions and relies on extremely challenging optimizations. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sequences of sentences, which are then used to fine-tune the model, allowing us to take advantage of the language comprehension capabilities of language models. We condition the model on a sequence of model generations paired with feedback. By doing so, the model is trained to generate outputs based on feedback, while learning to identify and correct negative attributes or errors. Applying our method to large language models, we observed that Chain of Hindsight significantly surpasses previous methods in aligning language models with human preferences. We report significant improvements on summarization and dialogue benchmarks, with our approach markedly preferred in human evaluations.\n    [9] Title: Languages are Rewards: Chain of Hindsight Finetuning using Human Feedback. Abstract: Learning from human preferences is important for language models to be helpful and useful for humans, and to align with human and social values. Existing works focus on supervised \ufb01netun-ing of pretrained models, based on curated model generations that are preferred by human labelers. Such works have achieved remarkable successes in understanding and following instructions ( e . g ., InstructGPT, ChatGPT, etc). However, to date, a key limitation of supervised \ufb01netuning is that it cannot learn from negative ratings; models are only trained on positive-rated data, which makes it data inef\ufb01cient. Because collecting human feed-back data is both time consuming and expensive, it is vital for the model to learn from all feedback, akin to the remarkable ability of humans to learn from diverse feedback. In this work, we propose a novel technique called Hindsight Finetuning for making language models learn from diverse human feedback. In fact, our idea is motivated by how humans learn from hindsight experience. We condition the model on a sequence of model generations paired with hindsight feedback, and \ufb01netune the model to predict the most preferred output. By doing so, models can learn to identify and correct negative attributes or errors. Applying the method to GPT-J, we observe that it significantly improves results on summarization and dialogue tasks using the same amount of human feedback.\n\n\n    Idea: Create an advanced literature search tool that uses **citation-based suggestions with visualizations** to **boost scientific innovation**. The tool would provide interactive visualizations of citation networks combined with detailed author profiles, allowing users to explore how different researchers and their work interconnect. Users could refine their searches using keywords and see the influence of various authors across different domains. The effectiveness of this tool would be assessed through **simulated sessions and user study**, ensuring it supports researchers in identifying novel connections and fostering interdisciplinary innovation.\n    Class: not novel\n    Review: The idea is not novel because it closely resembles existing tools like 'Bursting Scientific Filter Bubbles'[4], Bridger[9] and ResearchRabbit[3], which also use citation-based suggestions and visualizations. The proposed tool's combination of citation networks and author profiles is already well-covered by these systems.\n    Related Papers:\n    [0] Title: PUREsuggest: Citation-based Literature Search and Visual Exploration with Keyword-controlled Rankings. Abstract: Citations allow quickly identifying related research. If multiple publications are selected as seeds, specific suggestions for related literature can be made based on the number of incoming and outgoing citation links to this selection. Interactively adding recommended publications to the selection refines the next suggestion and incrementally builds a relevant collection of publications. Following this approach, the paper presents a search and foraging approach, PUREsuggest, which combines citation-based suggestions with augmented visualizations of the citation network. The focus and novelty of the approach is, first, the transparency of how the rankings are explained visually and, second, that the process can be steered through user-defined keywords, which reflect topics of interests. The system can be used to build new literature collections, to update and assess existing ones, as well as to use the collected literature for identifying relevant experts in the field. We evaluated the recommendation approach through simulated sessions and performed a user study investigating search strategies and usage patterns supported by the interface.\n    [1] Title: Efficacy improvement in searching MEDLINE database using a novel PubMed visual analytic system: EEEvis. Abstract: PubMed is the most extensively used database and search engine in the biomedical and healthcare fields. However, users could experience several difficulties in acquiring their target papers facing massive numbers of search results, especially in their unfamiliar fields. Therefore, we developed a novel user interface for PubMed and conducted three steps of study: step A, a preliminary user survey with 76 medical experts regarding the current usability for the biomedical literature search task at PubMed; step B is implementing EEEvis, a novel interactive visual analytic system for the search task; step C, a randomized user study comparing PubMed and EEEvis. First, we conducted a Google survey of 76 medical experts regarding the unmet needs of PubMed and the user requirements for a novel search interface. According to the data of preliminary Google survey, we implemented a novel interactive visual analytic system for biomedical literature search. This EEEvis provides enhanced literature data analysis functions including (1) an overview of the bibliographic features including publication date, citation count, and impact factors, (2) an overview of the co-authorship network, and (3) interactive sorting, filtering, and highlighting. In the randomized user study of 24 medical experts, the search speed of EEEvis was not inferior to PubMed in the time to reach the first article (median difference 3 sec, 95% CI -2.1 to 8.5, P = 0.535) nor in the search completion time (median difference 8 sec, 95% CI -4.7 to 19.1, P = 0.771). However, 22 participants (91.7%) responded that they are willing to use EEEvis as their first choice for a biomedical literature search task, and 21 participants (87.5%) answered the bibliographic sorting and filtering functionalities of EEEvis as a major advantage. EEEvis could be a supplementary interface for PubMed that can enhance the user experience in the search for biomedical literature.\n    [2] Title: Visual Analysis and Dissemination of Scientific Literature Collections with SurVis. Abstract: Bibliographic data such as collections of scientific articles and citation networks have been studied extensively in information visualization and visual analytics research. Powerful systems have been built to support various types of bibliographic analysis, but they require some training and cannot be used to disseminate the insights gained. In contrast, we focused on developing a more accessible visual analytics system, called SurVis, that is ready to disseminate a carefully surveyed literature collection. The authors of a survey may use our Web-based system to structure and analyze their literature database. Later, readers of the survey can obtain an overview, quickly retrieve specific publications, and reproduce or extend the original bibliographic analysis. Our system employs a set of selectors that enable users to filter and browse the literature collection as well as to control interactive visualizations. The versatile selector concept includes selectors for textual search, filtering by keywords and meta-information, selection and clustering of similar publications, and following citation links. Agreement to the selector is represented by word-sized sparkline visualizations seamlessly integrated into the user interface. Based on an analysis of the analytical reasoning process, we derived requirements for the system. We developed the system in a formative way involving other researchers writing literature surveys. A questionnaire study with 14 visual analytics experts confirms that SurVis meets the initially formulated requirements.\n    [3] Title: ResearchRabbit. Abstract: ResearchRabbit is a scholarly publication discovery tool supported by artificial intelligence (AI). It was developed in 2021 by a team of three in Seattle [1]. This tool lets users discover publications related to one or more seed publications with the help of visualization maps and lists of earlier, later, and similar publications. ResearchRabbit is designed to support the workflow of unstructured searching while providing a left-to-right trail from the original publication(s) through any selected authors or publications. These trails, which can run as deep as rabbit holes, suggest the origin of the tool\u2019s name. To start using ResearchRabbit, users first need to create an account. Then they need to create a collection and add at least one publication. The more publications that are added, the better ResearchRabbit can understand users\u2019 interests and generate recommendations similar to the contents of the collection. Publications can be added either by uploading a RIS or BibTeX file or by using ResearchRabbit\u2019s search, powered by PubMed, if users are searching the medical sciences, or Semantic Scholar, for any other subject area. While ResearchRabbit uses PubMed\u2019s and Semantic Scholar\u2019s search engines, the company claims its unique database of \u201c100s of millions of academic articles\u201d is second in size only to Google Scholar [2]. Once publications are in a collection, ResearchRabbit\u2019s algorithm will begin generating recommendations. These recommendations can be explored through two modes: 1) by Papers that are Similar work, Earlier work, or Later work or 2) by People that provide additional publications that These authors or Suggested authors have published (Figure 1). These recommendations are depicted using visualization maps. Fig. 1 Different exploration modes\n    [4] Title: Bursting Scientific Filter Bubbles: Boosting Innovation via Novel Author Discovery. Abstract: Isolated silos of scientific research and the growing challenge of information overload limit awareness across the literature and hinder innovation. Algorithmic curation and recommendation, which often prioritize relevance, can further reinforce these informational \u201cfilter bubbles.\u201d In response, we describe Bridger, a system for facilitating discovery of scholars and their work. We construct a faceted representation of authors with information gleaned from their papers and inferred author personas, and use it to develop an approach that locates commonalities and contrasts between scientists to balance relevance and novelty. In studies with computer science researchers, this approach helps users discover authors considered useful for generating novel research directions. We also demonstrate an approach for displaying information about authors, boosting the ability to understand the work of new, unfamiliar scholars. Our analysis reveals that Bridger connects authors who have different citation profiles and publish in different venues, raising the prospect of bridging diverse scientific communities.\n    [5] Title: Visual impact beam plots: Analyzing research profiles and bibliometric metrics using the following-leading clustering algorithm (FLCA). Abstract: Background: A new approach to showcasing author publications on a website involves using a visual representation instead of the conventional paper list. The creation of an impact beam plot (IBP) as a research profile for individuals is crucial, especially when incorporating collection edges that include self-cited articles through a rare cluster analysis technique not commonly found in the literature. This study presents the application of a unique method called the following-leading clustering algorithm (FLCA) to generate IBPs for 3 highly productive authors. Methods: For the 3 highly productive authors, Sung-Ho Jang from South Korea, Chia-Hung Kao from Taiwan, and Chin-Hsiao Tseng from Taiwan, all their published articles indexed in the Web of Science Core Collection were downloaded. Sung-Ho Jang published 593 articles, Chia-Hung Kao published 732 articles, and Chin-Hsiao Tseng published 160 articles. To analyze and showcase their publications, the FLCA was utilized. This algorithm helped cluster their articles and identify representative publications for each author. To assess the effectiveness and validity of the FLCA algorithm, both network charts and heatmaps with dendrograms were employed. IBPs were then created and compared for each of the 3 authors, taking into consideration their h-index, x-index, and self-citation rate. This allowed for a comprehensive visual representation of their research impact and citation patterns. Results: The results show that these authors\u2019 h-index, x-index, and self-citation rates were (37, 44.01, 1.66%), (42, 61.47, 0.23%), and (37, 40.3, 6.62%), respectively. A higher value in these metrics indicates a more remarkable research achievement. A higher self-citation rate with a lower cluster number indicates that manuscripts are more likely to have been self-drafted. Using the FLCA algorithm, IBPs were successfully generated for each author. Conclusion: The FLCA algorithm allows for the easy generation of visual IBPs based on authors\u2019 publication profiles. These IBPs incorporate 3 important bibliometric metrics: h-index, x-index, and self-citations. These metrics are highly recommended for use by researchers globally, particularly with the self-citation rate, as they offer valuable insights into the scholarly impact and citation patterns of individual researchers.\n    [6] Title: Science Mapping: A Systematic Review of the Literature. Abstract: Abstract Purpose We present a systematic review of the literature concerning major aspects of science mapping to serve two primary purposes: First, to demonstrate the use of a science mapping approach to perform the review so that researchers may apply the procedure to the review of a scientific domain of their own interest, and second, to identify major areas of research activities concerning science mapping, intellectual milestones in the development of key specialties, evolutionary stages of major specialties involved, and the dynamics of transitions from one specialty to another. Design/methodology/approach We first introduce a theoretical framework of the evolution of a scientific specialty. Then we demonstrate a generic search strategy that can be used to construct a representative dataset of bibliographic records of a domain of research. Next, progressively synthesized co-citation networks are constructed and visualized to aid visual analytic studies of the domain\u2019s structural and dynamic patterns and trends. Finally, trajectories of citations made by particular types of authors and articles are presented to illustrate the predictive potential of the analytic approach. Findings The evolution of the science mapping research involves the development of a number of interrelated specialties. Four major specialties are discussed in detail in terms of four evolutionary stages: conceptualization, tool construction, application, and codification. Underlying connections between major specialties are also explored. The predictive analysis demonstrates citations trajectories of potentially transformative contributions. Research limitations The systematic review is primarily guided by citation patterns in the dataset retrieved from the literature. The scope of the data is limited by the source of the retrieval, i.e. the Web of Science, and the composite query used. An iterative query refinement is possible if one would like to improve the data quality, although the current approach serves our purpose adequately. More in-depth analyses of each specialty would be more revealing by incorporating additional methods such as citation context analysis and studies of other aspects of scholarly publications. Practical implications The underlying analytic process of science mapping serves many practical needs, notably bibliometric mapping, knowledge domain visualization, and visualization of scientific literature. In order to master such a complex process of science mapping, researchers often need to develop a diverse set of skills and knowledge that may span multiple disciplines. The approach demonstrated in this article provides a generic method for conducting a systematic review. Originality/value Incorporating the evolutionary stages of a specialty into the visual analytic study of a research domain is innovative. It provides a systematic methodology for researchers to achieve a good understanding of how scientific fields evolve, to recognize potentially insightful patterns from visually encoded signs, and to synthesize various information so as to capture the state of the art of the domain.\n    [7] Title: ComLittee: Literature Discovery with Personal Elected Author Committees. Abstract: In order to help scholars understand and follow a research topic, significant research has been devoted to creating systems that help scholars discover relevant papers and authors. Recent approaches have shown the usefulness of highlighting relevant authors while scholars engage in paper discovery. However, these systems do not capture and utilize users\u2019 evolving knowledge of authors. We reflect on the design space and introduce ComLittee, a literature discovery system that supports author-centric exploration. In contrast to paper-centric interaction in prior systems, ComLittee\u2019s author-centric interaction supports curating research threads from individual authors, finding new authors and papers using combined signals from a paper recommender and the curated authors\u2019 authorship graphs, and understanding them in the context of those signals. In a within-subjects experiment that compares to a paper-centric discovery system with author-highlighting, we demonstrate how ComLittee improves author and paper discovery.\n    [8] Title: A typology of research discovery tools. Abstract: There has been a proliferation of new research discovery tools that aid scientists in finding relevant publications. To obtain a general overview of this development, this article generates a conceptual typology of all possible research discovery tools by drawing from the information-theoretical concepts of redundancy/variety. Bibliometric links between scholarly publications can thus exhibit \u2018redundancy\u2019 (i.e. expectable linkages between academic works) or \u2018variety\u2019 (i.e. original co-occurrence patterns). On the redundancy-reproducing end of the typology are machines that harness extant co-citations or keyword queries, such as academic search engines and paper recommender systems. The variety end of the spectrum harbours services that enable categorial browsing or that suggest publications randomly, such as journals\u2019 tables of contents or random paper bots. The typology has implications for understanding how the design of research discovery platforms may ultimately shape aggregate citational networks of science.\n    [9] Title: Bridger: Toward Bursting Scientific Filter Bubbles and Boosting Innovation via Novel Author Discovery. Abstract: Scientific silos can hinder innovation. These information \u201cfilter bubbles\u201d and the growing challenge of information overload limit awareness across the literature, making it difficult to keep track of even narrow areas of interest, let alone discover new ones. Al-gorithmic curation and recommendation, which often prioritize relevance, can further reinforce these bubbles. In response, we describe Bridger, a system for facilitating discovery of scholars and their work, to explore design tradeoffs among relevant and novel recommendations. We construct a faceted representation of authors using information extracted from their papers and inferred personas. We explore approaches both for recommending new content and for displaying it in a manner that helps researchers to understand the work of authors who they are unfamiliar with. In studies with computer science researchers, our approach substantially improves users\u2019 abilities to do so. We develop an approach that locates commonalities and contrasts between scientists\u2014retrieving partially similar authors, rather than aiming for strict similarity. We find this approach helps users discover authors useful for generating novel research ideas of relevance to their work, at a higher rate than a state-of-art neural model. Our analysis reveals that Bridger connects authors who have different citation profiles, publish in different venues, and are more distant in social co-authorship networks, raising the prospect of bridging diverse communities and facilitating discovery.\n\n\n    Idea: Develop a **visual toolkit with graphical interface** designed **to review genetic algorithms in bioinformatics tools**. This toolkit will enable researchers to visually explore and compare different genetic algorithms in various bioinformatics applications such as gene therapy, microbial genome analysis, and biotechnology. The system will support **in-lab and interview studies**, allowing users to provide feedback and iteratively refine the tool based on hands-on experiences and expert insights.\n    Class: not novel\n    Review: The idea is not novel because similar visual tools for genetic algorithms already exist, such as IIID VGAS[0], EA Sandbox[1], and GATutor[2]. These tools also offer graphical interfaces for exploring genetic algorithms, making the proposed toolkit redundant.\n    Related Papers:\n    [0] Title: IIID VGAS: A visual genome analysis studio incorporating high performance next generation sequence analysis. Abstract: Bioinformaticians have gained extensive knowledge and accumulated diverse sets of skills in the use and understanding of a wide range of genome alignment and analysis tools. Many of these tools have been developed by computer scientists with minimal domain expertise, or by experts in the bioinformatics field lacking strong graphical user interface design skills. This project has been funded by a world class institute and aims to combine ideas from experts in the appropriate fields to develop a solution which combines cutting-edge genomic based algorithms with an enhanced visual user experience.\n    [1] Title: Evolutionary algorithm sandbox: A web-based graphical user interface for evolutionary algorithms. Abstract: The Evolutionary Algorithm (EA) Sandbox is an Adobe\u00ae Flex\u00ae-based graphical user interface (GUI) that provides a visual demonstration of evolutionary algorithm simulations. It allows the user to select EA parameters and algorithms (such as a basic genetic algorithm, biogeography-based optimization, and opposition-based learning), run a simulation, and view the results after each generation. The EA Sandbox is meant to be a learning tool and starting point for users, giving them the ability to examine how different parameters and algorithms perform for a number of common benchmark functions. The EA Sandbox can also be easily extended to incorporate more algorithms and problem functions.\n    [2] Title: GATutor: a graphical tutorial system for genetic algorithms. Abstract: In this paper we discuss the design and implementation of GATutor, a graphical tutorial system for genetic algorithms (GA). The X Window/Motif system provides powerful tools for the development of a user interfaces with a familiar feel and look. We implemented the Traveling Salesman Problem (TSP) and the Set Covering Problem (SCP) as two example GA problems in the tutorial. The TSP problem uses an order-based chromosome representation (permutation of n objects), while the SCP uses bit strings. The user has numerous buttons to select the GA parameters. These include (a) type of initial population: random or from a file, (b) mode: steady-state or generational, (c) population size, (d) maximum number of generations or trials, (e) generation gap, (f) selection mode, (g) selection bias, (h) selection of the crossover operation from a choice of several possibilities, (i) mutation method, (j) mutation rate, (k) replacement method, (l), elitism, etc. The user has the ability to do astep by step execution or to do a continuous run. The screen layout provides visual representation of the chromosomes in the population with the ability to scroll. This gives the user the option of varying one or two GA parameters to visually see the effect on the algorithm. One of most important features of this tutorial is the set of help screens that explain, with examples, all of the options for each of the GA parameters. This package has already been very useful for teaching the fundamental features of GAs in many different courses, and it has been very valuable in our GA research projects.\n    [3] Title: Unipro UGENE: a unified bioinformatics toolkit. Abstract: UNLABELLED\nUnipro UGENE is a multiplatform open-source software with the main goal of assisting molecular biologists without much expertise in bioinformatics to manage, analyze and visualize their data. UGENE integrates widely used bioinformatics tools within a common user interface. The toolkit supports multiple biological data formats and allows the retrieval of data from remote data sources. It provides visualization modules for biological objects such as annotated genome sequences, Next Generation Sequencing (NGS) assembly data, multiple sequence alignments, phylogenetic trees and 3D structures. Most of the integrated algorithms are tuned for maximum performance by the usage of multithreading and special processor instructions. UGENE includes a visual environment for creating reusable workflows that can be launched on local resources or in a High Performance Computing (HPC) environment. UGENE is written in C++ using the Qt framework. The built-in plugin system and structured UGENE API make it possible to extend the toolkit with new functionality.\n\n\nAVAILABILITY AND IMPLEMENTATION\nUGENE binaries are freely available for MS Windows, Linux and Mac OS X at http://ugene.unipro.ru/download.html. UGENE code is licensed under the GPLv2; the information about the code licensing and copyright of integrated tools can be found in the LICENSE.3rd_party file provided with the source bundle.\n    [4] Title: Visual Genomics Analysis Studio as a Tool to Analyze Multiomic Data. Abstract: Type B adverse drug reactions (ADRs) are iatrogenic immune-mediated syndromes with mechanistic etiologies that remain incompletely understood. Some of the most severe ADRs, including delayed drug hypersensitivity reactions, are T-cell mediated, restricted by specific human leukocyte antigen risk alleles and sometimes by public or oligoclonal T-cell receptors (TCRs), central to the immunopathogenesis of tissue-damaging response. However, the specific cellular signatures of effector, regulatory, and accessory immune populations that mediate disease, define reaction phenotype, and determine severity have not been defined. Recent development of single-cell platforms bringing together advances in genomics and immunology provides the tools to simultaneously examine the full transcriptome, TCRs, and surface protein markers of highly heterogeneous immune cell populations at the site of the pathological response at a single-cell level. However, the requirement for advanced bioinformatics expertise and computational hardware and software has often limited the ability of investigators with the understanding of diseases and biological models to exploit these new approaches. Here we describe the features and use of a state-of-the-art, fully integrated application for analysis and visualization of multiomic single-cell data called Visual Genomics Analysis Studio (VGAS). This unique user-friendly, Windows-based graphical user interface is specifically designed to enable investigators to interrogate their own data. While VGAS also includes tools for sequence alignment and identification of associations with host or organism genetic polymorphisms, in this review we focus on its application for analysis of single-cell TCR\u2013RNA\u2013Cellular Indexing of Transcriptomes and Epitopes by Sequencing (CITE)-seq, enabling holistic cellular characterization by unbiased transcriptome and select surface proteome. Critically, VGAS does not require user-directed coding or access to high-performance computers, instead incorporating performance-optimized hidden code to provide application-based fast and intuitive tools for data analyses and production of high-resolution publication-ready graphics on standard specification laptops. Specifically, it allows analyses of comprehensive single-cell TCR sequencing (scTCR-seq) data, detailing (i) functional pairings of \u03b1\u2013\u03b2 heterodimer TCRs, (ii) one-click histograms to display entropy and gene rearrangements, and (iii) Circos and Sankey plots to visualize clonality and dominance. For unbiased single-cell RNA sequencing (scRNA-seq) analyses, users extract cell transcriptome signatures according to global structure via principal component analysis, t-distributed stochastic neighborhood embedding, or uniform manifold approximation and projection plots, with overlay of scTCR-seq enabling identification and selection of the immunodominant TCR-expressing populations. Further integration with similar sequence-based detection of surface protein markers using oligo-labeled antibodies (CITE-seq) provides comparative understanding of surface protein expression, with differential gene or protein analyses visualized using volcano plot or heatmap functions. These data can be compared to reference cell atlases or suitable controls to reveal discrete disease-specific subsets, from epithelial to tissue-resident memory T-cells, and activation status, from senescence through exhaustion, with more finite transcript expression displayed as violin and box plots. Importantly, guided tutorial videos are available, as are regular application updates based on the latest advances in bioinformatics and user feedback.\n    [5] Title: Investigation of the capabilities of the method of characteristic patterns for graphical presentation of large amounts of information. Abstract: The author examines new challenges of ergonomics and occupational health, including unknown risks, issues of prevention, and ethics. The author also presents an overview of modern bioinformatics systems and visualization methods in bioinformatics. The researcher analyzed the health risks of human interaction with large volumes of textual information and advanced computational methods to prevent computer syndrome, including overstrain of the visual analyzer and pain in the back, neck, and hands. The study aims to analyze the representations of hereditary molecular genetic information in the form of graphic patterns available for visual perception, characterizing the initial data, and study the possibility of visualizing large amounts of data using the method of characteristic patterns. The author developed new methods of presenting large volumes of hereditary genetic information in bioinformatic systems. The basis of the method is information processing based on computer algorithms. The methods allow us to visually assess the differences in the genetic structure of various species of living organisms and identify the features of their nucleotide composition. The fixation of the internal ordering of the information signal in an individual graphical quasi-fractal structure is a characteristic feature of the methods considered. It makes it possible to expand the possibilities of visual-analytical thinking of a person when interacting with large amounts of information through bioinformatics tools.\n    [6] Title: Visual management of large scale data mining projects.. Abstract: This paper describes a unified framework for visualizing the preparations for, and results of, hundreds of machine learning experiments. These experiments were designed to improve the accuracy of enzyme functional predictions from sequence, and in many cases were successful. Our system provides graphical user interfaces for defining and exploring training datasets and various representational alternatives, for inspecting the hypotheses induced by various types of learning algorithms, for visualizing the global results, and for inspecting in detail results for specific training sets (functions) and examples (proteins). The visualization tools serve as a navigational aid through a large amount of sequence data and induced knowledge. They provided significant help in understanding both the significance and the underlying biological explanations of our successes and failures. Using these visualizations it was possible to efficiently identify weaknesses of the modular sequence representations and induction algorithms which suggest better learning strategies. The context in which our data mining visualization toolkit was developed was the problem of accurately predicting enzyme function from protein sequence data. Previous work demonstrated that approximately 6% of enzyme protein sequences are likely to be assigned incorrect functions on the basis of sequence similarity alone. In order to test the hypothesis that more detailed sequence analysis using machine learning techniques and modular domain representations could address many of these failures, we designed a series of more than 250 experiments using information-theoretic decision tree induction and naive Bayesian learning on local sequence domain representations of problematic enzyme function classes. In more than half of these cases, our methods were able to perfectly discriminate among various possible functions of similar sequences. We developed and tested our visualization techniques on this application.\n    [7] Title: Data Analysis and Visualization in Genomics and Proteomics. Abstract: Preface. List of Contributors. SECTION I: INTRODUCTION - DATA DIVERSITY AND INTEGRATION. 1. Integrative Data Analysis and Visualization: Introduction to Critical Problems, Goals and Challenges (Francisco Azuaje and Joaquin Dopazo). 1.1 Data Analysis and Visualization: An Integrative Approach. 1.2 Critical Design and Implementation Factors. 1.3 Overview of Contributions. References. 2. Biological Databases: Infrastructure, Content and Integration (Allyson L. Williams, Paul J. Kersey, Manuela Pruess and Rolf Apweiler). 2.1 Introduction. 2.2 Data Integration. 2.3 Review of Molecular Biology Databases. 2.4 Conclusion. References. 3. Data and Predictive Model Integration: an Overview of Key Concepts, Problems and Solutions (Francisco Azuaje, Joaquin Dopazo and Haiying Wang). 3.1 Integrative Data Analysis and Visualization: Motivation and Approaches. 3.2 Integrating Informational Views and Complexity for Understanding Function. 3.3 Integrating Data Analysis Techniques for Supporting Functional Analysis. 3.4 Final Remarks. References. SECTION II: INTEGRATIVE DATA MINING AND VISUALIZATION -EMPHASIS ON COMBINATION OF MULTIPLE DATA TYPES. 4. Applications of Text Mining in Molecular Biology, from Name Recognition to Protein Interaction Maps (Martin Krallinger and Alfonso Valencia). 4.1 Introduction. 4.2 Introduction to Text Mining and NLP. 4.3 Databases and Resources for Biomedical Text Mining. 4.4 Text Mining and Protein-Protein Interactions. 4.5 Other Text-Mining Applications in Genomics. 4.6 The Future of NLP in Biomedicine. Acknowledgements. References. 5. Protein Interaction Prediction by Integrating Genomic Features and Protein Interaction Network Analysis (Long J. Lu, Yu Xia, Haiyuan Yu, Alexander Rives, Haoxin Lu, Falk Schubert and Mark Gerstein). 5.1 Introduction. 5.2 Genomic Features in Protein Interaction Predictions. 5.3 Machine Learning on Protein-Protein Interactions. 5.4 The Missing Value Problem. 5.5 Network Analysis of Protein Interactions. 5.6 Discussion. References. 6. Integration of Genomic and Phenotypic Data (Amanda Clare). 6.1 Phenotype. 6.2 Forward Genetics and QTL Analysis. 6.3 Reverse Genetics. 6.4 Prediction of Phenotype from Other Sources of Data. 6.5 Integrating Phenotype Data with Systems Biology. 6.6 Integration of Phenotype Data in Databases. 6.7 Conclusions. References. 7. Ontologies and Functional Genomics (Fatima Al-Shahrour and Joaquin Dopazo). 7.1 Information Mining in Genome-Wide Functional Analysis. 7.2 Sources of Information: Free Text Versus Curated Repositories. 7.3 Bio-Ontologies and the Gene Ontology in Functional Genomics. 7.4 Using GO to Translate the Results of Functional Genomic Experiments into Biological Knowledge. 7.5 Statistical Approaches to Test Significant Biological Differences. 7.6 Using FatiGO to Find Significant Functional Associations in Clusters of Genes. 7.7 Other Tools. 7.8 Examples of Functional Analysis of Clusters of Genes. 7.9 Future Prospects. References. 8. The C. elegans Interactome: its Generation and Visualization (Alban Chesnau and Claude Sardet). 8.1 Introduction. 8.2 The ORFeome: the first step toward the interactome of C. elegans. 8.3 Large-Scale High-Throughput Yeast Two-Hybrid Screens to Map the C. elegans Protein-Protein Interaction (Interactome) Network: Technical Aspects. 8.4 Visualization and Topology of Protein-Protein Interaction Networks. 8.5 Cross-Talk Between the C. elegans Interactome and other Large-Scale Genomics and Post-Genomics Data Sets. 8.6 Conclusion: From Interactions to Therapies. References. SECTION III: INTEGRATIVE DATA MINING AND VISUALIZATION - EMPHASIS ON COMBINATION OF MULTIPLE PREDICTION MODELS AND METHODS. 9. Integrated Approaches for Bioinformatic Data Analysis and Visualization - Challenges, Opportunities and New Solutions (Steve R. Pettifer, James R. Sinnott and Teresa K. Attwood). 9.1 Introduction. 9.2 Sequence Analysis Methods and Databases. 9.3 A View Through a Portal. 9.4 Problems with Monolithic Approaches: One Size Does Not Fit All. 9.5 A Toolkit View. 9.6 Challenges and Opportunities. 9.7 Extending the Desktop Metaphor. 9.8 Conclusions. Acknowledgements. References. 10. Advances in Cluster Analysis of Microarray Data (Qizheng Sheng, Yves Moreau, Frank De Smet, Kathleen Marchal and Bart De Moor). 10.1 Introduction. 10.2 Some Preliminaries. 10.3 Hierarchical Clustering. 10.4 k-Means Clustering. 10.5 Self-Organizing Maps. 10.6 A Wish List for Clustering Algorithms. 10.7 The Self-Organizing Tree Algorithm. 10.8 Quality-Based Clustering Algorithms. 10.9 Mixture Models. 10.10 Biclustering Algorithms. 10.11 Assessing Cluster Quality. 10.12 Open Horizons. References. 11. Unsupervised Machine Learning to Support Functional Characterization of Genes: Emphasis on Cluster Description and Class Discovery (Olga G. Troyanskaya). 11.1 Functional Genomics: Goals and Data Sources. 11.2 Functional Annotation by Unsupervised Analysis of Gene Expression Microarray Data. 11.3 Integration of Diverse Functional Data For Accurate Gene Function Prediction. 11.4 MAGIC - General Probabilistic Integration of Diverse Genomic Data. 11.5 Conclusion. References. 12. Supervised Methods with Genomic Data: a Review and Cautionary View (Ramon Diaz-Uriarte). 12.1 Chapter Objectives. 12.2 Class Prediction and Class Comparison. 12.3 Class Comparison: Finding/Ranking Differentially Expressed Genes. 12.4 Class Prediction and Prognostic Prediction. 12.5 ROC Curves for Evaluating Predictors and Differential Expression. 12.6 Caveats and Admonitions. 12.7 Final Note: Source Code Should be Available. Acknowledgements. References. 13. A Guide to the Literature on Inferring Genetic Networks by Probabilistic Graphical Models (Pedro Larranaga, Inaki Inza and Jose L. Flores). 13.1 Introduction. 13.2 Genetic Networks. 13.3 Probabilistic Graphical Models. 13.4 Inferring Genetic Networks by Means of Probabilistic Graphical Models. 13.5 Conclusions. Acknowledgements. References. 14. Integrative Models for the Prediction and Understanding of Protein Structure Patterns (Inge Jonassen). 14.1 Introduction. 14.2 Structure Prediction. 14.3 Classifications of Structures. 14.4 Comparing Protein Structures 14.5 Methods for the Discovery of Structure Motifs. 14.6 Discussion and Conclusions. References. Index.\n    [8] Title: OmicPioneer-sc: an integrated, interactive visualization environment for single-cell sequencing data. Abstract: OmicPioneer-sc is an open-source data visualization/analysis package that integrates dimensionality-reduction plots (DRPs) such as t-SNE and UMAP with Next-Generation Clustered Heat Maps (NGCHMs) and Pathway Visualization Modules (PVMs) in a seamless, highly interactive exploratory environment. It includes fluent zooming and navigation, a statistical toolkit, dozens of link-outs to external public bioinformatic resources, high-resolution graphics that meet the requirements of all major journals, and the ability to store all metadata needed to reproduce the visualizations at a later time. A user-friendly, multi-panel graphical interface enables non-informaticians to interact with the system without programming, asking and answering questions that require navigation among the three types of modules or extension from them to the Gene Ontology or information on therapies. The visual integration can be useful for detective work to identify and annotate cell-types for color-coding of the DRPs, and multiple NGCHMs can be layered on top of each other (with toggling among them) as an aid to multi-omic analysis. The tools are available in containerized form with APIs to facilitate incorporation as a plug-in to other bioinformatic environments. The capabilities of OmicPioneer-sc are illustrated here through application to a single-cell RNA-seq airway dataset pertinent to the biology of both cancer and COVID-19. [Supplemental material is available for this article.]\n    [9] Title: The bioinformatics resource for oral pathogens. Abstract: Complete genomic sequences of several oral pathogens have been deciphered and multiple sources of independently annotated data are available for the same genomes. Different gene identification schemes and functional annotation methods used in these databases present a challenge for cross-referencing and the efficient use of the data. The Bioinformatics Resource for Oral Pathogens (BROP) aims to integrate bioinformatics data from multiple sources for easy comparison, analysis and data-mining through specially designed software interfaces. Currently, databases and tools provided by BROP include: (i) a graphical genome viewer (Genome Viewer) that allows side-by-side visual comparison of independently annotated datasets for the same genome; (ii) a pipeline of automatic data-mining algorithms to keep the genome annotation always up-to-date; (iii) comparative genomic tools such as Genome-wide ORF Alignment (GOAL); and (iv) the Oral Pathogen Microarray Database. BROP can also handle unfinished genomic sequences and provides secure yet flexible control over data access. The concept of providing an integrated source of genomic data, as well as the data-mining model used in BROP can be applied to other organisms. BROP can be publicly accessed at .\n\n\n    Idea: Develop a **systematic review-based framework** designed **to align llm evaluation with human preferences**, ensuring that evaluation criteria are continuously refined based on comprehensive reviews of user feedback and emerging model behaviors. This framework will utilize **content analysis of user interactions and feedback** to identify patterns and areas of improvement. The effectiveness of this framework will be assessed through a **qualitative study** involving iterative cycles of user feedback and criteria refinement.\n    Class: not novel\n    Review: The idea is not novel because it closely resembles existing frameworks like EvalLM[0] and HumanELY[1], which already align LLM evaluations with human preferences using user-defined criteria and human feedback.\n    Related Papers:\n    [0] Title: EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria. Abstract: By simply composing prompts, developers can prototype novel generative applications with Large Language Models (LLMs). To refine prototypes into products, however, developers must iteratively revise prompts by evaluating outputs to diagnose weaknesses. Formative interviews (N=8) revealed that developers invest significant effort in manually evaluating outputs as they assess context-specific and subjective criteria. We present EvalLM, an interactive system for iteratively refining prompts by evaluating multiple outputs on user-defined criteria. By describing criteria in natural language, users can employ the system\u2019s LLM-based evaluator to get an overview of where prompts excel or fail, and improve these based on the evaluator\u2019s feedback. A comparative study (N=12) showed that EvalLM, when compared to manual evaluation, helped participants compose more diverse criteria, examine twice as many outputs, and reach satisfactory prompts with 59% fewer revisions. Beyond prompts, our work can be extended to augment model evaluation and alignment in specific application contexts.\n    [1] Title: HUMANELY: HUMAN EVALUATION OF LLM YIELD, USING A NOVEL WEB BASED EVALUATION TOOL. Abstract: Large language models (LLMs) have caught the imagination of researchers,developers and public in general the world over with their potential for transformation. Vast amounts of research and development resources are being provided to implement these models in all facets of life. Trained using billions of parameters, various measures of their accuracy and performance have been proposed and used in recent times. While many of the automated natural language assessment parameters measure LLM output performance for use of language, contextual outputs are still hard to measure and quantify. Hence, human evaluation is still an important measure of LLM performance,even though it has been applied variably and inconsistently due to lack of guidance and resource limitations. To provide a structured way to perform comprehensive human evaluation of LLM output, we propose the first guidance and tool called HumanELY. Our approach and tool built using prior knowledge helps perform evaluation of LLM outputs in a comprehensive, consistent, measurable and comparable manner. HumanELY comprises of five key evaluation metrics: relevance, coverage, coherence, harm and comparison. Additional submetrics within these five key metrics provide for Likert scale based human evaluation of LLM outputs. Our related webtool uses this HumanELY guidance to enable LLM evaluation and provide data for comparison against different users performing human evaluation. While all metrics may not be relevant and pertinent to all outputs, it is important to assess and address their use. Lastly, we demonstrate comparison of metrics used in HumanELY against some of the recent publications in the healthcare domain. We focused on the healthcare domain due to the need to demonstrate highest levels of accuracy and lowest levels of harm in a comprehensive manner. We anticipate our guidance and tool to be used for any domain where LLMs find an use case.\n    [2] Title: Evaluation of Code Generation for Simulating Participant Behavior in Experience Sampling Method by Iterative In-Context Learning of a Large Language Model. Abstract: The Experience Sampling Method (ESM) is commonly used to understand behaviors, thoughts, and feelings in the wild by collecting self-reports. Sustaining sufficient response rates, especially in long-running studies remains challenging. To avoid low response rates and dropouts, experimenters rely on their experience, proposed methodologies from earlier studies, trial and error, or the scarcely available participant behavior data from previous ESM protocols. This approach often fails in finding the acceptable study parameters, resulting in redesigning the protocol and repeating the experiment. Research has shown the potential of machine learning to personalize ESM protocols such that ESM prompts are delivered at opportune moments, leading to higher response rates. The corresponding training process is hindered due to the scarcity of open data in the ESM domain, causing a cold start, which could be mitigated by simulating participant behavior. Such simulations provide training data and insights for the experimenters to update their study design choices. Creating this simulation requires behavioral science, psychology, and programming expertise. Large language models (LLMs) have emerged as facilitators for information inquiry and programming, albeit random and occasionally unreliable. We aspire to assess the readiness of LLMs in an ESM use case. We conducted research using GPT-3.5 turbo-16k to tackle an ESM simulation problem. We explored several prompt design alternatives to generate ESM simulation programs, evaluated the output code in terms of semantics and syntax, and interviewed ESM practitioners. We found that engineering LLM-enabled ESM simulations have the potential to facilitate data generation, but they perpetuate trust and reliability challenges.\n    [3] Title: Human-Centered Evaluation and Auditing of Language Models. Abstract: The recent advancements in Large Language Models (LLMs) have significantly impacted numerous, and will impact more, real-world applications. However, these models also pose significant risks to individuals and society. To mitigate these issues and guide future model development, responsible evaluation and auditing of LLMs are essential. This workshop aims to address the current \u201cevaluation crisis\u201d in LLM research and practice by bringing together HCI and AI researchers and practitioners to rethink LLM evaluation and auditing from a human-centered perspective. The workshop will explore topics around understanding stakeholders\u2019 needs and goals with evaluation and auditing LLMs, establishing human-centered evaluation and auditing methods, developing tools and resources to support these methods, building community and fostering collaboration. By soliciting papers, organizing invited keynote and panel, and facilitating group discussions, this workshop aims to develop a future research agenda for addressing the challenges in LLM evaluation and auditing.\n    [4] Title: Aligning Model Evaluations with Human Preferences: Mitigating Token Count Bias in Language Model Assessments. Abstract: The SLAM paper demonstrated that on-device Small Language Models (SLMs) are a viable and cost-effective alternative to API-based Large Language Models (LLMs), such as OpenAI's GPT-4, offering comparable performance and stability. However, SLAM also identified discrepancies between human preferences and traditional auto-evaluators. This follow-up paper explores methods to align LLM evaluator preferences with human evaluations by addressing biases, particularly toward higher token counts. We employed Bayesian statistics and a t-test to quantify this bias and developed a recalibration procedure to adjust the GPTScorer. Our findings significantly improve aligning the recalibrated LLM evaluator with human evaluations across multiple use cases. For instance, spearman's ranking correlation score in the Recommendation use case improved from -27.27 to 44.55. These results highlight the importance of accounting for biases in automated evaluations to ensure fair and accurate model assessments. The recalibration process enhances the reliability of automated evaluators, leading to better AI models that align with human values and expectations. This study provides a robust methodology for future research into bias correction and emphasizes the feasibility and benefits of developing human-aligned AI evaluation systems.\n    [5] Title: Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences. Abstract: Due to the cumbersome nature of human evaluation and limitations of code-based evaluation, Large Language Models (LLMs) are increasingly being used to assist humans in evaluating LLM outputs. Yet LLM-generated evaluators simply inherit all the problems of the LLMs they evaluate, requiring further human validation. We present a mixed-initiative approach to ``validate the validators'' -- aligning LLM-generated evaluation functions (be it prompts or code) with human requirements. Our interface, EvalGen, provides automated assistance to users in generating evaluation criteria and implementing assertions. While generating candidate implementations (Python functions, LLM grader prompts), EvalGen asks humans to grade a subset of LLM outputs; this feedback is used to select implementations that better align with user grades. A qualitative study finds overall support for EvalGen but underscores the subjectivity and iterative process of alignment. In particular, we identify a phenomenon we dub \\emph{criteria drift}: users need criteria to grade outputs, but grading outputs helps users define criteria. What is more, some criteria appears \\emph{dependent} on the specific LLM outputs observed (rather than independent criteria that can be defined \\emph{a priori}), raising serious questions for approaches that assume the independence of evaluation from observation of model outputs. We present our interface and implementation details, a comparison of our algorithm with a baseline approach, and implications for the design of future LLM evaluation assistants.\n    [6] Title: Human-Centered Design Recommendations for LLM-as-a-judge. Abstract: Traditional reference-based metrics, such as BLEU and ROUGE, are less effective for assessing outputs from Large Language Models (LLMs) that produce highly creative or superior-quality text, or in situations where reference outputs are unavailable. While human evaluation remains an option, it is costly and difficult to scale. Recent work using LLMs as evaluators (LLM-as-a-judge) is promising, but trust and reliability remain a significant concern. Integrating human input is crucial to ensure criteria used to evaluate are aligned with the human\u2019s intent, and evaluations are robust and consistent. This paper presents a user study of a design exploration called EvaluLLM, that enables users to leverage LLMs as customizable judges, promoting human involvement to balance trust and cost-saving potential with caution. Through interviews with eight domain experts, we identified the need for assistance in developing effective evaluation criteria aligning the LLM-as-a-judge with practitioners\u2019 preferences and expectations. We offer findings and design recommendations to optimize human-assisted LLM-as-judge systems.\n    [7] Title: CheckEval: Robust Evaluation Framework using Large Language Model via Checklist. Abstract: We introduce CheckEval, a novel evaluation framework using Large Language Models, addressing the challenges of ambiguity and inconsistency in current evaluation methods. CheckEval addresses these challenges by dividing evaluation criteria into detailed sub-aspects and constructing a checklist of Boolean questions for each, simplifying the evaluation. This approach not only renders the process more interpretable but also significantly enhances the robustness and reliability of results by focusing on specific evaluation dimensions. Validated through a focused case study using the SummEval benchmark, CheckEval indicates a strong correlation with human judgments. Furthermore, it demonstrates a highly consistent Inter-Annotator Agreement. These findings highlight the effectiveness of CheckEval for objective, flexible, and precise evaluations. By offering a customizable and interactive framework, CheckEval sets a new standard for the use of LLMs in evaluation, responding to the evolving needs of the field and establishing a clear method for future LLM-based evaluation.\n    [8] Title: Discovering Language Model Behaviors with Model-Written Evaluations. Abstract: As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer (\"sycophancy\") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors.\n    [9] Title: Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models. Abstract: Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment. Additionally, they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. To address these issues, we introduce Prometheus 2, a more powerful evaluator LM than its predecessor that closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, Prometheus 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Our models, code, and data are all publicly available at https://github.com/prometheus-eval/prometheus-eval.\n\n\n    Idea: Develop a system **to support prompt engineering and hypothesis testing** by integrating an **error-mediated integration algorithm** that helps users identify and correct faulty prompts in real-time. This system would allow users to see immediate feedback on their prompt modifications, filtering out responses generated by error-prone prompts and highlighting the valid and effective ones. The system would be evaluated through a **user study with programmers** to assess its impact on improving prompt crafting efficiency and accuracy, ensuring that the tool is intuitive and beneficial for both novice and experienced users.\n    Class: not novel\n    Review: The idea is not novel because the error-mediated integration algorithm is already implemented in ChainForge[0].\n    Related Papers:\n    [0] Title: ChainForge: An open-source visual programming environment for prompt engineering. Abstract: Prompt engineering for large language models (LLMs) is a critical to effectively leverage their capabilities. However, due to the inherent stochastic and opaque nature of LLMs, prompt engineering is far from an exact science. Crafting prompts that elicit the desired responses still requires a lot of trial and error to gain a nuanced understanding of a model\u2019s strengths and limitations for one\u2019s specific task context and target application. To support users in sensemaking around the outputs of LLMs, we create ChainForge, an open-source visual programming environment for prompt engineering. ChainForge is publicly available, both on the web (https://chainforge.ai) and as a locally installable Python package hosted on PyPI. We detail some features of ChainForge and how we iterated the design in response to internal and external feedback.\n    [1] Title: Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt Editing. Abstract: Large language models (LLMs) have made impressive progress in natural language processing. These models rely on proper human instructions (or prompts) to generate suitable responses. However, the potential of LLMs are not fully harnessed by commonly-used prompting methods: many human-in-the-loop algorithms employ ad-hoc procedures for prompt selection; while auto prompt generation approaches are essentially searching all possible prompts randomly and inefficiently. We propose Evoke, an automatic prompt refinement framework. In Evoke, there are two instances of a same LLM: one as a reviewer (LLM-Reviewer), it scores the current prompt; the other as an author (LLM-Author), it edits the prompt by considering the edit history and the reviewer's feedback. Such an author-reviewer feedback loop ensures that the prompt is refined in each iteration. We further aggregate a data selection approach to Evoke, where only the hard samples are exposed to the LLM. The hard samples are more important because the LLM can develop deeper understanding of the tasks out of them, while the model may already know how to solve the easier cases. Experimental results show that Evoke significantly outperforms existing methods. For instance, in the challenging task of logical fallacy detection, Evoke scores above 80, while all other baseline methods struggle to reach 20.\n    [2] Title: ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing. Abstract: Evaluating outputs of large language models (LLMs) is challenging, requiring making\u2014and making sense of\u2014many responses. Yet tools that go beyond basic prompting tend to require knowledge of programming APIs, focus on narrow domains, or are closed-source. We present ChainForge, an open-source visual toolkit for prompt engineering and on-demand hypothesis testing of text generation LLMs. ChainForge provides a graphical interface for comparison of responses across models and prompt variations. Our system was designed to support three tasks: model selection, prompt template design, and hypothesis testing (e.g., auditing). We released ChainForge early in its development and iterated on its design with academics and online users. Through in-lab and interview studies, we find that a range of people could use ChainForge to investigate hypotheses that matter to them, including in real-world settings. We identify three modes of prompt engineering and LLM hypothesis testing: opportunistic exploration, limited evaluation, and iterative refinement.\n    [3] Title: Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review. Abstract: This paper delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models (LLMs). Prompt engineering is the process of structuring input text for LLMs and is a technique integral to optimizing the efficacy of LLMs. This survey elucidates foundational principles of prompt engineering, such as role-prompting, one-shot, and few-shot prompting, as well as more advanced methodologies such as the chain-of-thought and tree-of-thoughts prompting. The paper sheds light on how external assistance in the form of plugins can assist in this task, and reduce machine hallucination by retrieving external knowledge. We subsequently delineate prospective directions in prompt engineering research, emphasizing the need for a deeper understanding of structures and the role of agents in Artificial Intelligence-Generated Content (AIGC) tools. We discuss how to assess the efficacy of prompt methods from different perspectives and using different methods. Finally, we gather information about the application of prompt engineering in such fields as education and programming, showing its transformative potential. This comprehensive survey aims to serve as a friendly guide for anyone venturing through the big world of LLMs and prompt engineering.\n    [4] Title: Smart Prompt Advisor: Multi-Objective Prompt Framework for Consistency and Best Practices. Abstract: Recent breakthroughs in Large Language Models (LLM), comprised of billions of parameters, have achieved the ability to unveil exceptional insight into a wide range of Natural Language Processing (NLP) tasks. The onus of the performance of these models lies in the sophistication and completeness of the input prompt. Minimizing the enhancement cycles of prompt with improvised keywords becomes critically important as it directly affects the time to market and cost of the developing solution. However, this process inevitably has a trade-off between the learning curve/proficiency of the user and completeness of the prompt, as generating such a solutions is an incremental process. In this paper, we have designed a novel solution and implemented it in the form of a plugin for Visual Studio Code IDE, which can optimize this trade-off, by learning the underlying prompt intent to enhance with keywords. This will tend to align with developers' collection of semantics while developing a secure code, ensuring parameter and local variable names, return expressions, simple pre and post-conditions. and basic control and data flow are met.\n    [5] Title: Prompt Engineering a Prompt Engineer. Abstract: Prompt engineering is a challenging yet crucial task for optimizing the performance of large language models on customized tasks. It requires complex reasoning to examine the model's errors, hypothesize what is missing or misleading in the current prompt, and communicate the task with clarity. While recent works indicate that large language models can be meta-prompted to perform automatic prompt engineering, we argue that their potential is limited due to insufficient guidance for complex reasoning in the meta-prompt. We fill this gap by infusing into the meta-prompt three key components: detailed descriptions, context specification, and a step-by-step reasoning template. The resulting method, named PE2, exhibits remarkable versatility across diverse language tasks. It finds prompts that outperform\"let's think step by step\"by 6.3% on MultiArith and 3.1% on GSM8K, and outperforms competitive baselines on counterfactual tasks by 6.9%. Further, we show that PE2 can make targeted and highly specific prompt edits, rectify erroneous prompts, and induce multi-step plans for complex tasks.\n    [6] Title: EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria. Abstract: By simply composing prompts, developers can prototype novel generative applications with Large Language Models (LLMs). To refine prototypes into products, however, developers must iteratively revise prompts by evaluating outputs to diagnose weaknesses. Formative interviews (N=8) revealed that developers invest significant effort in manually evaluating outputs as they assess context-specific and subjective criteria. We present EvalLM, an interactive system for iteratively refining prompts by evaluating multiple outputs on user-defined criteria. By describing criteria in natural language, users can employ the system\u2019s LLM-based evaluator to get an overview of where prompts excel or fail, and improve these based on the evaluator\u2019s feedback. A comparative study (N=12) showed that EvalLM, when compared to manual evaluation, helped participants compose more diverse criteria, examine twice as many outputs, and reach satisfactory prompts with 59% fewer revisions. Beyond prompts, our work can be extended to augment model evaluation and alignment in specific application contexts.\n    [7] Title: On-line help: an aid to effective use of software. Abstract: Abstract The main thrust of this study is to investigate how users of mass-market word-processing software go about using the built-in help systems provided in application packages as they work: how frequently they seek help with the functions they need, how they approach and access this help, how they perceive it, and in what ways their performance on tasks is improved as a result. The study concludes by suggesting how users\u0092 confidence, effectiveness and approach to using on-line help systems might be improved, as well as recommending further investigations. The help facilities provided in computer software can take on varying forms: hints, help and coaching. To begin, I take hints to be small pieces of advice offered generally, at no particular time, but possibly ahead of the performance of the activity to which they relate. So the kind of information counted as hints would include what Word refers to as \u0091tool tips\u0092\u0097short messages that appear when the mouse pointer rests on a visible item, such as a button or an item on a window; as well as the \u0091tip of the day\u0092 which is displayed when the program starts up. As well as these explicit hints, there are also implicit ones, such as those embodied in the names of menu items (if any one doubts their effect, let them try using a version of a program intended for speakers of a different language, where the menu names and the names of menu items can be of very little help. For example, under the French \u0091Fichier\u0092 menu, there might be the following items: Nouveau, Ouvrir\u0085, Ouvrir l\u0092element recent, Fermer, Enregistrer. Enregistrer sous, Tout enregister, Format d\u0092impression, Imprimer\u0085 ). Even more abstractly, it could be claimed that the whole \u0091look and feel\u0092 of a program continually prompts the user with hints. Help is the general focus of the bulk of this thesis, taking in any feature that is meant to respond to a user\u0092s request for assistance. I need discuss this here no further as it will be comprehensively covered later. Finally, coaching can be defined as any activity that is intended to improve performance or to correct errors or bad habits. In contrast to help, it is often prompted by the user\u0092s behaviour (\u0093It looks like you\u0092re writing a letter. \u0085\u0094), or by the error itself. The coloured underlining done by the spelling and grammar checkers could also be called coaching. And the most obvious coaching lessons are those provided by the various \u0091wizards\u0092 that can be invoked via menu items to guide the user through complex procedures, such as executing a mail merge. The literature that was surveyed ranges over a number of areas of relevance, several of which were explored in some detail, such as: references to help systems specifically, the way that beginners typically fail to progress beyond a modest level of proficiency, the human\u0096computer interface (HCI), the efficacy of help systems and the factors influencing how users approach and value them, the categories of complaints frequently made about help, computer-mediated learning, the heuristics of usability testing and what evaluative approaches are possible. The literature provided valuable background for formulating and implementing an experimental program and in evaluating and analyzing its outcomes. In particular, it was realized at an early stage that several techniques commonly employed in usability testing were beyond the financial and staffing resources available; for example video recording of users\u0092 interactions with software, or monitoring of these interactions by expert, trained observers and evaluators. An alternative approach was developed, which drew on the resources of the student subjects, who worked in pairs so that one member of the pair tackled the set tasks while the other observed and took records on pro-formas devised for this study. This approach, using pairs (or \u0091dyads\u0092) turned out to be successful. Since an action-research paradigm was adopted, in which a process of evaluation and modification was applied to each successive experiment, it was possible to progressively refine the approach with respect to both the exercises given to the participants and the procedures and pro-formas used to monitor and record the results. An article on the approach was written for the British Journal of Educational Technology, and this is included as an appendix to this thesis. The experimental subjects (those who contributed to surveys, plus a total of 176 who participated in the initial focus groups and the task-based experiments) were recruited from the students, mainly in first year, of the School of Information Technology and Electrical Engineering at the University of Queensland. The assumption was made that this sample would not differ substantially from the general population of those who use word processors for a variety of purposes but who are not professional clerical workers. The word processor with which almost all of them were already familiar was Microsoft Word, and this was adopted as the vehicle for the experimental program After an initial phase in which students\u0092 attitudes and practices were explored to provide a context, using focus groups and an on-line survey, a series of experiments was designed and conducted, using an action research paradigm to refine the techniques and materials used as the program proceeded. A set of tasks was devised for each experiment, ranging from the commonplace (such as changing font and text size) through the more demanding (such as manipulating tables) to the less routine (such as insertion of graphics in the text and as background). The tasks were selected so that they would require most students to seek help at some point, while not being exceptionally challenging. Interesting and potentially valuable results have come out of these experiments: many of the students had been using the software for a number of years, starting at school, and were currently word-processing for several hours a week, but surprisingly they were typically using just a minimal sub-set of the capabilities of the software, given that they could satisfy most of their writing needs with them. There appeared to be a common tendency to avoid seeking help, since when the set tasks called for more advanced techniques a substantial proportion of users preferred to browse the menus, or use trial-and-error, rather than take full advantage of the help system. However these students, when forced into it by the terms of the experiments, were generally able to find and use the help that they needed to perform well in the more demanding tasks. This study suggests that many users are coping reasonably well with their day-to-day word processing but might save time and effort in the longer term (and produce more rewarding outcomes) if they could only take advantage of additional features of the software. One factor hindering them is their reluctance to use help, whether this arises from unfamiliarity, lack of confidence, unfortunate earlier experiences, sheer prejudice, or simple inertia. As well as the potential improvements in users\u0092 facility with the packages that might be brought about by training, which was once general but has recently been largely discarded, there are ways of making help more acceptable and thus more immediately usable for these people: help systems could be made more palatable and at the same time more educative, without being patronizing or paternalistic, and this depends in part on achieving a closer alignment between the mental models of the software held by various disparate groups of users and those used by its designers. In summary, and on careful inspection, Microsoft Word is revealed as a comprehensively accomplished application that can appear daunting at first. It is a pity that, according to this study, there are many users who never progress beyond the unambitious use of a modest set of its capabilities. This thesis concludes by suggesting that barriers to the full exploitation of the powers of Word and similar software packages may be overcome by a few simple strategies, including simplifying the already plentiful repertoire of customisation tools and devising training aimed specifically at enriching and smoothing the beginners\u0092 learning curve.\n    [8] Title: ConstitutionMaker: Interactively Critiquing Large Language Models by Converting Feedback into Principles. Abstract: Large language model (LLM) prompting is a promising new approach for users to create and customize their own chatbots. However, current methods for steering a chatbot\u2019s outputs, such as prompt engineering and fine-tuning, do not support users in converting their natural feedback on the model\u2019s outputs to changes in the prompt or model. In this work, we explore how to enable users to interactively refine model outputs through their feedback, by helping them convert their feedback into a set of principles (i.e. a constitution) that dictate the model\u2019s behavior. From a formative study, we (1) found that users needed support converting their feedback into principles for the chatbot and (2) classified the different principle types desired by users. Inspired by these findings, we developed ConstitutionMaker, an interactive tool for converting user feedback into principles, to steer LLM-based chatbots. With ConstitutionMaker, users can provide either positive or negative feedback in natural language, select auto-generated feedback, or rewrite the chatbot\u2019s response; each mode of feedback automatically generates a principle that is inserted into the chatbot\u2019s prompt. In a user study with 14 participants, we compare ConstitutionMaker to an ablated version, where users write their own principles. With ConstitutionMaker, participants felt that their principles could better guide the chatbot, that they could more easily convert their feedback into principles, and that they could write principles more efficiently, with less mental demand. ConstitutionMaker helped users identify ways to improve the chatbot, formulate their intuitive responses to the model into feedback, and convert this feedback into specific and clear principles. Together, these findings inform future tools that support the interactive critiquing of LLM outputs.\n    [9] Title: Prompt Engineering or Fine Tuning: An Empirical Assessment of Large Language Models in Automated Software Engineering Tasks. Abstract: In this paper, we investigate the effectiveness of state-of-the-art LLM, i.e., GPT-4, with three different prompting engineering techniques (i.e., basic prompting, in-context learning, and task-specific prompting) against 18 fine-tuned LLMs on three typical ASE tasks, i.e., code generation, code summarization, and code translation. Our quantitative analysis of these prompting strategies suggests that prompt engineering GPT-4 cannot necessarily and significantly outperform fine-tuning smaller/older LLMs in all three tasks. For comment generation, GPT-4 with the best prompting strategy (i.e., task-specific prompt) had outperformed the first-ranked fine-tuned model by 8.33% points on average in BLEU. However, for code generation, the first-ranked fine-tuned model outperforms GPT-4 with best prompting by 16.61% and 28.3% points, on average in BLEU. For code translation, GPT-4 and fine-tuned baselines tie as they outperform each other on different translation tasks. To explore the impact of different prompting strategies, we conducted a user study with 27 graduate students and 10 industry practitioners. From our qualitative analysis, we find that the GPT-4 with conversational prompts (i.e., when a human provides feedback and instructions back and forth with a model to achieve best results) showed drastic improvement compared to GPT-4 with automatic prompting strategies. Moreover, we observe that participants tend to request improvements, add more context, or give specific instructions as conversational prompts, which goes beyond typical and generic prompting strategies. Our study suggests that, at its current state, GPT-4 with conversational prompting has great potential for ASE tasks, but fully automated prompt engineering with no human in the loop requires more study and improvement.\n\n\n    Idea: Develop a scholarly paper recommendation system that uses **contextualized text descriptions** to **personalize paper recommendations**. By analyzing a researcher's prior publications and saved papers, the system generates detailed contextual descriptions for each recommended paper, comparing them to the user's body of work. This approach not only highlights the relevance but also provides a deeper understanding of how each recommendation aligns with the user's research interests. The effectiveness of this system can be assessed through a **user study**, ensuring that researchers find the recommended papers more relevant and easier to triage.\n    Class: not novel\n    Review: The idea is not novel because it closely mirrors the approach of PaperWeaver[0], which also uses user-collected papers to generate contextualized descriptions for recommendations. Both systems aim to enhance relevance and understanding of recommended papers through user-specific contexts.\n    Related Papers:\n    [0] Title: PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers. Abstract: With the rapid growth of scholarly archives, researchers subscribe to \u201cpaper alert\u2019\u2019 systems that periodically provide them with recommendations of recently published papers that are similar to previously collected papers. However, researchers sometimes struggle to make sense of nuanced connections between recommended papers and their own research context, as existing systems only present paper titles and abstracts. To help researchers spot these connections, we present PaperWeaver, an enriched paper alerts system that provides contextualized text descriptions of recommended papers based on user-collected papers. PaperWeaver employs a computational method based on Large Language Models (LLMs) to infer users\u2019 research interests from their collected papers, extract context-specific aspects of papers, and compare recommended and collected papers on these aspects. Our user study (N=15) showed that participants using PaperWeaver were able to better understand the relevance of recommended papers and triage them more confidently when compared to a baseline that presented the related work sections from recommended papers.\n    [1] Title: CiteSee: Augmenting Citations in Scientific Papers with Persistent and Personalized Historical Context. Abstract: When reading a scholarly article, inline citations help researchers contextualize the current article and discover relevant prior work. However, it can be challenging to prioritize and make sense of the hundreds of citations encountered during literature reviews. This paper introduces CiteSee, a paper reading tool that leverages a user\u2019s publishing, reading, and saving activities to provide personalized visual augmentations and context around citations. First, CiteSee connects the current paper to familiar contexts by surfacing known citations a user had cited or opened. Second, CiteSee helps users prioritize their exploration by highlighting relevant but unknown citations based on saving and reading history. We conducted a lab study that suggests CiteSee is significantly more effective for paper discovery than three baselines. A field deployment study shows CiteSee helps participants keep track of their explorations and leads to better situational awareness and increased paper discovery via inline citation when conducting real-world literature reviews.\n    [2] Title: Threddy: An Interactive System for Personalized Thread-based Exploration and Organization of Scientific Literature. Abstract: Reviewing the literature to understand relevant threads of past work is a critical part of research and vehicle for learning. However, as the scientific literature grows the challenges for users to find and make sense of the many different threads of research grow as well. Previous work has helped scholars to find and group papers with citation information or textual similarity using standalone tools or overview visualizations. Instead, in this work we explore a tool integrated into users\u2019 reading process that helps them with leveraging authors\u2019 existing summarization of threads, typically in introduction or related work sections, in order to situate their own work\u2019s contributions. To explore this we developed a prototype that supports efficient extraction and organization of threads along with supporting evidence as scientists read research articles. The system then recommends further relevant articles based on user-created threads. We evaluate the system in a lab study and find that it helps scientists to follow and curate research threads without breaking out of their flow of reading, collect relevant papers and clips, and discover interesting new articles to further grow threads.\n    [3] Title: Relatedly: Scaffolding Literature Reviews with Existing Related Work Sections. Abstract: Scholars who want to research a scientific topic must take time to read, extract meaning, and identify connections across many papers. As scientific literature grows, this becomes increasingly challenging. Meanwhile, authors summarize prior research in papers\u2019 related work sections, though this is scoped to support a single paper. A formative study found that while reading multiple related work paragraphs helps overview a topic, it is hard to navigate overlapping and diverging references and research foci. In this work, we design a system, Relatedly, that scaffolds exploring and reading multiple related work paragraphs on a topic, with features including dynamic re-ranking and highlighting to spotlight unexplored dissimilar information, auto-generated descriptive paragraph headings, and low-lighting of redundant information. From a within-subjects user study (n=15), we found that scholars generate more coherent, insightful, and comprehensive topic outlines using Relatedly compared to a baseline paper list.\n    [4] Title: CiteRead: Integrating Localized Citation Contexts into Scientific Paper Reading. Abstract: When reading a scholarly paper, scientists oftentimes wish to understand how follow-on work has built on or engages with what they are reading. While a paper itself can only discuss prior work, some scientific search engines can provide a list of all subsequent citing papers; unfortunately, they are undifferentiated and disconnected from the contents of the original reference paper. In this work, we introduce a novel paper reading experience that integrates relevant information about follow-on work directly into a paper, allowing readers to learn about newer papers and see how a paper is discussed by its citing papers in the context of the reference paper. We built a tool, called CiteRead, that implements the following three contributions: 1) automated techniques for selecting important citing papers, building on results from a formative study we conducted, 2) an automated process for localizing commentary provided by citing papers to a place in the reference paper, and 3) an interactive experience that allows readers to seamlessly alternate between the reference paper and information from citing papers (e.g., citation sentences), placed in the margins. Based on a user study with 12 scientists, we found that in comparison to having just a list of citing papers and their citation sentences, the use of CiteRead while reading allows for better comprehension and retention of information about follow-on work.\n    [5] Title: DiscipLink: Unfolding Interdisciplinary Information Seeking Process via Human-AI Co-Exploration. Abstract: Interdisciplinary studies often require researchers to explore literature in diverse branches of knowledge. Yet, navigating through the highly scattered knowledge from unfamiliar disciplines poses a significant challenge. In this paper, we introduce DiscipLink, a novel interactive system that facilitates collaboration between researchers and large language models (LLMs) in interdisciplinary information seeking (IIS). Based on users' topics of interest, DiscipLink initiates exploratory questions from the perspectives of possible relevant fields of study, and users can further tailor these questions. DiscipLink then supports users in searching and screening papers under selected questions by automatically expanding queries with disciplinary-specific terminologies, extracting themes from retrieved papers, and highlighting the connections between papers and questions. Our evaluation, comprising a within-subject comparative experiment and an open-ended exploratory study, reveals that DiscipLink can effectively support researchers in breaking down disciplinary boundaries and integrating scattered knowledge in diverse fields. The findings underscore the potential of LLM-powered tools in fostering information-seeking practices and bolstering interdisciplinary research.\n    [6] Title: SciLit: A Platform for Joint Scientific Literature Discovery, Summarization and Citation Generation. Abstract: Scientific writing involves retrieving, summarizing, and citing relevant papers, which can be time-consuming processes. Although in many workflows these processes are serially linked, there are opportunities for natural language processing (NLP) to provide end-to-end assistive tools. We propose SciLit, a pipeline that automatically recommends relevant papers, extracts highlights, and suggests a reference sentence as a citation of a paper, taking into consideration the user-provided context and keywords. SciLit efficiently recommends papers from large databases of hundreds of millions of papers using a two-stage pre-fetching and re-ranking literature search system that flexibly deals with addition and removal of a paper database. We provide a convenient user interface that displays the recommended papers as extractive summaries and that offers abstractively-generated citing sentences which are aligned with the provided context and which mention the chosen keyword(s). Our assistive tool for literature discovery and scientific writing is available at https://scilit.vercel.app\n    [7] Title: Rec4LRW - Scientific Paper Recommender System for Literature Review and Writing. Abstract: In this paper, we introduce Rec4LRW, a recommender system (RS) for assisting researchers in finding research papers for their literature review and writing purposes. This system focuses on three researcher tasks \u2013 (1) Building a reading list of research papers, (2) Finding similar papers based on a set of papers, and (3) Shortlisting papers from the final reading list for inclusion in manuscript based on article type. A set of intermediate criteria are proposed to capture the relations between a research paper and its bibliography. The recommendation techniques for the three tasks in Rec4LRW are specifically devised on top of the intermediate criteria. The Rec4LRW workflow along with the screen designs for the three tasks is provided in this paper. The recommendation techniques in the system will be evaluated with state-of-the-art approaches along with user-based evaluation in subsequent studies.\n    [8] Title: Recommender System for Literature Review and Writing. Abstract: In this paper, we introduce Rec4LRW, a recommender system (RS) for assisting researchers in finding research papers for their literature review and writing purposes. This system focuses on three researcher tasks \u2013 (1) Building a reading list of research papers, (2) Finding similar papers based on a set of papers, and (3) Shortlisting papers from the final reading list for inclusion in manuscript based on article type. A set of intermediate criteria are proposed to capture the relations between a research paper and its bibliography. The recommendation techniques for the three tasks in Rec4LRW are specifically devised on top of the intermediate criteria. The Rec4LRW workflow along with the screen designs for the three tasks is provided in this paper. The recommendation techniques in the system will be evaluated with state-of-the-art approaches along with user-based evaluation in subsequent studies.\n    [9] Title: Can I have more of these please?: Assisting researchers in finding similar research papers from a seed basket of papers. Abstract: During the literature review phase, the task of finding similar research papers can be a difficult proposition for researchers due to the procedural complexity of the task. Current systems and approaches help in finding similar papers for a given paper, even though researchers tend to additionally search using a set of papers. This paper aims to focus on conceptualizing and developing recommendation techniques for key literature review and manuscript preparatory tasks that are interconnected. In this paper, the user evaluation results of the task where seed basket-based discovery of papers is performed are presented.,A user evaluation study was conducted on a corpus of papers extracted from the ACM Digital Library. Participants in the study included 121 researchers who had experience in authoring research papers. Participants, split into students and staff groups, had to select one of the provided 43 topics and run the tasks offered by the developed assistive system. A questionnaire was provided at the end of each task for evaluating the task performance.,The results show that the student group evaluated the task more favourably than the staff group, even though the difference was statistically significant for only 5 of the 16 measures. The measures topical relevance, interdisciplinarity, familiarity and usefulness were found to be significant predictors for user satisfaction in this task. A majority of the participants, who explicitly stated the need for assistance in finding similar papers, were satisfied with the recommended papers in the study.,The current research helps in bridging the gap between novices and experts in terms of literature review skills. The hybrid recommendation technique evaluated in this study highlights the effectiveness of combining the results of different approaches in finding similar papers.\n\n                - Novel:\n                    Idea: Create a literature review tool that uses a **neural network with 3D-aware image context** to **enhance literature review efficiency**. The neural network would analyze and present citations in a 3D visual context, allowing users to interact with and explore citation networks spatially. **Comparative performance testing** will be conducted to evaluate the tool\u2019s effectiveness in improving citation discovery and comprehension compared to traditional methods.\n    Class: novel\n    Review: The idea is novel because it introduces a 3D-aware image context for literature review, which is not seen in related works\n    Related Papers:\n    [0] Title: Supporting Literature Review by Searching, Visualizing and Navigating Related Papers. Abstract: Citations and their accompanying information such as citation context is a very important consideration when searching for relevant information in literature. With literature continually expanding, obtaining this information in an efficient and effective way, using a standard search tool becomes a very laborious task. In addition, scientific articles found in the literature employ rich interrelationships e.g., citation and their accompanying information making it very difficult for standard search applications to present these interrelationships to the searcher in an efficient manner. In order to alleviate this intensive task, we have designed a tool VisNavi (Visualization and Navigation) that supports scientific researchers in their literature review, in a form of a visualized star-center approach that, for the current treated paper places a citing author at its center and the richly embedded interrelationships are spun around it. The star design enables researchers to gain a clearer insight by interactively exploring these rich interrelationships along with their accompanying information. We designed a human judgment experiment to obtain a human rating on the functionality of the tool. We then cast the human rating as a reference point to improve the tool's design.\n    [1] Title: PUREsuggest: Citation-based Literature Search and Visual Exploration with Keyword-controlled Rankings. Abstract: Citations allow quickly identifying related research. If multiple publications are selected as seeds, specific suggestions for related literature can be made based on the number of incoming and outgoing citation links to this selection. Interactively adding recommended publications to the selection refines the next suggestion and incrementally builds a relevant collection of publications. Following this approach, the paper presents a search and foraging approach, PUREsuggest, which combines citation-based suggestions with augmented visualizations of the citation network. The focus and novelty of the approach is, first, the transparency of how the rankings are explained visually and, second, that the process can be steered through user-defined keywords, which reflect topics of interests. The system can be used to build new literature collections, to update and assess existing ones, as well as to use the collected literature for identifying relevant experts in the field. We evaluated the recommendation approach through simulated sessions and performed a user study investigating search strategies and usage patterns supported by the interface.\n    [2] Title: Firework visualization: a model for local citation analysis. Abstract: Citation chasing, the pursuit of references from one publication to another, is a popular technique among researchers for retrieving relevant and related literature. While a focused and precise method, citation chasing may be incomplete and unstable. Different choices of paths can lead to different sets of literature and generate inconsistent search results. Revealing the linkages among references would guide citation chasing as well as improve and stabilize the results since it enables researchers to develop search strategies.\n In this paper we use network analysis to examine the connections among papers cited as references, identifying and investigating papers with high authority and hub values. We construct local citation networks, introducing the firework visualization model to support citation chasing. We further introduce an interactive browser, designed to provide direct manipulation of references and aggregated summaries of retrieved results. The proposed firework model allows users to intuitively browse a local citation network and find relevant documents efficiently.\n    [3] Title: PaperPoles: Facilitating adaptive visual exploration of scientific publications by citation links. Abstract: Finding relevant publications is a common task. Typically, a researcher browses through a list of publications and traces additional relevant publications. When relevant publications are identified, the list may be expanded by the citation links of the relevant publications. The information needs of researchers may change as they go through such iterative processes. The exploration process quickly becomes cumbersome as the list expands. Most existing academic search systems tend to be limited in terms of the extent to which searchers can adapt their search as they proceed. In this article, we introduce an adaptive visual exploration system named PaperPoles to support exploration of scientific publications in a context\u2010aware environment. Searchers can express their information needs by intuitively formulating positive and negative queries. The search results are grouped and displayed in a cluster view, which shows aspects and relevance patterns of the results to support navigation and exploration. We conducted an experiment to compare PaperPoles with a list\u2010based interface in performing two academic search tasks with different complexity. The results show that PaperPoles can improve the accuracy of searching for the simple and complex tasks. It can also reduce the completion time of searching and improve exploration effectiveness in the complex task. PaperPoles demonstrates a potentially effective workflow for adaptive visual search of complex information.\n    [4] Title: C-Rex: A Comprehensive System for Recommending In-Text Citations with Explanations. Abstract: Finding suitable citations for scientific publications can be challenging and time-consuming. To this end, context-aware citation recommendation approaches that recommend publications as candidates for in-text citations have been developed. In this paper, we present C-Rex, a web-based demonstration system available at http://c-rex.org for context-aware citation recommendation based on the Neural Citation Network [5] and millions of publications from the Microsoft Academic Graph. Our system is one of the first online context-aware citation recommendation systems and the first to incorporate not only a deep learning recommendation approach, but also explanation components to help users better understand why papers were recommended. In our offline evaluation, our model performs similarly to the one presented in the original paper and can serve as a basic framework for further implementations. In our online evaluation, we found that the explanations of recommendations increased users\u2019 satisfaction.\n    [5] Title: PaperVis: Literature Review Made Easy. Abstract: Reviewing literatures for a certain research field is always important for academics. One could use Google\u2010like information seeking tools, but oftentimes he/she would end up obtaining too many possibly related papers, as well as the papers in the associated citation network. During such a process, a user may easily get lost after following a few links for searching or cross\u2010referencing. It is also difficult for the user to identify relevant/important papers from the resulting huge collection of papers. Our work, called PaperVis, endeavors to provide a user\u2010friendly interface to help users quickly grasp the intrinsic complex citation\u2010reference structures among a specific group of papers. We modify the existing Radial Space Filling (RSF) and Bullseye View techniques to arrange involved papers as a node\u2010link graph that better depicts the relationships among them while saving the screen space at the same time. PaperVis applies visual cues to present node attributes and their transitions among interactions, and it categorizes papers into semantically meaningful hierarchies to facilitate ensuing literature exploration. We conduct experiments on the InfoVis 2004 Contest Dataset to demonstrate the effectiveness of PaperVis.\n    [6] Title: CiteSee: Augmenting Citations in Scientific Papers with Persistent and Personalized Historical Context. Abstract: When reading a scholarly article, inline citations help researchers contextualize the current article and discover relevant prior work. However, it can be challenging to prioritize and make sense of the hundreds of citations encountered during literature reviews. This paper introduces CiteSee, a paper reading tool that leverages a user\u2019s publishing, reading, and saving activities to provide personalized visual augmentations and context around citations. First, CiteSee connects the current paper to familiar contexts by surfacing known citations a user had cited or opened. Second, CiteSee helps users prioritize their exploration by highlighting relevant but unknown citations based on saving and reading history. We conducted a lab study that suggests CiteSee is significantly more effective for paper discovery than three baselines. A field deployment study shows CiteSee helps participants keep track of their explorations and leads to better situational awareness and increased paper discovery via inline citation when conducting real-world literature reviews.\n    [7] Title: Visualizing Feature-based Similarity for Research Paper Recommendation. Abstract: Research paper recommender systems are widely used by academics to discover and explore the most relevant publications on a topic. While existing recommendation interfaces present researchers with a ranked list of publications based on a global relevance score, they fail to visualize the full range of non-textual features uniquely present in academic publications: citations, figures, charts, or images, and mathematical formulae or expressions. Especially for STEM literature, examining such non-textual features efficiently can provide utility to researchers interested in answering specialized research questions or information needs. If research paper search and recommender systems are to consider the similarity of such features as one facet of a content-based similarity assessment for academic literature, new methods for visualizing these non-textual features are needed. In this paper, we review the state-of-the-art in visualizing feature-based similarity in documents. We subsequently propose a set of user-customizable visualization approaches tailored to STEM literature and the research paper recommendation context. Results from a study with 10 expert users show that the interactive visualization interface we propose for the exploration of non-textual features in publications can effectively address specialized information retrieval tasks, which cannot be addressed by existing research paper search or recommendation interfaces.\n    [8] Title: Rapid understanding of scientific paper collections: Integrating statistics, text analytics, and visualization. Abstract: Keeping up with rapidly growing research fields, especially when there are multiple interdisciplinary sources, requires substantial effort for researchers, program managers, or venture capital investors. Current theories and tools are directed at finding a paper or website, not gaining an understanding of the key papers, authors, controversies, and hypotheses. This report presents an effort to integrate statistics, text analytics, and visualization in a multiple coordinated window environment that supports exploration. Our prototype system, Action Science Explorer (ASE), provides an environment for demonstrating principles of coordination and conducting iterative usability tests of them with interested and knowledgeable users. We developed an understanding of the value of reference management, statistics, citation text extraction, natural language summarization for single and multiple documents, filters to interactively select key papers, and network visualization to see citation patterns and identify clusters. A three-phase usability study guided our revisions to ASE and led us to improve the testing methods. \u00a9 2012 Wiley Periodicals, Inc.\n    [9] Title: Interactive Exploration and Discovery of Scientific Publications with PubVis. Abstract: With an exponentially growing number of scientific papers published each year, advanced tools for exploring and discovering publications of interest are becoming indispensable. To empower users beyond a simple keyword search provided e.g. by Google Scholar, we present the novel web application PubVis. Powered by a variety of machine learning techniques, it combines essential features to help researchers find the content most relevant to them. An interactive visualization of a large collection of scientific publications provides an overview of the field and encourages the user to explore articles beyond a narrow research focus. This is augmented by personalized content based article recommendations as well as an advanced full text search to discover relevant references. The open sourced implementation of the app can be easily set up and run locally on a desktop computer to provide access to content tailored to the specific needs of individual users. Additionally, a PubVis demo with access to a collection of 10,000 papers can be tested online.\n\n\n    Idea: Develop a **human-centric explainable AI system** designed **to align LLM evaluation with human preferences**. This system will incorporate **explainable AI techniques** to clarify the reasoning behind LLM evaluations, making the process transparent and understandable to human evaluators. The system will be tested and refined through **descriptive and predictive analytics**, where human feedback and predictive models will be used to iteratively improve the accuracy and alignment of the evaluation criteria with human judgment. This approach aims to enhance trust and reliability in LLM evaluations by ensuring the evaluation process is both transparent and reflective of human preferences.\n    Class: novel\n    Review: The idea is novel because it uniquely combines explainable AI techniques with human-centric evaluation to align LLM assessments with human preferences, a concept not explored in similar works provided like EvalGen[0] and SLAM[1]. The use of descriptive and predictive analytics further distinguishes it.\n    Related Papers:\n    [0] Title: Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences. Abstract: Due to the cumbersome nature of human evaluation and limitations of code-based evaluation, Large Language Models (LLMs) are increasingly being used to assist humans in evaluating LLM outputs. Yet LLM-generated evaluators simply inherit all the problems of the LLMs they evaluate, requiring further human validation. We present a mixed-initiative approach to ``validate the validators'' -- aligning LLM-generated evaluation functions (be it prompts or code) with human requirements. Our interface, EvalGen, provides automated assistance to users in generating evaluation criteria and implementing assertions. While generating candidate implementations (Python functions, LLM grader prompts), EvalGen asks humans to grade a subset of LLM outputs; this feedback is used to select implementations that better align with user grades. A qualitative study finds overall support for EvalGen but underscores the subjectivity and iterative process of alignment. In particular, we identify a phenomenon we dub \\emph{criteria drift}: users need criteria to grade outputs, but grading outputs helps users define criteria. What is more, some criteria appears \\emph{dependent} on the specific LLM outputs observed (rather than independent criteria that can be defined \\emph{a priori}), raising serious questions for approaches that assume the independence of evaluation from observation of model outputs. We present our interface and implementation details, a comparison of our algorithm with a baseline approach, and implications for the design of future LLM evaluation assistants.\n    [1] Title: Aligning Model Evaluations with Human Preferences: Mitigating Token Count Bias in Language Model Assessments. Abstract: The SLAM paper demonstrated that on-device Small Language Models (SLMs) are a viable and cost-effective alternative to API-based Large Language Models (LLMs), such as OpenAI's GPT-4, offering comparable performance and stability. However, SLAM also identified discrepancies between human preferences and traditional auto-evaluators. This follow-up paper explores methods to align LLM evaluator preferences with human evaluations by addressing biases, particularly toward higher token counts. We employed Bayesian statistics and a t-test to quantify this bias and developed a recalibration procedure to adjust the GPTScorer. Our findings significantly improve aligning the recalibrated LLM evaluator with human evaluations across multiple use cases. For instance, spearman's ranking correlation score in the Recommendation use case improved from -27.27 to 44.55. These results highlight the importance of accounting for biases in automated evaluations to ensure fair and accurate model assessments. The recalibration process enhances the reliability of automated evaluators, leading to better AI models that align with human values and expectations. This study provides a robust methodology for future research into bias correction and emphasizes the feasibility and benefits of developing human-aligned AI evaluation systems.\n    [2] Title: Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems. Abstract: Explainable artificially intelligent (XAI) systems form part of sociotechnical systems, e.g., human+AI teams tasked with making decisions. Yet, current XAI systems are rarely evaluated by measuring the performance of human+AI teams on actual decision-making tasks. We conducted two online experiments and one in-person think-aloud study to evaluate two currently common techniques for evaluating XAI systems: (1) using proxy, artificial tasks such as how well humans predict the AI's decision from the given explanations, and (2) using subjective measures of trust and preference as predictors of actual performance. The results of our experiments demonstrate that evaluations with proxy tasks did not predict the results of the evaluations with the actual decision-making tasks. Further, the subjective measures on evaluations with actual decision-making tasks did not predict the objective performance on those same tasks. Our results suggest that by employing misleading evaluation methods, our field may be inadvertently slowing its progress toward developing human+AI teams that can reliably perform better than humans or AIs alone.\n    [3] Title: A Conceptual Model Framework for XAI Requirement Elicitation of Application Domain System. Abstract: The use of data analytics and Machine Learning (ML) branches of AI for predictive and analytic knowledge retrieval has surged significantly in various industries (e.g., health, finance, business, and manufacturing). However, the acceptance of AI has been hindered by opaque models that lack transparency. Explainability in AI (XAI) has gained significant prominence owing to its focus on introducing avenues of accountability in AI. XAI acknowledges the importance of human factors and strives to incorporate them into the design process, recognising that the cognitive effort involved in understanding explanations is a key aspect. Mental Models play a crucial role in the XAI evaluative premise, but their current utility is limited. By intentionally designing explanations that align with users\u2019 mental models, their experiences can be significantly enhanced, leading to improved understanding, satisfaction, trust, and performance. This study proposes using Mental Models to elicit explainability requirements and to develop an Ontology-Driven Conceptual Model to facilitate the learning process for a better understanding of explanations.\n    [4] Title: Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making. Abstract: In AI-assisted decision-making, humans often passively review AI's suggestion and decide whether to accept or reject it as a whole. In such a paradigm, humans are found to rarely trigger analytical thinking and face difficulties in communicating the nuances of conflicting opinions to the AI when disagreements occur. To tackle this challenge, we propose Human-AI Deliberation, a novel framework to promote human reflection and discussion on conflicting human-AI opinions in decision-making. Based on theories in human deliberation, this framework engages humans and AI in dimension-level opinion elicitation, deliberative discussion, and decision updates. To empower AI with deliberative capabilities, we designed Deliberative AI, which leverages large language models (LLMs) as a bridge between humans and domain-specific models to enable flexible conversational interactions and faithful information provision. An exploratory evaluation on a graduate admissions task shows that Deliberative AI outperforms conventional explainable AI (XAI) assistants in improving humans' appropriate reliance and task performance. Based on a mixed-methods analysis of participant behavior, perception, user experience, and open-ended feedback, we draw implications for future AI-assisted decision tool design.\n    [5] Title: Explaining Groups of Instances Counterfactually for XAI: A Use Case, Algorithm and User Study for Group-Counterfactuals. Abstract: Counterfactual explanations are an increasingly popular form of post hoc explanation due to their (i) applicability across problem domains, (ii) proposed legal compliance (e.g., with GDPR), and (iii) reliance on the contrastive nature of human explanation. Although counterfactual explanations are normally used to explain individual predictive-instances, we explore a novel use case in which groups of similar instances are explained in a collective fashion using ``group counterfactuals'' (e.g., to highlight a repeating pattern of illness in a group of patients). These group counterfactuals meet a human preference for coherent, broad explanations covering multiple events/instances. A novel, group-counterfactual algorithm is proposed to generate high-coverage explanations that are faithful to the to-be-explained model. This explanation strategy is also evaluated in a large, controlled user study (N=207), using objective (i.e., accuracy) and subjective (i.e., confidence, explanation satisfaction, and trust) psychological measures. The results show that group counterfactuals elicit modest but definite improvements in people's understanding of an AI system. The implications of these findings for counterfactual methods and for XAI are discussed.\n    [6] Title: Human-Centered Evaluation and Auditing of Language Models. Abstract: The recent advancements in Large Language Models (LLMs) have significantly impacted numerous, and will impact more, real-world applications. However, these models also pose significant risks to individuals and society. To mitigate these issues and guide future model development, responsible evaluation and auditing of LLMs are essential. This workshop aims to address the current \u201cevaluation crisis\u201d in LLM research and practice by bringing together HCI and AI researchers and practitioners to rethink LLM evaluation and auditing from a human-centered perspective. The workshop will explore topics around understanding stakeholders\u2019 needs and goals with evaluation and auditing LLMs, establishing human-centered evaluation and auditing methods, developing tools and resources to support these methods, building community and fostering collaboration. By soliciting papers, organizing invited keynote and panel, and facilitating group discussions, this workshop aims to develop a future research agenda for addressing the challenges in LLM evaluation and auditing.\n    [7] Title: Human-Centered Design Recommendations for LLM-as-a-judge. Abstract: Traditional reference-based metrics, such as BLEU and ROUGE, are less effective for assessing outputs from Large Language Models (LLMs) that produce highly creative or superior-quality text, or in situations where reference outputs are unavailable. While human evaluation remains an option, it is costly and difficult to scale. Recent work using LLMs as evaluators (LLM-as-a-judge) is promising, but trust and reliability remain a significant concern. Integrating human input is crucial to ensure criteria used to evaluate are aligned with the human\u2019s intent, and evaluations are robust and consistent. This paper presents a user study of a design exploration called EvaluLLM, that enables users to leverage LLMs as customizable judges, promoting human involvement to balance trust and cost-saving potential with caution. Through interviews with eight domain experts, we identified the need for assistance in developing effective evaluation criteria aligning the LLM-as-a-judge with practitioners\u2019 preferences and expectations. We offer findings and design recommendations to optimize human-assisted LLM-as-judge systems.\n    [8] Title: ALLURE: A Systematic Protocol for Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning. Abstract: From grading papers to summarizing medical documents, large language models (LLMs) are evermore used for evaluation of text generated by humans and AI alike. However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors. ALLURE involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs. Through this iterative process, we aim to refine the performance of the evaluator LLM, ultimately reducing the reliance on human annotators in the evaluation process. We anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data and productivity in these fields.\n    [9] Title: EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria. Abstract: By simply composing prompts, developers can prototype novel generative applications with Large Language Models (LLMs). To refine prototypes into products, however, developers must iteratively revise prompts by evaluating outputs to diagnose weaknesses. Formative interviews (N=8) revealed that developers invest significant effort in manually evaluating outputs as they assess context-specific and subjective criteria. We present EvalLM, an interactive system for iteratively refining prompts by evaluating multiple outputs on user-defined criteria. By describing criteria in natural language, users can employ the system\u2019s LLM-based evaluator to get an overview of where prompts excel or fail, and improve these based on the evaluator\u2019s feedback. A comparative study (N=12) showed that EvalLM, when compared to manual evaluation, helped participants compose more diverse criteria, examine twice as many outputs, and reach satisfactory prompts with 59% fewer revisions. Beyond prompts, our work can be extended to augment model evaluation and alignment in specific application contexts.\n\n\n    Idea: Develop a **joint analysis framework** designed **to automate survey paper relevance review**, thereby improving the efficiency and objectivity of the process. This framework will integrate multi-level analysis, including document-, sentence-, and word-level evaluations, to comprehensively assess the relevance of survey papers to specified prompts. The system will be evaluated using a **benchmark comparison** that measures performance across various datasets and against human reviewers, enabling the identification of strengths and weaknesses in different contexts.\n    Class: novel\n    Review: The idea is novel because it focuses on two unique aspects (1) automating survey paper relevance review and (2) assesing the relevance of the survey paper to a prompt. Both of this is not addressed by similar papers: NLPeer[0], ReviewRobot[1], or ReviVal[2].\n    Related Papers:\n    [0] Title: NLPeer: A Unified Resource for the Computational Study of Peer Review. Abstract: Peer review constitutes a core component of scholarly publishing; yet it demands substantial expertise and training, and is susceptible to errors and biases. Various applications of NLP for peer reviewing assistance aim to support reviewers in this complex process, but the lack of clearly licensed datasets and multi-domain corpora prevent the systematic study of NLP for peer review. To remedy this, we introduce NLPeer\u2013 the first ethically sourced multidomain corpus of more than 5k papers and 11k review reports from five different venues. In addition to the new datasets of paper drafts, camera-ready versions and peer reviews from the NLP community, we establish a unified data representation and augment previous peer review datasets to include parsed and structured paper representations, rich metadata and versioning information. We complement our resource with implementations and analysis of three reviewing assistance tasks, including a novel guided skimming task.Our work paves the path towards systematic, multi-faceted, evidence-based study of peer review in NLP and beyond. The data and code are publicly available.\n    [1] Title: ReviewRobot: Explainable Paper Review Generation based on Knowledge Synthesis. Abstract: To assist human review process, we build a novel ReviewRobot to automatically assign a review score and write comments for multiple categories such as novelty and meaningful comparison. A good review needs to be knowledgeable, namely that the comments should be constructive and informative to help improve the paper; and explainable by providing detailed evidence. ReviewRobot achieves these goals via three steps: (1) We perform domain-specific Information Extraction to construct a knowledge graph (KG) from the target paper under review, a related work KG from the papers cited by the target paper, and a background KG from a large collection of previous papers in the domain. (2) By comparing these three KGs, we predict a review score and detailed structured knowledge as evidence for each review category. (3) We carefully select and generalize human review sentences into templates, and apply these templates to transform the review scores and evidence into natural language comments. Experimental results show that our review score predictor reaches 71.4%-100% accuracy. Human assessment by domain experts shows that 41.7%-70.5% of the comments generated by ReviewRobot are valid and constructive, and better than human-written ones for 20% of the time. Thus, ReviewRobot can serve as an assistant for paper reviewers, program chairs and authors.\n    [2] Title: ReviVal: Towards Automatically Evaluating the Informativeness of Peer Reviews. Abstract: The peer-review process is currently under stress due to the increasingly large number of submissions to top-tier venues, especially in Artificial Intelligence (AI) and Machine Learning (ML). Consequently, the quality of peer reviews is under question, and dissatisfaction among authors is not uncommon but rather prominent. In this work, we propose \"ReviVal\" (expanded as \"REVIew eVALuation\"), a system to automatically grade a peer-review report for its informativeness. We define review informativeness in terms of its Exhaustiveness and Strength, where Exhaustiveness signifies how exhaustively the review covers the different sections and qualitative aspects1 of the paper and Strength signifies how sure the reviewer is of their evaluation. We train ReviVal, a multitask deep network for review informativeness prediction on the publicly available peer reviews, which we curate from the openreview2 platform. We annotate the review sentence(s) with labels for (a) which sections and (b) what quality aspects of the paper those refer. We automatically annotate our data with the reviewer\u2019s sentiment intensity to capture the reviewer\u2019s conviction. Our approach significantly outperforms several intuitive baselines for this novel task. To the best of our knowledge, our work is a first-of-its-kind to automatically estimate the informativeness of a peer review report.\n    [3] Title: Can We Automate Scientific Reviewing?. Abstract: The rapid development of science and technology has been accompanied by an exponential growth in peer-reviewed scientific publications. At the same time, the review of each paper is a laborious process that must be carried out by subject matter experts. Thus, providing high-quality reviews of this growing number of papers is a significant challenge. In this work, we ask the question \u201ccan we automate scientific reviewing? \u201d, discussing the possibility of using natural language processing (NLP) models to generate peer reviews for scientific papers. Because it is non-trivial to define what a \u201cgood\u201d review is in the first place, we first discuss possible evaluation metrics that could be used to judge success in this task. We then focus on the machine learning domain and collect a dataset of papers in the domain, annotate them with different aspects of content covered in each review, and train targeted summarization models that take in papers as input and generate reviews as output. Comprehensive experimental results on the test set show that while system-generated reviews are comprehensive, touching upon more aspects of the paper than human-written reviews, the generated texts are less constructive and less factual than human-written reviews for all aspects except the explanation of the core ideas of the papers, which are largely factually correct. Given these results, we pose eight challenges in the pursuit of a good review generation system together with potential solutions, which, hopefully, will inspire more future research in this direction.\nWe make relevant resource publicly available for use by future research: https://github. com/neulab/ReviewAdvisor. In addition, while our conclusion is that the technology is not yet ready for use in high-stakes review settings we provide a system demo, ReviewAdvisor (http://review.nlpedia.ai/), showing the current capabilities and failings of state-of-the-art NLP models at this task (see demo screenshot in A.2). A review of this paper written by the system proposed in this paper can be found in A.1.\n    [4] Title: A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications. Abstract: Peer reviewing is a central component in the scientific publishing process. We present the first public dataset of scientific peer reviews available for research purposes (PeerRead v1),1 providing an opportunity to study this important artifact. The dataset consists of 14.7K paper drafts and the corresponding accept/reject decisions in top-tier venues including ACL, NIPS and ICLR. The dataset also includes 10.7K textual peer reviews written by experts for a subset of the papers. We describe the data collection process and report interesting observed phenomena in the peer reviews. We also propose two novel NLP tasks based on this dataset and provide simple baseline models. In the first task, we show that simple models can predict whether a paper is accepted with up to 21% error reduction compared to the majority baseline. In the second task, we predict the numerical scores of review aspects and show that simple models can outperform the mean baseline for aspects with high variance such as \u2018originality\u2019 and \u2018impact\u2019.\n    [5] Title: KID-Review: Knowledge-Guided Scientific Review Generation with Oracle Pre-training. Abstract: The surge in the number of scientific submissions has brought challenges to the work of peer review. In this paper, as a first step, we explore the possibility of designing an automated system, which is not meant to replace humans, but rather providing a first-pass draft for a machine-assisted human review process. Specifically, we present an end-to-end knowledge-guided review generation framework for scientific papers grounded in cognitive psychology research that a better understanding of text requires different types of knowledge. In practice, we found that this seemingly intuitive idea suffered from training difficulties. In order to solve this problem, we put forward an oracle pre-training strategy, which can not only make the Kid-Review better educated but also make the generated review cover more aspects. Experimentally, we perform a comprehensive evaluation (human and automatic) from different perspectives. Empirical results have shown the effectiveness of different types of knowledge as well as oracle pre-training. We make all code, relevant dataset available: https://github.com/Anonymous4nlp233/KIDReview as well as the Kid-Review system: http://nlpeer.reviews.\n    [6] Title: ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing. Abstract: Given the rapid ascent of large language models (LLMs), we study the question: (How) can large language models help in reviewing of scientific papers or proposals? We first conduct some pilot studies where we find that (i) GPT-4 outperforms other LLMs (Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly, OpenAssistant, StableLM), and (ii) prompting with a specific question (e.g., to identify errors) outperforms prompting to simply write a review. With these insights, we study the use of LLMs (specifically, GPT-4) for three tasks: 1. Identifying errors: We construct 13 short computer science papers each with a deliberately inserted error, and ask the LLM to check for the correctness of these papers. We observe that the LLM finds errors in 7 of them, spanning both mathematical and conceptual errors. 2. Verifying checklists: We task the LLM to verify 16 closed-ended checklist questions in the respective sections of 15 NeurIPS 2022 papers. We find that across 119 {checklist question, paper} pairs, the LLM had an 86.6% accuracy. 3. Choosing the\"better\"paper: We generate 10 pairs of abstracts, deliberately designing each pair in such a way that one abstract was clearly superior than the other. The LLM, however, struggled to discern these relatively straightforward distinctions accurately, committing errors in its evaluations for 6 out of the 10 pairs. Based on these experiments, we think that LLMs have a promising use as reviewing assistants for specific reviewing tasks, but not (yet) for complete evaluations of papers or proposals.\n    [7] Title: Can large language models provide useful feedback on research papers? A large-scale empirical analysis. Abstract: Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR machine learning conference (1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature journals, 39.23% for ICLR) is comparable to the overlap between two human reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The overlap between GPT-4 and human reviewers is larger for the weaker papers. We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half (57.4%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations.\n    [8] Title: GPT4 is Slightly Helpful for Peer-Review Assistance: A Pilot Study. Abstract: In this pilot study, we investigate the use of GPT4 to assist in the peer-review process. Our key hypothesis was that GPT-generated reviews could achieve comparable helpfulness to human reviewers. By comparing reviews generated by both human reviewers and GPT models for academic papers submitted to a major machine learning conference, we provide initial evidence that artificial intelligence can contribute effectively to the peer-review process. We also perform robustness experiments with inserted errors to understand which parts of the paper the model tends to focus on. Our findings open new avenues for leveraging machine learning tools to address resource constraints in peer review. The results also shed light on potential enhancements to the review process and lay the groundwork for further research on scaling oversight in a domain where human-feedback is increasingly a scarce resource.\n    [9] Title: Prophy: An automated reviewer finder to improve the efficiency, diversity and quality of reviews. Abstract: Peer review is under pressure. Without fair, transparent and efficient peer review we cannot ensure the right proposals get funded and the correct manuscripts get published. In the era of Open Access, which is driving an exponential increase in the number of submitted publications, how we carry out peer review is becoming increasingly important and how we find reviewers is coming under scrutiny. The current methods are slow and produce bias pools of reviewers. As such we need an improved way. At Prophy we have developed a state-of-the-art referee finder that can find experts to review any manuscript from any scientific field in seconds. Then through post-processing filters we can find appropriate candidate referees who are most likely to review a paper, whilst highlighting important conflicts of interest through our complex citation networks. These methods can ensure fair and independent experts who can review interdisciplinary papers from any discipline. These methods are being delivered through APIs and the editorial workflow of editors ensure the right people get access to these tools. Finally, as large-language models improve, so does Prophy and as such we will be looking to drive real innovation in this area in years to come.\n\n\n    Idea: Develop a **system that aligns LLM evaluation with human preferences** by incorporating **EEG and pupillometric signals** to capture real-time human feedback and emotional responses. This system would use these physiological signals to better understand human evaluators' cognitive load and emotional engagement, thus refining the LLM's evaluation criteria dynamically. The system would be evaluated through a **mixed-methods user study**, involving both quantitative analysis of physiological data and qualitative feedback from human users on the system's alignment accuracy and usability.\n    Class: novel\n    Review: The idea is novel because it uniquely combines EEG and pupillometric signals for aligning LLM evaluations with human preferences, unlike the related papers shown.\n    Related Papers:\n    [0] Title: A Mental Workload Estimation for Visualization Evaluation Using EEG Data and NASA-TLX. Abstract: Mental workload is a cognitive effort felt by users while solving tasks, and good visualizations tend to induce a low mental work-load. For better visualizations, various visualization techniques have been evaluated through quantitative methods that compare the response accuracy and performance time for completing visualization tasks. However, accuracy and time do not always represent the mental workload of a subject. Since quantitative approaches do not fully mirror mental workload, questionnaires and biosignals have been employed to measure mental workload in visualization assessments. The electroencephalogram (EEG) as biosignal is one of the indicators frequently utilized to measure mental workload. Since everyone judges and senses differently, EEG signals and mental workload differ from person to person. In this paper, we propose a mental workload personalized estimation model with EEG data specialized for each individual to evaluate visualizations. We use scatter plot, bar, line, and map visualizations and collect NASA-TLX scores as mental workload and EEG data. NASA-TLX and EEG data as training data are used for the mental workload estimation model.\n    [1] Title: THE IMPACT OF LEARNING THROUGH COGNITIVE LOAD ASSESSMENT AND EMOTIONAL STATE EVALUATION. Abstract: The cognitive load (CL) and the emotion that encompasses each learning process, despite its type or level, represent very challenging aspects of a learning process, their assessment being difficult mainly due to the subjective appreciation. On the other hand, nowadays, students learn by spending the majority of their time on using digital devices. Therefore, analyzing the emotional state during learning, evaluating the visual effort, and assessing the cognitive load level, all induced by the use of software applications or electronic devices, becomes a necessity.\nEmotions are defined as mental and physiological sensorial states that influence human perception, thinking, experience, learning, behavior, and decision making. They are apprehended as a short and conscious experience characterized by an intense mental activity classified between negative or unpleasant and positive or pleasant feelings. Some peripheral physiological signals as heart rate, arterial pressure, electrodermal activity, or body temperature reflect the excitement level, but they're unable to decern the positivity and the negativity relative to a reference state. A student's concentration effort or its emotional state could be evaluated as well by using precise measurements of the pupil dimension of the blinking frequency or the eye movement. Instruction should aim to reduce the extrinsic CL and to optimize the intrinsic CL enhancing the learning relevant CL. Nevertheless, only EEG signals offer precise information concerning the emotional state and the CL level through sensors placed on the scalp according to the functioning of the cerebral lobes and their reactions to certain short-term or long-term stimuli.\nRecording and processing physiological signals or cerebral waves are some of the most explored solutions of the last decade that are leading to encouraging results. This paper mainly studies and illustrates some approaches to evaluating the cognitive load and emotional state of students during a learning process. Essentially, this paper focuses on elaborating experimental sessions, choosing the proper stimulus and equipment, exploiting adequate recording and preprocessing methods for the involved physiological information, and engaging artificial intelligence techniques for feature selection and data classification to achieve the best calibration, appreciation, and monitoring of a learning process.\n    [2] Title: Measuring User Experience of Adaptive User Interfaces using EEG: A Replication Study. Abstract: Background: Adaptive user interfaces have the advantage of being able to dynamically change their aspect and/or behaviour depending on the characteristics of the context of use, i.e. to improve user experience. User experience is an important quality factor that has been primarily evaluated with classical measures (e.g. effectiveness, efficiency, satisfaction), but to a lesser extent with physiological measures, such as emotion recognition, skin response, or brain activity. Aim: In a previous exploratory experiment involving users with different profiles and a wide range of ages, we analysed user experience in terms of cognitive load, engagement, attraction and memorisation when employing twenty graphical adaptive menus through the use of an Electroencephalogram (EEG) device. The results indicated that there were statistically significant differences for these four variables. However, we considered that it was necessary to confirm or reject these findings using a more homogeneous group of users. Method: We conducted an operational internal replication study with 40 participants. We also investigated the potential correlation between EEG signals and the participants\u2019 user experience ratings, such as their preferences. Results: The results of this experiment confirm that there are statistically significant differences between the EEG variables when the participants interact with the different adaptive menus. Moreover, there is a high correlation among the participants\u2019 user experience ratings and the EEG signals, and a trend regarding performance has emerged from our analysis. Conclusions: These findings suggest that EEG signals could be used to evaluate user experience. With regard to the menus studied, our results suggest that graphical menus with different structures and font types produce more differences in users\u2019 brain responses, while menus which use colours produce more similarities in users\u2019 brain responses. Several insights with which to improve users\u2019 experience of graphical adaptive menus are outlined.\n    [3] Title: The Role of EEG Signals: SVM Classification of Cognitive Load as a Support for UX Evaluation. Abstract: Cognitive load is the mental effort that needs to be applied to working memory to process information received over a period of time. Cognitive load can be viewed as the level of mental energy required to process a given amount of information. In user experience design, cognitive load is considered as the mental processing power required to use a product. If the amount of information processed exceeds the user's ability to process it, the overall performance will be disrupted. An EEG device is needed that is used to record electrical activity that occurs in the brain by channeling brain electrical waves to cables and modulators that are sensitive to electrical waves. The object of this research is the EEG Beta signal with the attention wave type from UX testing activities on students aged 21-24 years with a frequency level of 13-30 Hz. The EEG tool records the activity of the respondent's wave signal by collecting data on the activity of working on a questionnaire about evaluating the WhatsApp application using the Google Form application. The classification of cognitive load studied is unencumbered and burdened. Unencumbered represents the ease that is felt when interacting with the application, while burdened represents the difficulty or confusion that is felt when interacting with the application. Testing is done with the Confusion matrix. The best accuracy results among the kernel types in the SVM method are linear kernel types with an accuracy result of 89% consisting of 1 data that is categorized as an unencumbered label and 8 data labels that are loaded\n    [4] Title: Cognitive Load Evaluation of Human-computer Interface Based on EEG Multi-dimensional Feature. Abstract: To evaluate accurately the cognitive load (CL) of the operator under the digital interactive interface is helpful to guide the optimization of the digital interface and finally improve the ergonomics. In order to further improve the stability and accuracy of cognitive load evaluation method, combined with EEG experiment, deep learning is applied to CL evaluation problem. Firstly, the preprocessed EEG signals are directly input into CNN-LSTM network to extract the timedomain features of EEG. Secondly, the frequency-domain features of EEG are extracted by FFT and deep belief network (DBN). Thirdly, the time-frequency feature of EEG is obtained by Morlet wavelet transform and the multi-CNN. Finally, the cognitive load of interactive interface is classified by support vector machine (SVM). By recruiting 16 subjects, EEG data under two CL conditions were collected for experiments. The experimental results show that compared with other single deep learning algorithms, it can extract EEG time-domain features, frequency-domain features and time-frequency-domain features more accurately, so has stronger robustness.\n    [5] Title: A Comprehensive Survey of EEG Preprocessing Methods for Cognitive Load Assessment. Abstract: Preprocessing electroencephalographic (EEG) signals during computer-mediated Cognitive Load tasks is crucial in Human-Computer Interaction (HCI). This process significantly influences subsequent EEG analysis and the efficacy of Artificial Intelligence (AI) models employed in Cognitive Load Assessment. Consequently, it stands as an indispensable procedure for developing dependable systems capable of adapting to users\u2019 cognitive capacities and constraints. We systematically analyzed fifty-seven (57) research papers on computer-mediated Cognitive Load EEG experiments published between 2018 and 2023. The preprocessing methods identified were multiple, controversial, and strongly dependent on the particularities of each experiment and the derived experimental dataset. Our investigation involved the meticulous classification of preprocessing methods based on distinct parameters, namely the degree of user intervention, the noise level, and the subject pool size. Particular attention was paid to semi-automated denoising technology since conventional methods, advanced approaches, and standardized pipelines overwhelm research, but no optimum solution is available yet. This survey is anticipated to provide a valuable contribution to the rising demand for an efficient and fully automated preprocessing approach in EEG-based computerized Cognitive Load experiments.\n    [6] Title: Cognitive Load Prediction from Multimodal Physiological Signals using Multiview Learning.. Abstract: Predicting cognitive load is a crucial issue in the emerging field of human-computer interaction and holds significant practical value, particularly in flight scenarios. Although previous studies have realized efficient cognitive load classification, new research is still needed to adapt the current state-of-the-art multimodal fusion methods. Here, we proposed a feature selection framework based on multiview learning to address the challenges of information redundancy and reveal the common physiological mechanisms underlying cognitive load. Specifically, the multimodal signal features (EEG, EDA, ECG, EOG, & eye movements) at three cognitive load levels were estimated during multiattribute task battery (MATB) tasks performed by 22 healthy participants and fed into a feature selection-multiview classification with cohesion and diversity (FS-MCCD) framework. The optimized feature set was extracted from the original feature set by integrating the weight of each view and the feature weights to formulate the ranking criteria. The cognitive load prediction model, evaluated using real-time classification results, achieved an average accuracy of 81.08% and an average F1-score of 80.94% for three-class classification among 22 participants. Furthermore, the weights of the physiological signal features revealed the physiological mechanisms related to cognitive load. Specifically, heightened cognitive load was linked to amplified \u03b4 and \u03b8 power in the frontal lobe, reduced \u03b1 power in the parietal lobe, and an increase in pupil diameter. Thus, the proposed multimodal feature fusion framework emphasizes the effectiveness and efficiency of using these features to predict cognitive load.\n    [7] Title: Emotional Activity Is Negatively Associated With Cognitive Load in Multimedia Learning: A Case Study With EEG Signals. Abstract: We aimed to investigate the relationship between emotional activity and cognitive load during multimedia learning from an emotion dynamics perspective using electroencephalography (EEG) signals. Using a between-subjects design, 42 university students were randomly assigned to two video lecture conditions (color-coded vs. grayscale). While the participants watched the assigned video, their EEG signals were recorded. After processing the EEG signals, we employed the correlation-based feature selector (CFS) method to identify emotion-related subject-independent features. We then put these features into the Isomap model to obtain a one-dimensional trajectory of emotional changes. Next, we used the zero-crossing rate (ZCR) as the quantitative characterization of emotional changes ZCREC. Meanwhile, we extracted cognitive load-related features to analyze the degree of cognitive load (CLI). We employed a linear regression fitting method to study the relationship between ZCREC and CLI. We conducted this study from two perspectives. One is the frequency domain method (wavelet feature), and the other is the non-linear dynamic method (entropy features). The results indicate that emotional activity is negatively associated with cognitive load. These findings have practical implications for designing video lectures for multimedia learning. Learning material should reduce learners\u2019 cognitive load to keep their emotional experience at optimal levels to enhance learning.\n    [8] Title: Wearable Device-Based Real-Time Monitoring of Physiological Signals: Evaluating Cognitive Load Across Different Tasks. Abstract: This study employs cutting-edge wearable monitoring technology to conduct high-precision, high-temporal-resolution (1-second interval) cognitive load assessment on electroencephalogram (EEG) data from the FP1 channel and heart rate variability (HRV) data of secondary vocational students. By jointly analyzing these two critical physiological indicators, the research delves into their application value in assessing cognitive load among secondary vocational students and their utility across various tasks. The study designed two experiments to validate the efficacy of the proposed approach: Initially, a random forest classification model, developed using the N-BACK task, enabled the precise decoding of physiological signal characteristics in secondary vocational students under different levels of cognitive load, achieving a classification accuracy of 97%. Subsequently, this classification model was applied in a cross-task experiment involving the National Computer Rank Examination (Level-1), demonstrating the method's significant applicability and cross-task transferability in diverse learning contexts. Conducted with high portability, this research holds substantial theoretical and practical significance for optimizing teaching resource allocation in secondary vocational education, as well as for cognitive load assessment methods and monitoring. Currently, the research findings are undergoing trial implementation in the school.\n    [9] Title: Wearable Device-Based Physiological Signal Monitoring: An Assessment Study of Cognitive Load Across Tasks. Abstract: This study employs cutting-edge wearable monitoring technology to conduct high-precision, high-temporal-resolution cognitive load assessment on EEG data from the FP1 channel and heart rate variability (HRV) data of secondary vocational students(SVS). By jointly analyzing these two critical physiological indicators, the research delves into their application value in assessing cognitive load among SVS students and their utility across various tasks. The study designed two experiments to validate the efficacy of the proposed approach: Initially, a random forest classification model, developed using the N-BACK task, enabled the precise decoding of physiological signal characteristics in SVS students under different levels of cognitive load, achieving a classification accuracy of 97%. Subsequently, this classification model was applied in a cross-task experiment involving the National Computer Rank Examination, demonstrating the method's significant applicability and cross-task transferability in diverse learning contexts. Conducted with high portability, this research holds substantial theoretical and practical significance for optimizing teaching resource allocation in secondary vocational education, as well as for cognitive load assessment methods and monitoring. Currently, the research findings are undergoing trial implementation in the school.\n\n\n    Idea: Develop a **hybrid optimization with machine learning** model designed **to enhance language model problem-solving** by optimizing the reasoning paths selected by the language model. This new model would integrate the **hybrid Moth Flame Optimization (HMFO)** approach to dynamically adjust and refine the reasoning pathways, enhancing the model\u2019s strategic decision-making capabilities. The model's effectiveness would be evaluated based on its **performance on novel tasks**, including complex planning and creative tasks.\n    Class: novel\n    Review: The idea is novel because it introduces the hybrid Moth Flame Optimization (HMFO) approach, which is not mentioned in related works.\n    Related Papers:\n    [0] Title: Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training. Abstract: Recent works like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the reasoning capabilities of LLMs by using tree-search algorithms to guide multi-step reasoning. These methods rely on prompting a pre-trained model to serve as a value function and focus on problems with low search depth. As a result, these methods will not work in domains where the pre-trained LLM does not have enough knowledge to serve as an effective value function or in domains that require long-horizon planning. To address these limitations, we present an AlphaZero-like tree-search learning framework for LLMs (termed TS-LLM), systematically illustrating how tree-search with a learned value function can guide LLM decoding. TS-LLM distinguishes itself in two key ways. (1) Leveraging a learned value function and AlphaZero-like algorithms, our approach can be generally adaptable to a wide range of tasks, language models of any size, and tasks of varying search depths. (2) Our approach can guide LLMs during both inference and training, iteratively improving the LLM. Empirical results across reasoning, planning, alignment, and decision-making tasks show that TS-LLM outperforms existing approaches and can handle trees with a depth of 64.\n    [1] Title: Reasoning with Language Model is Planning with World Model. Abstract: Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.\n    [2] Title: Tree of Thoughts: Deliberate Problem Solving with Large Language Models. Abstract: Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.\n    [3] Title: Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models. Abstract: While language models (LMs) have shown potential across a range of decision-making tasks, their reliance on simple acting processes limits their broad deployment as autonomous agents. In this paper, we introduce Language Agent Tree Search (LATS) -- the first general framework that synergizes the capabilities of LMs in reasoning, acting, and planning. By leveraging the in-context learning ability of LMs, we integrate Monte Carlo Tree Search into LATS to enable LMs as agents, along with LM-powered value functions and self-reflections for proficient exploration and enhanced decision-making. A key feature of our approach is the incorporation of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that surpasses the constraints of existing techniques. Our experimental evaluation across diverse domains, including programming, interactive question-answering (QA), web navigation, and math, validates the effectiveness and generality of LATS in decision-making while maintaining competitive or improved reasoning performance. Notably, LATS achieves state-of-the-art pass@1 accuracy (92.7%) for programming on HumanEval with GPT-4 and demonstrates gradient-free performance (average score of 75.9) comparable to gradient-based fine-tuning for web navigation on WebShop with GPT-3.5. Code can be found at https://github.com/lapisrocks/LanguageAgentTreeSearch\n    [4] Title: Large Language Model Guided Tree-of-Thought. Abstract: In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implemented a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. Our implementation of the ToT-based Sudoku solver is available on GitHub: \\url{https://github.com/jieyilong/tree-of-thought-puzzle-solver}.\n    [5] Title: Tree of Uncertain Thoughts Reasoning for Large Language Models. Abstract: While the recently introduced Tree of Thoughts (ToT) has heralded advancements in allowing Large Language Models (LLMs) to reason through foresight and backtracking for global decision-making, it has overlooked the inherent local uncertainties in intermediate decision points or \"thoughts\". These local uncertainties, intrinsic to LLMs given their potential for diverse responses, remain a significant concern in the reasoning process. Addressing this pivotal gap, we introduce the Tree of Uncertain Thoughts (TouT) \u2014 a reasoning framework tailored for LLMs. Our TouT effectively leverages Monte Carlo Dropout to quantify uncertainty scores associated with LLMs\u2019 diverse local responses at these intermediate steps. By marrying this local uncertainty quantification with global search algorithms, TouT enhances the model\u2019s precision in response generation. We substantiate our approach with rigorous experiments on two demanding planning tasks: Game of 24 and Mini Crosswords. The empirical evidence underscores TouT\u2019s superiority over both ToT and chain-of-thought prompting methods.\n    [6] Title: Graph of Thoughts: Solving Elaborate Problems with Large Language Models. Abstract: We introduce Graph of Thoughts (GoT): a framework that\nadvances prompting capabilities in large language models\n(LLMs) beyond those offered by paradigms such as \nChain-of-Thought or Tree of Thoughts (ToT). The key idea and \nprimary advantage of GoT is the ability to model the information \ngenerated by an LLM as an arbitrary graph, where units of \ninformation (\"LLM thoughts\") are vertices, and edges correspond\nto dependencies between these vertices. This approach enables \ncombining arbitrary LLM thoughts into synergistic outcomes, \ndistilling the essence of whole networks of thoughts,\nor enhancing thoughts using feedback loops. We illustrate\nthat GoT offers advantages over state of the art on different\ntasks, for example increasing the quality of sorting by 62%\nover ToT, while simultaneously reducing costs by >31%.\nWe ensure that GoT is extensible with new thought \ntransformations and thus can be used to spearhead new prompting\nschemes. This work brings the LLM reasoning closer to human \nthinking or brain mechanisms such as recurrence, both\nof which form complex networks\n    [7] Title: Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving. Abstract: Large Language Models (LLMs) have achieved tremendous progress, yet they still often struggle with challenging reasoning problems. Current approaches address this challenge by sampling or searching detailed and low-level reasoning chains. However, these methods are still limited in their exploration capabilities, making it challenging for correct solutions to stand out in the huge solution space. In this work, we unleash LLMs' creative potential for exploring multiple diverse problem solving strategies by framing an LLM as a hierarchical policy via in-context learning. This policy comprises of a visionary leader that proposes multiple diverse high-level problem-solving tactics as hints, accompanied by a follower that executes detailed problem-solving processes following each of the high-level instruction. The follower uses each of the leader's directives as a guide and samples multiple reasoning chains to tackle the problem, generating a solution group for each leader proposal. Additionally, we propose an effective and efficient tournament-based approach to select among these explored solution groups to reach the final answer. Our approach produces meaningful and inspiring hints, enhances problem-solving strategy exploration, and improves the final answer accuracy on challenging problems in the MATH dataset. Code will be released at https://github.com/lz1oceani/LLM-As-Hierarchical-Policy.\n    [8] Title: Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning. Abstract: Large Language Models (LLMs) prompted to generate chain-of-thought (CoT) exhibit impressive reasoning capabilities. Recent attempts at prompt decomposition toward solving complex, multi-step reasoning problems depend on the ability of the LLM to simultaneously decompose and solve the problem. A significant disadvantage is that foundational LLMs are typically not available for fine-tuning, making adaptation computationally prohibitive. We believe (and demonstrate) that problem decomposition and solution generation are distinct capabilites, better addressed in separate modules, than by one monolithic LLM. We introduce DaSLaM, which uses a decomposition generator to decompose complex problems into subproblems that require fewer reasoning steps. These subproblems are answered by a solver. We use a relatively small (13B parameters) LM as the decomposition generator, which we train using policy gradient optimization to interact with a solver LM (regarded as black-box) and guide it through subproblems, thereby rendering our method solver-agnostic. Evaluation on multiple different reasoning datasets reveal that with our method, a 175 billion parameter LM (text-davinci-003) can produce competitive or even better performance, compared to its orders-of-magnitude larger successor, GPT-4. Additionally, we show that DaSLaM is not limited by the solver's capabilities as a function of scale; e.g., solver LMs with diverse sizes give significant performance improvement with our solver-agnostic decomposition technique. Exhaustive ablation studies evince the superiority of our modular finetuning technique over exorbitantly large decomposer LLMs, based on prompting alone.\n    [9] Title: SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks. Abstract: We introduce SwiftSage, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks. SwiftSage integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance task completion performance. The framework comprises two primary modules: the Swift module, representing fast and intuitive thinking, and the Sage module, emulating deliberate thought processes. The Swift module is a small encoder-decoder LM fine-tuned on the oracle agent's action trajectories, while the Sage module employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a heuristic method to harmoniously integrate the two modules, resulting in a more efficient and robust problem-solving process. In 30 tasks from the ScienceWorld benchmark, SwiftSage significantly outperforms other methods such as SayCan, ReAct, and Reflexion, demonstrating its effectiveness in solving complex interactive tasks.\n\n\n    Idea: <b style=\"background-color:#F0FFF0;\">To improve drivable area segmentation</b> in human-AI collaboration, we can use <b style=\"background-color:#FFFACD;\">AI explanations generated with different techniques</b> to provide context-aware feedback. These explanations can incorporate data from multiple sensors (e.g., visual, LiDAR) to inform users about the AI\u2019s decision-making process. We will conduct <b style=\"background-color:#E6E6FA;\">mixed-method user studies</b> to measure the effectiveness of these explanations in enhancing human understanding and decision-making accuracy. This approach aims to create a more transparent and robust collaboration framework between humans and AI systems.\n    Class: novel\n    Review: The idea is novel because it uniquely integrates AI explanations from multiple sensors for drivable area segmentation, unlike existing works that focus on general decision-making tasks with explanations, such as bird species identification[0] and object identification[4].\n    Related Papers:\n    [0] Title: The Impact of Imperfect XAI on Human-AI Decision-Making. Abstract: Explainability techniques are rapidly being developed to improve human-AI decision-making across various cooperative work settings. Consequently, previous research has evaluated how decision-makers collaborate with imperfect AI by investigating appropriate reliance and task performance with the aim of designing more human-centered computer-supported collaborative tools. Several human-centered explainable AI (XAI) techniques have been proposed in hopes of improving decision-makers' collaboration with AI; however, these techniques are grounded in findings from previous studies that primarily focus on the impact of incorrect AI advice. Few studies acknowledge the possibility of the explanations being incorrect even if the AI advice is correct. Thus, it is crucial to understand how imperfect XAI affects human-AI decision-making. In this work, we contribute a robust, mixed-methods user study with 136 participants to evaluate how incorrect explanations influence humans' decision-making behavior in a bird species identification task, taking into account their level of expertise and an explanation's level of assertiveness. Our findings reveal the influence of imperfect XAI and humans' level of expertise on their reliance on AI and human-AI team performance. We also discuss how explanations can deceive decision-makers during human-AI collaboration. Hence, we shed light on the impacts of imperfect XAI in the field of computer-supported cooperative work and provide guidelines for designers of human-AI collaboration systems.\n    [1] Title: Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making. Abstract: Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.\n    [2] Title: Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance. Abstract: Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations. However, prior studies observed improvements from explanations only when the AI, alone, outperformed both the human and the best team. Can explanations help lead to complementary performance, where team accuracy is higher than either the human or the AI working solo? We conduct mixed-method user studies on three datasets, where an AI with accuracy comparable to humans helps participants solve a task (explaining itself in some conditions). While we observed complementary improvements from AI augmentation, they were not increased by explanations. Rather, explanations increased the chance that humans will accept the AI\u2019s recommendation, regardless of its correctness. Our result poses new challenges for human-centered AI: Can we develop explanatory approaches that encourage appropriate trust in AI, and therefore help generate (or improve) complementary performance?\n    [3] Title: Uncalibrated Models Can Improve Human-AI Collaboration. Abstract: In many practical applications of AI, an AI model is used as a decision aid for human users. The AI provides advice that a human (sometimes) incorporates into their decision-making process. The AI advice is often presented with some measure of\"confidence\"that the human can use to calibrate how much they depend on or trust the advice. In this paper, we present an initial exploration that suggests showing AI models as more confident than they actually are, even when the original AI is well-calibrated, can improve human-AI performance (measured as the accuracy and confidence of the human's final prediction after seeing the AI advice). We first train a model to predict human incorporation of AI advice using data from thousands of human-AI interactions. This enables us to explicitly estimate how to transform the AI's prediction confidence, making the AI uncalibrated, in order to improve the final human prediction. We empirically validate our results across four different tasks--dealing with images, text and tabular data--involving hundreds of human participants. We further support our findings with simulation analysis. Our findings suggest the importance of jointly optimizing the human-AI system as opposed to the standard paradigm of optimizing the AI model alone.\n    [4] Title: You Complete Me: Human-AI Teams and Complementary Expertise. Abstract: People consider recommendations from AI systems in diverse domains ranging from recognizing tumors in medical images to deciding which shoes look cute with an outfit. Implicit in the decision process is the perceived expertise of the AI system. In this paper, we investigate how people trust and rely on an AI assistant that performs with different levels of expertise relative to the person, ranging from completely overlapping expertise to perfectly complementary expertise. Through a series of controlled online lab studies where participants identified objects with the help of an AI assistant, we demonstrate that participants were able to perceive when the assistant was an expert or non-expert within the same task and calibrate their reliance on the AI to improve team performance. We also demonstrate that communicating expertise through the linguistic properties of the explanation text was effective, where embracing language increased reliance and distancing language reduced reliance on AI.\n    [5] Title: Human-Centered Evaluation of Explanations in AI-Assisted Decision-Making. Abstract: AI explanations have been increasingly used to help people better utilize AI recommendations in AI-assisted decision making. While numerous technical transparency approaches have been established, a human-centered perspective is needed for understanding how human decision makers use and process AI explanations. In my thesis, I start with an empirical exploration of how AI explanations shape the way people understand and utilize AI decision aids. Next, I move to the time\u2011evolving nature of AI explanations, exploring how explanation changes due to AI model updates affect human decision makers\u2019 perception and usage of AI models. Lastly, I construct computational human behavior models to gain a more quantitative understandings of human decision makers\u2019 cognitive interactions with AI explanations. I conclude with future work on carefully identifying user needs for explainable AI in an era when AI models are becoming more complex and human-AI collaboration scenarios are increasingly diversified.\n    [6] Title: On Selective, Mutable and Dialogic XAI: a Review of What Users Say about Different Types of Interactive Explanations. Abstract: Explainability (XAI) has matured in recent years to provide more human-centered explanations of AI-based decision systems. While static explanations remain predominant, interactive XAI has gathered momentum to support the human cognitive process of explaining. However, the evidence regarding the benefits of interactive explanations is unclear. In this paper, we map existing findings by conducting a detailed scoping review of 48 empirical studies in which interactive explanations are evaluated with human users. We also create a classification of interactive techniques specific to XAI and group the resulting categories according to their role in the cognitive process of explanation: \"selective\", \"mutable\" or \"dialogic\". We identify the effects of interactivity on several user-based metrics. We find that interactive explanations improve perceived usefulness and performance of the human+AI team but take longer. We highlight conflicting results regarding cognitive load and overconfidence. Lastly, we describe underexplored areas including measuring curiosity or learning or perturbing outcomes.\n    [7] Title: Calibrating Trust in AI-Assisted Decision Making. Abstract: With the proliferation of AI products, humans and AI are increasingly working in partnership with each other to make decisions. For this type of collaboration to be successful, humans need to understand AI capability in order to effectively calibrate their trust. In these partnerships, it\u2019s critical to explain decisions and predictions in a manner that can be understood by humans in order to encourage trust calibration. The field of explainable AI is focused on integrating explainability into AI, but is geared towards making AI models more interpretable. As a result, this research often approaches explanations from a model-centric perspective, as opposed to a human-centered perspective. At the same time, industry researchers have developed guidelines to help interface designers effectively generate user-friendly explanations. However, these guidelines are typically too broad to be effective for the day-to-day work of industry designers. Our research addresses this gap through two approaches: an empirical experiment to investigate how people respond to explanations and what types of explanations are most helpful for trust calibration, and an educational resource for industry designers to help them understand what questions users might have, and how the context of use influences what explanations they may use. Findings from our experiment indicate that explanations do not always aid trust calibration, and can actually hurt it, especially in the face of novice users who have low self-competence. Our exploratory interviews and usability testing with industry designers reveal that there is a desire for a comprehensive but accessible educational resource that translates research such as our experiment and guides the design of explainable interfaces for AI products.\n    [8] Title: Understanding the Effect of Out-of-distribution Examples and Interactive Explanations on Human-AI Decision Making. Abstract: Although AI holds promise for improving human decision making in societally critical domains, it remains an open question how human-AI teams can reliably outperform AI alone and human alone in challenging prediction tasks (also known as complementary performance). We explore two directions to understand the gaps in achieving complementary performance. First, we argue that the typical experimental setup limits the potential of human-AI teams. To account for lower AI performance out-of-distribution than in-distribution because of distribution shift, we design experiments with different distribution types and investigate human performance for both in-distribution and out-of-distribution examples. Second, we develop novel interfaces to support interactive explanations so that humans can actively engage with AI assistance. Using virtual pilot studies and large-scale randomized experiments across three tasks, we demonstrate a clear difference between in-distribution and out-of-distribution, and observe mixed results for interactive explanations: while interactive explanations improve human perception of AI assistance's usefulness, they may reinforce human biases and lead to limited performance improvement. Overall, our work points out critical challenges and future directions towards enhancing human performance with AI assistance.\n    [9] Title: Understanding the Role of Human Intuition on Reliance in Human-AI Decision-Making with Explanations. Abstract: AI explanations are often mentioned as a way to improve human-AI decision-making, but empirical studies have not found consistent evidence of explanations' effectiveness and, on the contrary, suggest that they can increase overreliance when the AI system is wrong. While many factors may affect reliance on AI support, one important factor is how decision-makers reconcile their own intuition---beliefs or heuristics, based on prior knowledge, experience, or pattern recognition, used to make judgments---with the information provided by the AI system to determine when to override AI predictions. We conduct a think-aloud, mixed-methods study with two explanation types (feature- and example-based) for two prediction tasks to explore how decision-makers' intuition affects their use of AI predictions and explanations, and ultimately their choice of when to rely on AI. Our results identify three types of intuition involved in reasoning about AI predictions and explanations: intuition about the task outcome, features, and AI limitations. Building on these, we summarize three observed pathways for decision-makers to apply their own intuition and override AI predictions. We use these pathways to explain why (1) the feature-based explanations we used did not improve participants' decision outcomes and increased their overreliance on AI, and (2) the example-based explanations we used improved decision-makers' performance over feature-based explanations and helped achieve complementary human-AI performance. Overall, our work identifies directions for further development of AI decision-support systems and explanation methods that help decision-makers effectively apply their intuition to achieve appropriate reliance on AI.\n\n\n    Idea: Create a gamified health promotion platform that combines digital and tabletop game elements to support caregiver psychological adjustment. The platform will include stress-relief games, emotional support activities, and community-building features tailored to caregivers' needs.\n    Class: novel\n    Review: The idea is novel because it uniquely combines digital and tabletop game elements to support caregiver psychological adjustment, a focus not seen in related works\n    Related Papers:\n    [0] Title: Tabletop Board Game Elements and Gamification Interventions for Health Behavior Change: Realist Review and Proposal of a Game Design Framework. Abstract: Background Games, when used as interventional tools, can influence behavior change by incentivizing, reinforcing, educating, providing feedback loops, prompting, persuading, or providing meaning, fun, and community. However, not all game elements will appeal to all consumers equally, and different elements might work for different people and in different contexts. Objective The aim of this study was to conduct a realist review of tabletop games targeting behavior change and to propose a framework for designing effective behavior change games. Methods A realist review was conducted to inform program theory in the development of tabletop games for health behavior change. The context, mechanisms used to change behavior, and outcomes of included studies were reviewed through a realist lens. Results Thirty-one papers met the eligibility criteria and were included in the review. Several design methods were identified that enhanced the efficacy of the games to change behavior. These included design by local teams, pilot testing, clearly defined targets of behavior change, conscious attention to all aspects of game design, including game mechanics, dynamics, aesthetics, and the elicitation of emotions. Delivery with other mediums, leveraging behavioral insights, prior training for delivery, and repeated play were also important. Some design elements that were found to reduce efficacy included limited replayability or lack of fun for immersive engagement. Conclusions Game designers need to consider all aspects of the context and the mechanisms to achieve the desired behavior change outcomes. Careful design thinking should include consideration of the game mechanics, dynamics, aesthetics, emotions, and contexts of the game and the players. People who know the players and the contexts well should design the games or have significant input. Testing in real-world settings is likely to lead to better outcomes. Careful selection and purposeful design of the behavior change mechanisms at play is essential. Fun and enjoyment of the player should be considered, as without engagement, there will be no desired intervention effect.\n    [1] Title: Tabletop Board Game Elements and Gamification Interventions for Health Behavior Change: Realist Review and Proposal of a Game Design Framework (Preprint). Abstract: \n BACKGROUND\n Games, when used as interventional tools, can influence behavior change by incentivizing, reinforcing, educating, providing feedback loops, prompting, persuading, or providing meaning, fun, and community. However, not all game elements will appeal to all consumers equally, and different elements might work for different people and in different contexts.\n \n \n OBJECTIVE\n The aim of this study was to conduct a realist review of tabletop games targeting behavior change and to propose a framework for designing effective behavior change games.\n \n \n METHODS\n A realist review was conducted to inform program theory in the development of tabletop games for health behavior change. The context, mechanisms used to change behavior, and outcomes of included studies were reviewed through a realist lens.\n \n \n RESULTS\n Thirty-one papers met the eligibility criteria and were included in the review. Several design methods were identified that enhanced the efficacy of the games to change behavior. These included design by local teams, pilot testing, clearly defined targets of behavior change, conscious attention to all aspects of game design, including game mechanics, dynamics, aesthetics, and the elicitation of emotions. Delivery with other mediums, leveraging behavioral insights, prior training for delivery, and repeated play were also important. Some design elements that were found to reduce efficacy included limited replayability or lack of fun for immersive engagement.\n \n \n CONCLUSIONS\n Game designers need to consider all aspects of the context and the mechanisms to achieve the desired behavior change outcomes. Careful design thinking should include consideration of the game mechanics, dynamics, aesthetics, emotions, and contexts of the game and the players. People who know the players and the contexts well should design the games or have significant input. Testing in real-world settings is likely to lead to better outcomes. Careful selection and purposeful design of the behavior change mechanisms at play is essential. Fun and enjoyment of the player should be considered, as without engagement, there will be no desired intervention effect.\n\n    [2] Title: A game plan: Gamification design principles in mHealth applications for chronic disease management. Abstract: Effective chronic disease management is essential to improve positive health outcomes, and incentive strategies are useful in promoting self-care with longevity. Gamification, applied with mHealth (mobile health) applications, has the potential to better facilitate patient self-management. This review article addresses a knowledge gap around the effective use of gamification design principles, or mechanics, in developing mHealth applications. Badges, leaderboards, points and levels, challenges and quests, social engagement loops, and onboarding are mechanics that comprise gamification. These mechanics are defined and explained from a design and development perspective. Health and fitness applications with gamification mechanics include: bant which uses points, levels, and social engagement, mySugr which uses challenges and quests, RunKeeper which uses leaderboards as well as social engagement loops and onboarding, Fitocracy which uses badges, and Mango Health, which uses points and levels. Specific design considerations are explored, an example of the efficacy of a gamified mHealth implementation in facilitating improved self-management is provided, limitations to this work are discussed, a link between the principles of gaming and gamification in health and wellness technologies is provided, and suggestions for future work are made. We conclude that gamification could be leveraged in developing applications with the potential to better facilitate self-management in persons with chronic conditions.\n    [3] Title: Playing alone: can game design elements satisfy user needs in gamified mHealth services?. Abstract: Chronic health conditions have necessitated the need for behavioral interventions (such as exercise programs) outside of clinical contexts, increasingly managed through technology such as mobile health (mHealth) services. Gamification has emerged as a promising tool to facilitate greater engagement in these services; however, no studies investigate the links between specific game design elements (GDEs) and psychological or behavioral outcomes within a health context. This domain is motivationally complex and has shown resistance to the satisfaction of social (relatedness) needs, presenting a challenge to the design of gamification products for health promotion. Drawing on self-determination theory, this research demonstrates the strengths of a taxonomy based upon structural features of GDEs (such as social, narrative or reward elements) rather than the design intent definitions of these elements used in previous studies. This taxonomy is then used to assess the relationship between GDEs and psychological needs satisfaction in a survey (N\u2009=\u2009236) of gamified exercise/fitness application users. Further qualitative interviews (N\u2009=\u200920) were conducted to clarify survey findings. This research demonstrates the positive association between control and presentation elements and autonomy satisfaction, and control and reward elements and competency satisfaction. However, it also suggests that player type and context may limit the ability for GDEs alone to support relatedness satisfaction in mHealth services. Implications for managers and researchers are discussed, particularly the strengths and weaknesses of using structural taxonomies in gamification assessment.\n    [4] Title: Design Features in Games for Health: Disciplinary and Interdisciplinary Expert Perspectives. Abstract: Games for health (G4H) aim to improve health outcomes and encourage behavior change. While existing theoretical frameworks describe features of both games and health interventions, there has been limited systematic investigation into how disciplinary and interdisciplinary stakeholders understand design features in G4H. We recruited 18 experts from the fields of game design, behavioral health, and games for health, and prompted them with 16 sample games. Applying methods including open card sorting and triading, we elicited themes and features (e.g., real-world interaction, game mechanics) around G4H. We found evidence of conceptual differences suggesting that a G4H perspective is not simply the sum of game and health perspectives. At the same time, we found evidence of convergence in stakeholder views, including areas where game experts provided insights about health and vice versa. We discuss how this work can be applied to provide conceptual tools, improve the G4H design process, and guide approaches to encoding G4H-related data for large-scale empirical analysis.\n    [5] Title: Guidelines for Designing Effective Games as Clinical Interventions: Mechanics, Dynamics, Aesthetics, and Outcomes (MDAO) Framework. Abstract: Games are a successful pedagogical tool to change attitudes and behaviors. This chapter will examine how games facilitate change, discuss common pitfalls, and outline best practices for making serious games for clinical practice. Sustained engagement and motivation are key to lasting clinical interventions. When developing a game for clinical practice, the designer should avoid \u201cpunishing by rewards\u201d (Kohn, 1993), damaging motivation towards the desired goal. Understanding game design principles is crucial to creating intrinsically engaging experiences that lead to lasting motivation. The Mechanics, Dynamics, and Aesthetics (MDA) framework is widely accepted by game designers as a framework to make compelling games. Using MDA as a base for understanding how to create engaging experiences, this chapter proposes a new framework for serious games called Mechanics, Dynamics, Aesthetics, and Outcomes. MDAO describes how to design a game that is intrinsically motivating and effective by focusing on the interplay between outcomes and other vectors of design.\n    [6] Title: Quittr: The Design of a Video Game to Support Smoking Cessation. Abstract: Background Smoking is recognized as the largest, single, preventable cause of death and disease in the developed world. While the majority of smokers report wanting to quit, and many try each year, smokers find it difficult to maintain long-term abstinence. Behavioral support, such as education, advice, goal-setting, and encouragement, is known to be beneficial in improving the likelihood of succeeding in a quit attempt, but it remains difficult to effectively deliver this behavioral support and keep the patient engaged with the process for a sufficient duration. In an attempt to solve this, there have been numerous mobile apps developed, yet engagement and retention have remained key challenges that limit the potential effectiveness of these interventions. Video games have been clearly linked with the effective delivery of health interventions, due to their capacity to increase motivation and engagement of players. Objective The objective of this study is to describe the design and development of a smartphone app that is theory-driven, and which incorporates gaming characteristics in order to promote engagement with content, and thereby help smokers to quit. Methods Game design and development was informed by a taxonomy of motivational affordances for meaningful gamified and persuasive technologies. This taxonomy describes a set of design components that is grounded in well-established psychological theories on motivation. Results This paper reports on the design and development process of Quittr, a mobile app, describing how game design principles, game mechanics, and game elements can be used to embed education and support content, such that the app actually requires the user to access and engage with relevant educational content. The next stage of this research is to conduct a randomized controlled trial to determine whether the additional incentivization game features offer any value in terms of the key metrics of engagement\u2013how much content users are consuming, how many days users are persisting with using the app, and what proportion of users successfully abstain from smoking for 28 days, based on user-reported data and verified against a biochemical baseline using cotinine tests. Conclusions We describe a novel, and theoretically-informed mobile app design approach that has a broad range of potential applications. By using the virtual currency approach, we remove the need for the game to comprehensively integrate the healthy activity as part of its actual play mechanics. This opens up the potential for a wide variety of health problems to be tackled through games where no obvious play mechanic presents itself. The implications of this app are that similar approaches may be of benefit in areas such as managing chronic conditions (diabetes, heart disease, etc), treating substance abuse (alcohol, illicit drugs, etc), diet and exercise, eating disorders (anorexia, bulimia, and binge eating), and various phobias.\n    [7] Title: Developing Theory-Driven, Evidence-Based Serious Games for Health: Framework Based on Research Community Insights. Abstract: Background The idea of using serious games to effectuate better outcomes in health care has gained significant traction among a growing community of researchers, developers, and health care professionals. Many now recognize the importance of creating evidence-based games that are purposefully designed to address physical and mental health challenges faced by end users. To date, no regulatory resources have been established to guide the development of serious games for health (SGH). Developers must therefore look elsewhere for guidance. Although a more robust level of evidence exists in the research literature, it is neither structured nor is there any clear consensus. Developers currently use a variety of approaches and methodologies. The establishment of a well-defined framework that represents the consensus views of the SGH research community would help developers improve the efficiency of internal development processes, as well as chances of success. A consensus framework would also enhance the credibility of SGH and help provide quality evidence of their effectiveness. Objective This research aimed to (1) identify and evaluate the requirements, recommendations, and guidelines proposed by the SGH community in the research literature, and; (2) develop a consensus framework to guide developers, designers, researchers, and health care professionals in the development of evidence-based SGH. Methods A critical review of the literature was performed in October to November 2018. A 3-step search strategy and a predefined set of inclusion criteria were used to identify relevant articles in PubMed, ScienceDirect, Institute of Electrical and Electronics Engineers Xplore, CiteSeerX, and Google Scholar. A supplemental search of publications from regulatory authorities was conducted to capture their specific requirements. Three researchers independently evaluated the identified articles. The evidence was coded and categorized for analysis. Results This review identified 5 categories of high-level requirements and 20 low-level requirements suggested by the SGH community. These advocate a methodological approach that is multidisciplinary, iterative, and participatory. On the basis of the requirements identified, we propose a framework for developing theory-driven, evidence-based SGH. It comprises 5 stages that are informed by various stakeholders. It focuses on building strong scientific and design foundations that guide the creative and technical development. It includes quantitative trials to evaluate whether the SGH achieve the intended outcomes, as well as efforts to disseminate trial findings and follow-up monitoring after the SGH are rolled out for use. Conclusions This review resulted in the formulation of a framework for developing theory-driven, evidence-based SGH that represents many of the requirements set out by SGH stakeholders in the literature. It covers all aspects of the development process (scientific, technological, and design) and is transparently described in sufficient detail to allow SGH stakeholders to implement it in a wide variety of projects, irrespective of discipline, health care segments, or focus.\n    [8] Title: The Construction and Evaluation of a Design Framework for Narrative Games for Health. Abstract: A larger number of games for health were developed over the past two decades to provide an engaging way of health care and behavior change intervention. However, many problems with the design of these games, as well as with the methodologies used to evaluate them emerged: the games were generally designed without the consultation or direct involvement of a professional game designer, and created without the guidance of a proper game design framework; the health messages delivered in the games were mostly simple and knowledge-oriented and not crafted based on theories from behavioral medicine; and the evaluation studies were also poorly designed. To solve these problems, I define the DraGuNa (Drama-Guided Narrative Health Game) framework, a methodology that uses drama theory and sound principles from behavioral medicine to guide games for health design to solve the current problems in games for health. The dissertation introduces a methodology of game design, specifically developed for games for health, which addresses two key constructs: engagement \u2013 ensuring users stick with the game for the duration of the intervention; and adherence \u2013 ensuring users perform those actions in the game hypothesized by behavioral medicine theories to lead to health behavior change. The dissertation also provides a methodology to develop interactive narrative-based games based on existing story media, which also suggests a new path of research for the intelligent narrative community. Finally, the dissertation describes an experimental framework for testing the effects of a game on the two fundamental dimensions of player involvement in the intervention \u2013 engagement and adherence \u2013 and tests the relative contributions of each on health outcomes.\n    [9] Title: The Model of Gamification Principles for Digital Health Interventions: Evaluation of Validity and Potential Utility. Abstract: Background Although gamification continues to be a popular approach to increase engagement, motivation, and adherence to behavioral interventions, empirical studies have rarely focused on this topic. There is a need to empirically evaluate gamification models to increase the understanding of how to integrate gamification into interventions. Objective The model of gamification principles for digital health interventions proposes a set of five independent yet interrelated gamification principles. This study aimed to examine the validity and reliability of this model to inform its use in Web- and mobile-based apps. Methods A total of 17 digital health interventions were selected from a curated website of mobile- and Web-based apps (PsyberGuide), which makes independent and unbiased ratings on various metrics. A total of 133 independent raters trained in gamification evaluation techniques were instructed to evaluate the apps and rate the degree to which gamification principles are present. Multiple ratings (n\u226520) were collected for each of the five gamification principles within each app. Existing measures, including the PsyberGuide credibility score, mobile app rating scale (MARS), and the app store rating of each app were collected, and their relationship with the gamification principle scores was investigated. Results Apps varied widely in the degree of gamification implemented (ie, the mean gamification rating ranged from 0.17\u2264m\u22644.65 out of 5). Inter-rater reliability of gamification scores for each app was acceptable (\u03ba\u22650.5). There was no significant correlation between any of the five gamification principles and the PsyberGuide credibility score (P\u2265.49 in all cases). Three gamification principles (supporting player archetypes, feedback, and visibility) were significantly correlated with the MARS score, whereas three principles (meaningful purpose, meaningful choice, and supporting player archetypes) were significantly correlated with the app store rating. One gamification principle was statistically significant with both the MARS and the app store rating (supporting player archetypes). Conclusions Overall, the results support the validity and potential utility of the model of gamification principles for digital health interventions. As expected, there was some overlap between several gamification principles and existing app measures (eg, MARS). However, the results indicate that the gamification principles are not redundant with existing measures and highlight the potential utility of a 5-factor gamification model structure in digital behavioral health interventions. These gamification principles may be used to improve user experience and enhance engagement with digital health programs.\n\n\n    Idea: Develop a **hierarchical topic model** that **integrates a parallel Transformer-CNN mixture block** to simultaneously capture local and global topic dependencies. The **parallel Transformer component** will handle long-range dependencies and global topic coherence, while the **CNN component** will excel in identifying local topic structures and nuances within the documents. This hybrid approach aims **to improve hierarchical topic modeling** by making the model more robust in capturing intricate topic hierarchies. The enhanced model will be evaluated through **baseline comparisons** with existing hierarchical topic models to demonstrate improvements in topic coherence and scalability.\n    Class: novel\n    Review: The idea is novel because it integrates a parallel Transformer-CNN mixture block, which is not mentioned in any of the related papers.\n    Related Papers:\n    [0] Title: Large-Scale Hierarchical Topic Models. Abstract: In the past decade, a number of advances in topic modeling have produced sophisticated models that are capable of generating hierarchies of topics. One challenge for these models is scalability: they are incapable of working at the massive scale of millions of documents and hundreds of thousands of terms. We address this challenge with a technique that learns a hierarchy of topics by iteratively applying topic models and processing subtrees of the hierarchy in parallel. This approach has a number of scalability advantages compared to existing techniques, and shows promising results in experiments assessing runtime and human evaluations of quality. We detail extensions to this approach that may further improve hierarchical topic modeling for large-scale applications.\n    [1] Title: Nonlinear Structural Equation Model Guided Gaussian Mixture Hierarchical Topic Modeling. Abstract: Hierarchical topic models, which can extract semantically meaningful topics from a textcorpus in an unsupervised manner and automatically organise them into a topic hierarchy, have been widely used to discover the underlying semantic structure of documents. However, the existing models often assume in the prior that the topic hierarchy is a tree structure, ignoring symmetrical dependenciesbetween topics at the same level. Moreover, the sparsity of text data often complicate the analysis. To address these issues, we propose NSEM-GMHTM as a deep topic model, witha Gaussian mixture prior distribution to improve the model\u2019s ability to adapt to sparse data, which explicitly models hierarchical and symmetric relations between topics through the dependency matrices and nonlinear structural equations. Experiments on widely used datasets show that our NSEM-GMHTM generates more coherent topics and a more rational topic structure when compared to state-of-theart baselines. Our code is available at https: //github.com/nbnbhwyy/NSEM-GMHTM.\n    [2] Title: Inter and Intra Topic Structure Learning with Word Embeddings. Abstract: One important task of topic modeling for text analysis is interpretability. By discovering structured topics one is able to yield improved interpretability as well as modeling accuracy. In this paper, we propose a novel topic model with a deep structure that explores both inter-topic and intra-topic structures informed by word embeddings. Specifically, our model discovers inter topic structures in the form of topic hierarchies and discovers intra topic structures in the form of sub-topics, each of which is informed by word embeddings and captures a fine-grained thematic aspect of a normal topic. Extensive experiments demonstrate that our model achieves the state-of-the-art performance in terms of perplexity, document classification, and topic quality. Moreover, with topic hierarchies and sub-topics, the topics discovered in our model are more interpretable, providing an illuminating means to understand text data.\n    [3] Title: Towards the TopMost: A Topic Modeling System Toolkit. Abstract: Topic models have a rich history with various applications and have recently been reinvigorated by neural topic modeling. However, these numerous topic models adopt totally distinct datasets, implementations, and evaluations. This impedes quick utilization and fair comparisons, and thereby hinders their research progress and applications. To tackle this challenge, we in this paper propose a Topic Modeling System Toolkit (TopMost). Compared to existing toolkits, TopMost stands out by supporting more extensive features. It covers a broader spectrum of topic modeling scenarios with their complete lifecycles, including datasets, preprocessing, models, training, and evaluations. Thanks to its highly cohesive and decoupled modular design, TopMost enables rapid utilization, fair comparisons, and flexible extensions of diverse cutting-edge topic models. Our code, tutorials, and documentation are available at https://github.com/bobxwu/topmost.\n    [4] Title: The Evolution of Topic Modeling. Abstract: Topic models have been applied to everything from books to newspapers to social media posts in an effort to identify the most prevalent themes of a text corpus. We provide an in-depth analysis of unsupervised topic models from their inception to today. We trace the origins of different types of contemporary topic models, beginning in the 1990s, and we compare their proposed algorithms, as well as their different evaluation approaches. Throughout, we also describe settings in which topic models have worked well and areas where new research is needed, setting the stage for the next generation of topic models.\n    [5] Title: Apples to Apples: A Systematic Evaluation of Topic Models. Abstract: From statistical to neural models, a wide variety of topic modelling algorithms have been proposed in the literature. However, because of the diversity of datasets and metrics, there have not been many efforts to systematically compare their performance on the same benchmarks and under the same conditions. In this paper, we present a selection of 9 topic modelling techniques from the state of the art reflecting a diversity of approaches to the task, an overview of the different metrics used to compare their performance, and the challenges of conducting such a comparison. We empirically evaluate the performance of these models on different settings reflecting a variety of real-life conditions in terms of dataset size, number of topics, and distribution of topics, following identical preprocessing and evaluation processes. Using both metrics that rely on the intrinsic characteristics of the dataset (different coherence metrics), as well as external knowledge (word embeddings and ground-truth topic labels), our experiments reveal several shortcomings regarding the common practices in topic models evaluation.\n    [6] Title: Semantically-Enhanced Topic Modeling. Abstract: In this paper, we advance the state-of-the-art in topic modeling by means of the design and development of a novel (semi-formal) general topic modeling framework. The novel contributions of our solution include: (i) the introduction of new semantically-enhanced data representations for topic modeling based on pooling, and (ii) the proposal of a novel topic extraction strategy - ASToC - that solves the difficulty in representing topics in our semantically-enhanced information space. In our extensive experimentation evaluation, covering 12 datasets and 12 state-of-the-art baselines, totalizing 108 tests, we exceed (with a few ties) in almost 100 cases, with gains of more than 50% against the best baselines (achieving up to 80% against some runner-ups). We provide qualitative and quantitative statistical analyses of why our solutions work so well. Finally, we show that our method is able to improve document representation in automatic text classification.\n    [7] Title: Topic Modeling based on Keywords and Context. Abstract: Current topic models often suffer from discovering topics not matching human intuition, unnatural switching of topics within documents and high computational demands. We address these concerns by proposing a topic model and an inference algorithm based on automatically identifying characteristic keywords for topics. Keywords influence topic-assignments of nearby words. Our algorithm learns (key)word-topic scores and it self-regulates the number of topics. Inference is simple and easily parallelizable. Qualitative analysis yields comparable results to state-of-the-art models (eg. LDA), but with different strengths and weaknesses. Quantitative analysis using 9 datasets shows gains in terms of classification accuracy, PMI score, computational performance and consistency of topic assignments within documents, while most often using less topics.\n    [8] Title: Topic Modeling Using Latent Dirichlet allocation. Abstract: We are not able to deal with a mammoth text corpus without summarizing them into a relatively small subset. A computational tool is extremely needed to understand such a gigantic pool of text. Probabilistic Topic Modeling discovers and explains the enormous collection of documents by reducing them in a topical subspace. In this work, we study the background and advancement of topic modeling techniques. We first introduce the preliminaries of the topic modeling techniques and review its extensions and variations, such as topic modeling over various domains, hierarchical topic modeling, word embedded topic models, and topic models in multilingual perspectives. Besides, the research work for topic modeling in a distributed environment, topic visualization approaches also have been explored. We also covered the implementation and evaluation techniques for topic models in brief. Comparison matrices have been shown over the experimental results of the various categories of topic modeling. Diverse technical challenges and future directions have been discussed.\n    [9] Title: HTMOT: Hierarchical Topic Modelling over Time. Abstract: Topic models provide an efficient way of extracting insights from text and supporting decision-making. Recently, novel methods have been proposed to model topic hierarchy or temporality. Modeling temporality provides more precise topics by separating topics that are characterized by similar words but located over distinct time periods. Conversely, modeling hierarchy provides a more detailed view of the content of a corpus by providing topics and sub-topics. However, no models have been proposed to incorporate both hierarchy and temporality which could be beneficial for applications such as environment scanning. Therefore, we propose a novel method to perform Hierarchical Topic Modelling Over Time (HTMOT). We evaluate the performance of our approach on a corpus of news articles using the Word Intrusion task. Results demonstrate that our model produces topics that elegantly combine a hierarchical structure and a temporal aspect. Furthermore, our proposed Gibbs sampling implementation shows competitive performance compared to previous state-of-the-art methods.\n\n\n    Idea: Develop a research initiative to investigate social bias in language models by applying discourse analysis techniques. This approach will analyze narrative outputs from language models to uncover the implicit and explicit power dynamics and biases they reproduce. By focusing on presuppositions and entailments within generated texts, researchers can identify how language models reinforce hegemonic norms and marginalize minority groups. This initiative will be evaluated through a series of case studies, using qualitative analyses of generated content in multiple languages and socio-cultural contexts to measure the extent and nature of biases.\n    Class: novel\n    Review: The idea is novel because it introduces discourse analysis techniques to investigate social bias in language models, differing from the NLP mechanisms used in EFL discourse analysis[0] and Social Bias Frames[2].\n    Related Papers:\n    [0] Title: EFL discourse as cultural practice. Abstract: ABSTRACT This paper presents a culturally based discussion of the role English as a foreign language (EFL) plays in shaping the ideological positions of those involved in teaching, learning, and planning English in Israel. The English curriculum is uniform for speakers of Hebrew (the majority) and speakers of Arabic (the minority). Furthermore, the English curriculum states that EFL pedagogy should take a global rather than an Anglo-centric approach. This situation constitutes the paper\u2019s main assumption regarding the cultural content of EFL materials being ideologically oriented toward: (1) reproducing and perpetuating dominant Western hegemonic ideologies; (2) maintaining social misrepresentations and inequalities regarding the Other. By employing critical discourse analysis (CDA) to uncover Westcentric cultural biases in EFL discourse and cultural discourse studies (CDS) to reconstruct the role of Eastern and local Arab discourses in EFL pedagogies, I examine a new culturally critical way of observing EFL discourse and linking it with social change. I suggest a holistic, ecological EFL pedagogy that constructs EFL discourse as cultural practice. Promoting an EFL \u2018ecological discourse\u2019 prioritizes narratives emphasizing positive multicultural values, respect, and acceptance of the Other and encourages learners to consider cultural differences as the source for reading EFL texts about the Other.\n    [1] Title: Gender stereotypes and biases in Iranian EFL textbooks / Amir Biglarbeigi Ghajarieh. Abstract: Gender biases and ideologies are manifested through many avenues, including the representations of gendered social actors in school textbooks (Lee & Collins, 2008; Blumberg, 2008). This study examines the representations of male and female social actors within gendered discourses in Iranian EFL textbooks. These prescribed English textbooks, which enjoy a nation-wide audience, cater for the needs of Iranian EFL students at the secondary level, high school, and pre-university levels and serve as the students\u2018 initial \nintroduction to the English Language and culture. \nThe theoretical background of this study is grounded in Critical Discourse Analysis (CDA). To examine the depictions of male and female social actors within the identified gendered discourses in these textbooks, this study fused van Leeuwen\u2018s (2003) \u2017Social Actor Network Model\u2018 and Sunderland\u2018s (2004) \u2017Gendered Discourses Model\u2018. I studied a focus group of students who use Iranian EFL textbooks at different educational levels to triangulate my data. \nThis study employed Sunderland\u2018s (2006) framework together with Hall\u2018s (1980) idea of text readers\u2018 interpretations; i.e., hegemonic, oppositional and negotiated readings, to analyse the focus group\u2018s readings of sample gendered \ntexts in Iranian EFL textbooks. \nThe conservative gendered discourses provisionally identified in this study were supported to a great extend through the representations of male and female social actors in the texts and images closely related to the texts of Iranian EFL textbooks at different educational levels. Conversely, the subversive gendered discourses were resisted through such representations to a great extent. Grounded on CDA, one can argue that, the support for \nconservative gendered discourses and the resistance against subversive gendered discourses in Iranian EFL textbooks underpin gender norms and religious ideologies existing in Iran. This could serve the interests of powerful groups in Iran, maintain the status quo and ensure the dominance of powerful groups\u2018 ideologies on gender (Fairclough, 1992a, 1995, 2001, 2003; Fairclough & Wodak, 1997; van Dijk, 1993a, 1998, 2008, Wodak, 1995, 1997, 2005, 2012). \nWith regard to interview results one can argue that, the existence of hegemonic readings in the focus group of this study indicates that the hegemonic messages disseminated through Iranian EFL textbooks could produce the intended reaction in some audiences of such textbooks (Hall, 1990). The existence of oppositional readings indicates that some audiences of Iranian EFL textbooks constructed themselves different from the representations of male and female social actors depicted by their textbooks. According to van \nLeeuwen (2003), one can assert that these social actors were excluded from the representations of male and female social actors in Iranian EFL textbooks. \nSuch exclusions could obscure the reality regarding the existence of such gender identities (van Leeuwen, 2003; Sunderland, 2004). \nThe findings suggest that a truly balanced treatment of gender equality has yet to be achieved in the analysed textbooks. CDA could underlie the existence of hegemonic ideologies in educational materials and the results of studies such as the one at hand should be shared with both the academia and men and women in society at large.\n    [2] Title: Social Bias Frames: Reasoning about Social and Power Implications of Language. Abstract: Warning: this paper contains content that may be offensive or upsetting. Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people\u2019s judgments about others. For example, given a statement that \u201cwe shouldn\u2019t lower our standards to hire more women,\u201d most listeners will infer the implicature intended by the speaker - that \u201cwomen (candidates) are less qualified.\u201d Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text. We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80% F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames. Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.\n    [3] Title: A cosy consensus on deviant discourse: How the refugee and asylum seeker meta-narrative has endorsed an interpretative crisis in relation to the transnational politics of the world's displaced persons.. Abstract: Immigration is a key feature in late capitalist societies, with some 20,000,000 displaced persons worldwide. This paper reports on coverage of refugees and asylum seekers in English-language newspapers worldwide, drawing on media content between 2003 and 2004. It analyses media discourse on refugees and asylum seekers across the world, with a particular focus on deconstructing negative coverage. Five dominant negative frames in international media discourses are identified. These themes are examined in the context of theories of racism and xenophobia to highlight their negative potential for displaced persons and attitudes towards them in their host countries. Theory is also employed to explore the potential utility of such negative narratives for the media and social elites. The work being presented here is part of a much larger research project being undertaken by the authors at the University of Limerick. (For preliminary findings see Devereux and Breen, 2003 and 2004). Introduction The role of the media in perpetuating the \u2018otherness\u2019 of refugees and asylum seekers is the focus of this paper. The concept of the other and our understanding of its construction and function are applied to the empirical example of international press coverage of refugee and asylum seekers. Using this data source, we explore the role of the media in excluding refugees and asylum seekers through the production and reproduction of othering discourses. Specifically, we deconstruct negative media discourses to demonstrate their basis in notions of otherness and threat. We also acknowledge the existence of more positive coverage and the potential of the media to promote inclusion. This paper begins with a discussion of key concepts in theories of racism and xenophobia, which inform our analysis. In section two, the linkages between these concepts and media theory are outlined. The methodology informing this paper and the wider research project of which it is part are then elaborated. The five key negative frames identified within international media discourse are subsequently outlined; asylum seekers are represented as \u2018an economic threat\u2019; \u2018a threat to national and local integrity\u2019; \u2018a criminal element\u2019; \u2018social deviants\u2019 and as \u2018illegal aliens\u2019. Finally, our nascent conclusions regarding media constructions of asylum seekers and refugees are presented. The Social Function of Fear Fear is a powerful emotion. At a primeval level, fear can draw people together, seeking safety in numbers. The group that faces a common threat often benefits in terms of cohesion. Thus, historically a common fear \u2013 of flood, of a hard winter, of wild animals has often had positive ramifications for communities, strengthening bonds that might otherwise have been weak, creating interdependence where there might instead have been anomie, establishing a community where instead there would only have been individuals. Our innate understanding of its binding power, is reflected in manufactured fears emerging from the common consciousness \u2013 religion and the paranormal providing many examples which are experienced in parallel with genuine threats, filling a void should one exist or perhaps providing a more manageable source of fear in place of the too frightening uncontrollability of external and natural forces. Those in leadership roles have been particularly cognisant of the benefits of a common threat. They have long recognised its potential not only for creating cohesion, but also for preventing dissent and in-fighting \u2013 phenomena which not only have a negative impact upon community, but also upon a leader\u2019s hold over power. Thus, fears have also been manufactured by elites to generate necessary or desired cohesion. Closely related to the common threat is scapegoating. Cohesion is maintained through the creation of a scapegoat, which can be blamed for negative happenings. Scapegoats can prevent a group having to face its own flaws \u2013 in The Burning of Bridget Cleary, for example, Angela Bourke (1999) identifies fairies as a device used by the community to apportion blame for socially unacceptable events. A communities\u2019 complicit acceptance of a supernatural explanation for a spouse\u2019s disappearance could prevent the necessity of addressing the existence of illicit activity in their midst and the flawed nature of their members. Bourke\u2019s (1999) analysis also evokes an understanding of the function of scapegoating for disempowered groups. For the dependent and powerless, blaming the fairies for domestic violence may have been easier than challenging a seemingly immutable patriarchal structure. In contemporary society also, it often seems preferable for the powerless to misapportion blame rather than face an immutable opponent. False class-consciousness is a powerful example of this. Hegemony is achieved and sustained by an ideology that emphasises that everybody can \u2018make it\u2019 in material terms if they choose to. Barry Glassner (2003), in his seminal text Culture of Fear, emphasises that misinformation rather than consciously misplaced blame may, however, be the key to understanding widespread scapegoating. Elites who which to divert attention from their corruption and errors and beneficiaries of inequality who wish to maintain the status quo all benefit from, if not generating, at least leaving unchallenged, misinformation and myths. Glassner\u2019s (2003) text provides clear examples of the reciprocal relationship between the common threat and the scapegoat. The construction of an external threat often employs the devise of scapegoating to demonstrate the immediacy and relevance of the danger, or to manufacture a threat where there exist only uncharted waters, while the scapegoat as a source of blame is inherently viewed as threatening. Both require the creation of a narrative, which elaborates to the in-group members whom they should fear and why. Prejudice, viewed as a social norm to which individuals conform, has been theorised as having its roots not in individual personality disorder, but in a societal need to scapegoat. According to this theory hostility towards minorities is a result of a social system in which: \"the achievement level of large sections of the population falls short of the normatively sanctioned aspiration level.\" (Hartman and Husband, 1974: 45) Thus someone who has high ethnic status (i.e. is white) and a high educational status, but has a low occupational status, may use a minority group as a scapegoat to explain their status inconsistency for instance they might claim that immigrants were 'taking all the jobs'. Discrimination, the manifest expression of prejudice, is also not simply a number of isolated actions, which favour one group over another. It is a system of social relations, both individual and institutional that places and maintains power in the hands of the majority. Discrimination may mean denial of access to wealth, education, legal and social protection. It can also mean forced assimilation or segregation (Hartmann and Husband, 1974). In this paper we are focused upon a situation in which the subject of the narrative, the source of the manufactured or artificially inflated threat, the scapegoat, is not a figment of fairytale, but human beings, in this case asylum seekers and refugees. We identify and deconstruct media coverage which create this group as a threat. Our aim is to highlight the congruence between the various elements of these narratives and scapegoating, in order to add to our understanding of how such threats are made palpable and acute for the public. Through this undertaking, we also aim to highlight the enormous negative potential of such discourses for the human beings who are their subject. Bad News Sells ... General Panics Run and Run The \u2018knowledge gap\u2019 between host populations and displaced persons is key to understanding the significance of media coverage in formulating public attitudes towards asylum seekers and refugees. In addition to the general climate of social distance, which exists between the majority and minorities, asylum seekers often live lives very distant from those of host populations. Opportunities to interact in the same social space are often limited by restrictions on the right to work, where to live and financial deprivation resulting from limited welfare. In other cases, the opportunity to occupy even the same physical space are limited by detention, placement in remote locations and restriction to communal living centres. In Ireland, for example, it was suggested that asylum seekers be housed on specially converted ships on the river Suir in County Waterford. Foucault\u2019s \u2018Ship of Fools\u2019 was to become a \u2018Ship of Asylum Seekers\u2019. While refugees are not subject to such spatial segregation, they may experience social segregation as a consequences of racial prejudice and discrimination. Whether rooted in spatial or social segregation, the resultant social distance between host populations and asylum seekers and refugees leaves the former with few (respected) routes of learning about the latter, beyond the media. Although the public may not unquestioningly accept the perspective of the media, lacking an alternative, the power of the media\u2019s message is indisputable. According to Benjamin D. Singer: \"..it may be argued that the potential for identifiable symbolic content through the media is probably greater than direct contact with the native people (themselves)..\" (Singer, 1983: 234) In Racism and the Mass Media: A study of the role of the mass media in the formation of white beliefs and attitudes in Britain, Hartmann and Husband (1974) as a prelude to a content analysis of press coverage similar to that elaborated here, examined people\u2019s attitudes to and definitions of the racial situation in Britain. They concluded that people\u2019s situations, the attitudes prevailing in their local area and the numbers of non-white people living there, determined affective and evaluative attitudes in combi\n    [4] Title: Lexicalisation in News Stories of Some Nigerian National Newspapers. Abstract: There has not been much research on unmasking the i deological bias embedded in the language of the seemingly objective representations f people, events, institutions, and policies in Nigerian newspapers. Thus, this study e xplores how lexical choices are used to simultaneously convey information and judgment on p eople, events, and policies in Nigerian newspaper news reports. The study applies critical discourse analysis to analyse selected news stories relating to politics, economy, securit y, sports, health, and religion, from four Nigerian national daily newspapers ( The Guardian, The Nation, Nigerian Tribune, and Daily Trust). Findings of the study reveal that Nigerian newsp aper reporters\u2019 choice of lexical patterns produces differential judgmental stances w hich have some control on the attitudes and actions of readers towards the people, events a nd policies represented. The findings also depict that Nigerian newspapers often represent peo ple, events, and policies through sensational but very blatantly biased, inflammatory and pejorative lexical items, which undoubtedly reveal the reporters\u2019 desperation to pr opagandise, sensationalise, binarise, or impose certain meanings upon the webs of complexity in news discourse in Nigerian newspapers. Thus, the study recommends that Nigeria n journalists, who are products of a nation-state that is confronted with many problems, should strive to be fair, balanced, and restrained in their deployment of representational resources. LUMINA, Vol. 23, No.2, ISSN 2094-1188 Introduction In Nigerian media discourse, the reporter uses a wi de variety of representational resources in doing his or her job. Some of the ling uistic structures, pragmatic codings, styles of presentation, and visual imagery are his or her own choosing while others are re-enacted or reproduced to appropriately represent the ideas, in tentions, power, and emotion of the agents or news makers (who do not hesitate to express thei r pr judice and discrimination on some issues, societies or groups) in the news discourse. Thus, the reporter\u2019s language, style, and visual images at times affect, shape and control th e attitude, opinion and thinking of the audience about some people, policies, events and si tuations. The problem of the re-enactment of power, prejudice and discrimination in Nigerian print media discourse can be said to be capable of promoting social, political and economic discrimination in the country because newspapers have enormous and lasting influence, lan guage being the main tool of this influence (Smirnova, 2009: 80). Newspapers can be r ead and re-read by audience; they may be shared with friends, family members and workmate s; and they may be photocopied and archived in order to have permanent access to them. This is why, according to Liu (2009: 60), the last few decades have witnessed a great deal of research on print media representations from different perspectives. The newspaper is a text that exemplifies language use in a social context. It is a multimodal or multisemiotic text. It is polythemati c and involves a lot of societal problems and issues. It uses linguistic, discourse and visua l resources in representing (or misrepresenting) people, events, opinions, and poli cies. The choice and use of these linguistic resources are not in all cases the journalists\u2019 cre ation but are dictated by the values and norms of their institution (Pan, 2002: 51). In other word s, lexicalisation and imaging in media discourse are rooted in some specific cultures and ideologies as discourse itself is ideology (van Dijk, 1991). Media representations through lan guage and images are often found to be discrepant from so-called objective reality (Teo, 2 000). Such representations, however, determine public perceptions (or misperception) of s me people, sects, events, and policies are capable of distorting audience\u2019s attitudes, opi ni ns, and actions towards other people, societies, and events. Media researchers claim that media representations influence people\u2019s perceptions of reality. In other words, individuals learn about events, people, and issues from the media and they react based on this knowledge (S otirovic, 2001). LUMINA, Vol. 23, No.2, ISSN 2094-1188 Although there have been several critical studies o n media discourse in the Western countries, there are very few of such in Nigeria. T he few ones available focused primarily on interesting devices in Nigerian newspaper reporting , using methodologies and analytical insights of error analysis, text linguistics, genre analysis, stylistics, sociolinguistics, pragmatics, and semiotics. Duncan (2006) employs di scourse analysis in his assessment of language and discourse trends and themes which mani fest in print media representation of Black South Africans in the fading days of aparthei d. His findings reveal that the overriding discourse was that of Blacks as violent, untrustwor thy, subhuman, racist, and unreasonable. He also reports the near-complete absence of simila r emotive descriptors in news stories depicting Whites as perpetrators of violence. Oloru nnisola (2006) is also a study that uses discourse analysis to unravel the language and disc our e of racism in some local and international newspapers in South Africa. The study clearly confirms the existence of a typical separation of powers along racial lines typ ified. He observes that there are a few instances of language of conflict and discriminatio n n some news stories in a way that readers are left with little doubt that there are d ivisions along racial lines. Nevertheless, Duncan (2006) and Olorunnisola (2006) address only e social problem racism. Yin (2007) combines the analysis of media texts and u ience interpretations to explore the extent towhich the media narratives con strain audience interpretations and the extent to which audience members can resistthe pref rred meanings presented by the media. His study affirms that media texts serve as mate-na rratives foraudience interpretations. Even when audience members are able to resist the ideolo gy c nveyed by one type ofmedia, they rely on the dominant meanings of another. Caldas-Co ulthard and Moon (2010) use corpus methodology as a research tool to investigate how s cial actors are classified in the public discourse of the media with lexis as a point of ent ry. Their findings indicate that uses of premodification associated with the two newspapers in Br tain and their lexical choices produce differential judgmental stances that have some soci al effects. They also observe that the media categorize people through very specific point f view and values not always apparent to the uncritical audience. This indicates that med ia all over the world craftily often take advantage of narrative as a powerful tool to influe nce the audience or determine their thoughts about people. Ismail (2010) investigates m ainstream U.S. newspaper discourses concerning the dividing wall that Israel built as a eparation barrier from the West Bank. He LUMINA, Vol. 23, No.2, ISSN 2094-1188 explores news media\u2019s role as agents of social cont rol and influence and observes that while the media\u2019s role in social control and influence ma y be significant, the adequacy with which they perform this role is questionable. Closely rel at d to the above, Pounds (2010) uses critical linguistic analysis to investigate authori al stance in English news reporting and the nature of authorial voice itself. He observes that expressive resources and reporting style in Italian news reporting are often loaded with prejud ice.\n    [5] Title: Progressive planning, biased discourse, and critique: An agentic perspective. Abstract: Progressive theorists and reflective practitioners have exhorted agents to renounce exploitative planning discourses. This repudiation can, however, only be successful if the agent\u2019s own investment in biased discourses is accounted for. Reflecting on the work of Jon Elster, John Thompson, and Raymond Geuss, it is argued that the progressive planner should direct agents toward an acknowledgement of the motives compelling them to become invested in biased discourse, and how these discourses satisfy these motives by capitalizing on the trappings of habitual modes of reasoning and thriving on the ambiguity of referents. The value of this perspective is illustrated in an analysis of neoliberalism as a contemporary hegemonic discourse. It is discussed how popular anxieties about social mobility and community power motivate the investment in discourses characterized by a thesis of the inevitability of neoliberal restructuring and associations with narratives on entrepreneurialism, multiculturalism, and self-help.\n    [6] Title: Carnivalising Postcolonial Zimbabwe: The Vulgar and Grotesque Logic of Postcolonial Protest in NoViolet Bulawayo's We Need New Names (2013). Abstract: Summary This article set out to explore NoViolet Bulawayo's We need new names from the perspective of carnivalised writing. The objectives of the article were to unpack how the vulgar and the grotesque were used to create carnival moments in the narrative and to examine how marginal subjects gain voice and some degree of power to live an alternative life, even if this is momentary. It sought to examine how Bulawayo derives her aesthetics from the vulgar and the grotesque to create a carnivalesque logic that informs postcolonial protest in the novel. The analysis made use of the theoretical concepts of carnival propounded by Mikhail Bakhtin. This article argues that the text is constituted by a regime of the vulgar, which the child characters deploy for transgressing hegemonic practices and authoritative discourses. Social norms are suspended and the children have a subversive agency, courtesy of parody and satire. The article reveals that apart from speaking back to power, the children harness the image of kaka (human excrement) as a discursive resource to satirise the failures of the Zimbabwean postcolony and to degrade all forms of authority. It is concluded that while the scatological in the novel suggests social indictment, the images of kaka and dirt fail to transcend protest to see the realisation of a desired postcolonial condition.\n    [7] Title: Comunicaci\u00f3n y representaci\u00f3n: an\u00e1lisis de la construcci\u00f3n sociocultural de los modelos de feminidad en la revista Hogar. Abstract: This study makes a discursive analysis around the social construction of the femininity in the contents of the Hogar magazine, in order to recognize and highlight a hegemonic model of woman. The categories wich make and essential part of development of this work are: representation, gender, culture, communication and narrative; the review of each one of them represents a theorical contribution to work on the discourses that are produced and exposed to everybody in an Ecuadorian women's magazine. The discursive content selected for analysis in this study are the sing of biased view of the woman, wich has not changed significantly over time, but it also presents women who are \u201cfully incuded in the life of the country\u201d only through reduced numbers of labor inclusion, educational and of growth of female-headed households in families.\n    [8] Title: nan. Abstract: nan\n    [9] Title: nan. Abstract: nan\n\n\n    Idea: Develop a **natural language processing classifier designed to improve scientific paper revisions** by automatically identifying and categorizing reviewer comments that are most likely to lead to substantial and actionable revisions. The system would be trained on a **manually-labeled dataset analysis** of scientific review comments and the corresponding paper edits, leveraging features such as linguistic cues, sentiment, and comment specificity to predict the likelihood of a comment being acted upon. This classifier could then be used to prioritize reviewer feedback, helping authors focus on the most impactful suggestions first.\n    Class: novel\n    Review: The idea is novel because it uniquely focuses on prioritizing reviewer comments for actionable revisions, which is not explicitly addressed in ARIES[0] or other related works like ReviVal[9].\n    Related Papers:\n    [0] Title: ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews. Abstract: We introduce the task of automatically revising scientific papers based on peer feedback and release ARIES, a dataset of review comments and their corresponding paper edits. The data is drawn from real reviewer-author interactions from computer science, and we provide labels linking each reviewer comment to the specific paper edits made by the author in response. We automatically create a high-precision silver training set, as well as an expert-labeled test set that shows high inter-annotator agreement. In experiments with 10 models covering the state of the art, we find that they struggle even to identify which edits correspond to a comment -- especially when the relationship between the edit and the comment is indirect and requires reasoning to uncover. We also extensively analyze GPT-4's ability to generate edits given a comment and the original paper. We find that it often succeeds on a superficial level, but tends to rigidly follow the wording of the feedback rather than the underlying intent, and lacks technical details compared to human-written edits.\n    [1] Title: Can large language models provide useful feedback on research papers? A large-scale empirical analysis. Abstract: Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR machine learning conference (1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature journals, 39.23% for ICLR) is comparable to the overlap between two human reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The overlap between GPT-4 and human reviewers is larger for the weaker papers. We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half (57.4%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations.\n    [2] Title: A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications. Abstract: Peer reviewing is a central component in the scientific publishing process. We present the first public dataset of scientific peer reviews available for research purposes (PeerRead v1),1 providing an opportunity to study this important artifact. The dataset consists of 14.7K paper drafts and the corresponding accept/reject decisions in top-tier venues including ACL, NIPS and ICLR. The dataset also includes 10.7K textual peer reviews written by experts for a subset of the papers. We describe the data collection process and report interesting observed phenomena in the peer reviews. We also propose two novel NLP tasks based on this dataset and provide simple baseline models. In the first task, we show that simple models can predict whether a paper is accepted with up to 21% error reduction compared to the majority baseline. In the second task, we predict the numerical scores of review aspects and show that simple models can outperform the mean baseline for aspects with high variance such as \u2018originality\u2019 and \u2018impact\u2019.\n    [3] Title: arXivEdits: Understanding the Human Revision Process in Scientific Writing. Abstract: Scientific publications are the primary means to communicate research discoveries, where the writing quality is of crucial importance. However, prior work studying the human editing process in this domain mainly focused on the abstract or introduction sections, resulting in an incomplete picture. In this work, we provide a complete computational framework for studying text revision in scientific writing. We first introduce arXivEdits, a new annotated corpus of 751 full papers from arXiv with gold sentence alignment across their multiple versions of revision, as well as fine-grained span-level edits and their underlying intentions for 1,000 sentence pairs. It supports our data-driven analysis to unveil the common strategies practiced by researchers for revising their papers. To scale up the analysis, we also develop automatic methods to extract revision at document-, sentence-, and word-levels. A neural CRF sentence alignment model trained on our corpus achieves 93.8 F1, enabling the reliable matching of sentences between different versions. We formulate the edit extraction task as a span alignment problem, and our proposed method extracts more fine-grained and explainable edits, compared to the commonly used diff algorithm. An intention classifier trained on our dataset achieves 78.9 F1 on the fine-grained intent classification task. Our data and system are released at tiny.one/arxivedits.\n    [4] Title: Characterizing Text Revisions to Better Support Collaborative. Abstract: Despite advancement in collaborative writing tools, the track changes capability in modern editors remains limited to highlighting syntactic changes, with authors still required to manually read through each of the revisions. We envision a collaborative authoring system where an author could accept all minor edits first and then focus on the substantial changes. To support this, we define the task of significant revision identification as the task of identifying the revisions between two versions of a text according to one of four categories, i.e. formal, meaning preserving, micro- and macro-structure. Micro-structure change corresponds to minor meaning change while macro-structure change corresponds to major meaning change. Our main contribution is to define a computational approach to this task, by framing the task as bi-directional entailment between the original and revised sentences. An existing recognition of textual entailment (RTE) system is applied to evaluate whether the revised texts entails. We evaluate the approach through a novel corpus consisting of multiple versions of drafts of academic papers written by multiple authors, which were annotated with the four revision types by both authors and non-authors of the papers. The proposed bi-directional textual entailment approach performs better than baseline edit distance approaches, which is similar to the current track changes capability built into most word processors.\n    [5] Title: Can We Automate Scientific Reviewing?. Abstract: The rapid development of science and technology has been accompanied by an exponential growth in peer-reviewed scientific publications. At the same time, the review of each paper is a laborious process that must be carried out by subject matter experts. Thus, providing high-quality reviews of this growing number of papers is a significant challenge. In this work, we ask the question \u201ccan we automate scientific reviewing? \u201d, discussing the possibility of using natural language processing (NLP) models to generate peer reviews for scientific papers. Because it is non-trivial to define what a \u201cgood\u201d review is in the first place, we first discuss possible evaluation metrics that could be used to judge success in this task. We then focus on the machine learning domain and collect a dataset of papers in the domain, annotate them with different aspects of content covered in each review, and train targeted summarization models that take in papers as input and generate reviews as output. Comprehensive experimental results on the test set show that while system-generated reviews are comprehensive, touching upon more aspects of the paper than human-written reviews, the generated texts are less constructive and less factual than human-written reviews for all aspects except the explanation of the core ideas of the papers, which are largely factually correct. Given these results, we pose eight challenges in the pursuit of a good review generation system together with potential solutions, which, hopefully, will inspire more future research in this direction.\nWe make relevant resource publicly available for use by future research: https://github. com/neulab/ReviewAdvisor. In addition, while our conclusion is that the technology is not yet ready for use in high-stakes review settings we provide a system demo, ReviewAdvisor (http://review.nlpedia.ai/), showing the current capabilities and failings of state-of-the-art NLP models at this task (see demo screenshot in A.2). A review of this paper written by the system proposed in this paper can be found in A.1.\n    [6] Title: DeepReviewer: Collaborative Grammar and Innovation Neural Network for Automatic Paper Review. Abstract: Nowadays, there are more and more papers submitted to various periodicals and conferences. Typically, reviewers need to read through the paper and give a review comment and score to it based on somehow certain criterion. This review process is labor intensive and time-consuming. Recently, AI technology is widely used to alleviate human labor burden. Can machine learn from human to review papers automatically? In this paper, we propose a collaborative grammar and innovation model - DeepReviewer to achieve automatic paper review. This model learning the semantic, grammar and innovative features of an article by three main well-designed components simultaneously. Moreover, these three factors are integrated by an attention layer to get the final review score of the paper. We crawled paper review data from Openreview and built a real data set. Experimental results demonstrate that our model exceeds many baselines.\n    [7] Title: Aspect-based Sentiment Analysis of Scientific Reviews. Abstract: Scientific papers are complex and understanding the usefulness of these papers requires prior knowledge. Peer reviews are comments on a paper provided by designated experts on that field and hold a substantial amount of information, not only for the editors and chairs to make the final decision, but also to judge the potential impact of the paper. In this paper, we propose to use aspect-based sentiment analysis of scientific reviews to be able to extract useful information, which correlates well with the accept/reject decision. While working on a dataset of close to 8k reviews from ICLR, one of the top conferences in the field of machine learning, we use an active learning framework to build a training dataset for aspect prediction, which is further used to obtain the aspects and sentiments for the entire dataset. We show that the distribution of aspect-based sentiments obtained from a review is significantly different for accepted and rejected papers. We use the aspect sentiments from these reviews to make an intriguing observation, certain aspects present in a paper and discussed in the review strongly determine the final recommendation. As a second objective, we quantify the extent of disagreement among the reviewers refereeing a paper. We also investigate the extent of disagreement between the reviewers and the chair and find that the inter-reviewer disagreement may have a link to the disagreement with the chair. One of the most interesting observations from this study is that reviews, where the reviewer score and the aspect sentiments extracted from the review text written by the reviewer are consistent, are also more likely to be concurrent with the chair's decision.\n    [8] Title: Aspect-based sentiment analysis of online peer re \u9104 views and prediction of paper acceptance results. Abstract: Peer\ufffdreviews\ufffdof\ufffdacademic\ufffdarticles\ufffdcontain\ufffdreviewers'\ufffdoverall\ufffdimpressions\ufffdand\ufffdspecific\ufffdcomments\ufffd on\ufffd the\ufffd contributed\ufffd articles,\ufffd which\ufffd have\ufffd a\ufffd lot\ufffd of\ufffd sentimental\ufffd information.\ufffd By\ufffd exploring\ufffd the\ufffd fine-grained sentiments\ufffd in\ufffd peer\ufffd reviews,\ufffd we\ufffd can\ufffd discover\ufffd critical\ufffd aspects\ufffd of\ufffd interest\ufffd to\ufffd the\ufffd reviewers.\ufffdThe\ufffdresults\ufffdcan\ufffdalso\ufffdassist\ufffdeditors\ufffdand\ufffdchairmen\ufffdin\ufffdmaking\ufffdfinal\ufffddecisions.\ufffdHowever,\ufffd currentresearchontheaspectsofpeerreviewsiscoarse-grained,\ufffd and\ufffd mostly\ufffd focuses\ufffd on\ufffd the\ufffd overall\ufffd evaluation\ufffd of\ufffd the\ufffd review\ufffd objects.\ufffd Therefore,\ufffd this\ufffd paper\ufffd constructs\ufffd a\ufffd multi-level fine-grainedaspectsetofpeerreviews for\ufffd further\ufffd study.\ufffd First,\ufffd this\ufffd paper\ufffd uses\ufffd the\ufffd multi-level aspect\ufffdextraction\ufffdmethod\ufffdto\ufffdextract\ufffdthe\ufffdaspects\ufffdfrom\ufffdpeer\ufffdreviews\ufffdof\ufffdICLR\ufffdconference\ufffdpapers.\ufffd Comparative\ufffdexperiments\ufffdconfirm\ufffdthe\ufffdvalidity\ufffdof\ufffdthe\ufffdmethod.\ufffdSecondly,\ufffdvarious\ufffdDeep\ufffdLearning\ufffd models\ufffdare\ufffdused\ufffdto\ufffdclassify\ufffdaspects'\ufffdsentiments\ufffdautomatically,withLCFS-BERTperformingbest.\ufffd By\ufffd calculating\ufffd the\ufffd correlation\ufffd between\ufffd sentimental\ufffd scores\ufffd of\ufffd the\ufffd review\ufffd aspects\ufffd and\ufffd the\ufffd acceptance\ufffdresult\ufffdof\ufffdpapers,\ufffdwe\ufffdcan\ufffdfind\ufffdthe\ufffdimportant\ufffdaspects\ufffdaffecting\ufffdacceptance.\ufffdFinally,\ufffdthis\ufffd paper\ufffdpredicts\ufffdacceptance\ufffdresults\ufffdof\ufffdpapers\ufffd(accepted/rejected)\ufffdaccording\ufffdto\ufffdthe\ufffdpeer\ufffdreviews.\ufffd The\ufffdoptimal\ufffdacceptance\ufffdprediction\ufffdmodel\ufffdis\ufffdXGboost,\ufffdachieving\ufffda\ufffdMacro_F 1 \ufffdscore\ufffdof\ufffd87.43%.\n    [9] Title: ReviVal: Towards Automatically Evaluating the Informativeness of Peer Reviews. Abstract: The peer-review process is currently under stress due to the increasingly large number of submissions to top-tier venues, especially in Artificial Intelligence (AI) and Machine Learning (ML). Consequently, the quality of peer reviews is under question, and dissatisfaction among authors is not uncommon but rather prominent. In this work, we propose \"ReviVal\" (expanded as \"REVIew eVALuation\"), a system to automatically grade a peer-review report for its informativeness. We define review informativeness in terms of its Exhaustiveness and Strength, where Exhaustiveness signifies how exhaustively the review covers the different sections and qualitative aspects1 of the paper and Strength signifies how sure the reviewer is of their evaluation. We train ReviVal, a multitask deep network for review informativeness prediction on the publicly available peer reviews, which we curate from the openreview2 platform. We annotate the review sentence(s) with labels for (a) which sections and (b) what quality aspects of the paper those refer. We automatically annotate our data with the reviewer\u2019s sentiment intensity to capture the reviewer\u2019s conviction. Our approach significantly outperforms several intuitive baselines for this novel task. To the best of our knowledge, our work is a first-of-its-kind to automatically estimate the informativeness of a peer review report.\n\n\n    Idea: Create a **decision support system** that leverages **adaptive interfaces and interaction design** to present **ai explanations generated with different techniques** in a user-friendly manner. This system will dynamically adjust the presentation of AI explanations based on user behavior and confidence levels, ensuring that users receive the right amount of information when needed. The system's effectiveness will be assessed through **mixed-method user studies**, examining its impact on decision accuracy, user trust, and overall team performance in various collaborative tasks.\n    Class: novel\n    Review: The idea is novel because it introduces the use of user confidence levels to dynamically adjust AI explanations, a feature not present in related works.\n    Related Papers:\n    [0] Title: On Selective, Mutable and Dialogic XAI: a Review of What Users Say about Different Types of Interactive Explanations. Abstract: Explainability (XAI) has matured in recent years to provide more human-centered explanations of AI-based decision systems. While static explanations remain predominant, interactive XAI has gathered momentum to support the human cognitive process of explaining. However, the evidence regarding the benefits of interactive explanations is unclear. In this paper, we map existing findings by conducting a detailed scoping review of 48 empirical studies in which interactive explanations are evaluated with human users. We also create a classification of interactive techniques specific to XAI and group the resulting categories according to their role in the cognitive process of explanation: \"selective\", \"mutable\" or \"dialogic\". We identify the effects of interactivity on several user-based metrics. We find that interactive explanations improve perceived usefulness and performance of the human+AI team but take longer. We highlight conflicting results regarding cognitive load and overconfidence. Lastly, we describe underexplored areas including measuring curiosity or learning or perturbing outcomes.\n    [1] Title: Human-Centered Evaluation of Explanations in AI-Assisted Decision-Making. Abstract: AI explanations have been increasingly used to help people better utilize AI recommendations in AI-assisted decision making. While numerous technical transparency approaches have been established, a human-centered perspective is needed for understanding how human decision makers use and process AI explanations. In my thesis, I start with an empirical exploration of how AI explanations shape the way people understand and utilize AI decision aids. Next, I move to the time\u2011evolving nature of AI explanations, exploring how explanation changes due to AI model updates affect human decision makers\u2019 perception and usage of AI models. Lastly, I construct computational human behavior models to gain a more quantitative understandings of human decision makers\u2019 cognitive interactions with AI explanations. I conclude with future work on carefully identifying user needs for explainable AI in an era when AI models are becoming more complex and human-AI collaboration scenarios are increasingly diversified.\n    [2] Title: Selective Explanations: Leveraging Human Input to Align Explainable AI. Abstract: While a vast collection of explainable AI (XAI) algorithms has been developed in recent years, they have been criticized for significant gaps with how humans produce and consume explanations. As a result, current XAI techniques are often found to be hard to use and lack effectiveness. In this work, we attempt to close these gaps by making AI explanations selective ---a fundamental property of human explanations---by selectively presenting a subset of model reasoning based on what aligns with the recipient's preferences. We propose a general framework for generating selective explanations by leveraging human input on a small dataset. This framework opens up a rich design space that accounts for different selectivity goals, types of input, and more. As a showcase, we use a decision-support task to explore selective explanations based on what the decision-maker would consider relevant to the decision task. We conducted two experimental studies to examine three paradigms based on our proposed framework: in Study 1, we ask the participants to provide critique-based or open-ended input to generate selective explanations (self-input). In Study 2, we show the participants selective explanations based on input from a panel of similar users (annotator input). Our experiments demonstrate the promise of selective explanations in reducing over-reliance on AI and improving collaborative decision making and subjective perceptions of the AI system, but also paint a nuanced picture that attributes some of these positive effects to the opportunity to provide one's own input to augment AI explanations. Overall, our work proposes a novel XAI framework inspired by human communication behaviors and demonstrates its potential to encourage future work to make AI explanations more human-compatible.\n    [3] Title: The Impact of Imperfect XAI on Human-AI Decision-Making. Abstract: Explainability techniques are rapidly being developed to improve human-AI decision-making across various cooperative work settings. Consequently, previous research has evaluated how decision-makers collaborate with imperfect AI by investigating appropriate reliance and task performance with the aim of designing more human-centered computer-supported collaborative tools. Several human-centered explainable AI (XAI) techniques have been proposed in hopes of improving decision-makers' collaboration with AI; however, these techniques are grounded in findings from previous studies that primarily focus on the impact of incorrect AI advice. Few studies acknowledge the possibility of the explanations being incorrect even if the AI advice is correct. Thus, it is crucial to understand how imperfect XAI affects human-AI decision-making. In this work, we contribute a robust, mixed-methods user study with 136 participants to evaluate how incorrect explanations influence humans' decision-making behavior in a bird species identification task, taking into account their level of expertise and an explanation's level of assertiveness. Our findings reveal the influence of imperfect XAI and humans' level of expertise on their reliance on AI and human-AI team performance. We also discuss how explanations can deceive decision-makers during human-AI collaboration. Hence, we shed light on the impacts of imperfect XAI in the field of computer-supported cooperative work and provide guidelines for designers of human-AI collaboration systems.\n    [4] Title: Watch Out for Updates: Understanding the Effects of Model Explanation Updates in AI-Assisted Decision Making. Abstract: AI explanations have been increasingly used to help people better utilize AI recommendations in AI-assisted decision making. While AI explanations may change over time due to updates of the AI model, little is known about how these changes may affect people\u2019s perceptions and usage of the model. In this paper, we study how varying levels of similarity between the AI explanations before and after a model update affects people\u2019s trust in and satisfaction with the AI model. We conduct randomized human-subject experiments on two decision making contexts where people have different levels of domain knowledge. Our results show that changes in AI explanation during the model update do not affect people\u2019s tendency to adopt AI recommendations. However, they may change people\u2019s subjective trust in and satisfaction with the AI model via changing both their perceived model accuracy and perceived consistency of AI explanations with their prior knowledge.\n    [5] Title: Understanding the Role of Human Intuition on Reliance in Human-AI Decision-Making with Explanations. Abstract: AI explanations are often mentioned as a way to improve human-AI decision-making, but empirical studies have not found consistent evidence of explanations' effectiveness and, on the contrary, suggest that they can increase overreliance when the AI system is wrong. While many factors may affect reliance on AI support, one important factor is how decision-makers reconcile their own intuition---beliefs or heuristics, based on prior knowledge, experience, or pattern recognition, used to make judgments---with the information provided by the AI system to determine when to override AI predictions. We conduct a think-aloud, mixed-methods study with two explanation types (feature- and example-based) for two prediction tasks to explore how decision-makers' intuition affects their use of AI predictions and explanations, and ultimately their choice of when to rely on AI. Our results identify three types of intuition involved in reasoning about AI predictions and explanations: intuition about the task outcome, features, and AI limitations. Building on these, we summarize three observed pathways for decision-makers to apply their own intuition and override AI predictions. We use these pathways to explain why (1) the feature-based explanations we used did not improve participants' decision outcomes and increased their overreliance on AI, and (2) the example-based explanations we used improved decision-makers' performance over feature-based explanations and helped achieve complementary human-AI performance. Overall, our work identifies directions for further development of AI decision-support systems and explanation methods that help decision-makers effectively apply their intuition to achieve appropriate reliance on AI.\n    [6] Title: Are Explanations Helpful? A Comparative Study of the Effects of Explanations in AI-Assisted Decision-Making. Abstract: This paper contributes to the growing literature in empirical evaluation of explainable AI (XAI) methods by presenting a comparison on the effects of a set of established XAI methods in AI-assisted decision making. Specifically, based on our review of previous literature, we highlight three desirable properties that ideal AI explanations should satisfy\u2014improve people\u2019s understanding of the AI model, help people recognize the model uncertainty, and support people\u2019s calibrated trust in the model. Through randomized controlled experiments, we evaluate whether four types of common model-agnostic explainable AI methods satisfy these properties on two types of decision making tasks where people perceive themselves as having different levels of domain expertise in (i.e., recidivism prediction and forest cover prediction). Our results show that the effects of AI explanations are largely different on decision making tasks where people have varying levels of domain expertise in, and many AI explanations do not satisfy any of the desirable properties for tasks that people have little domain expertise in. Further, for decision making tasks that people are more knowledgeable, feature contribution explanation is shown to satisfy more desiderata of AI explanations, while the explanation that is considered to resemble how human explain decisions (i.e., counterfactual explanation) does not seem to improve calibrated trust. We conclude by discussing the implications of our study for improving the design of XAI methods to better support human decision making.\n    [7] Title: Effects of Explanations in AI-Assisted Decision Making: Principles and Comparisons. Abstract: Recent years have witnessed the growing literature in empirical evaluation of explainable AI (XAI) methods. This study contributes to this ongoing conversation by presenting a comparison on the effects of a set of established XAI methods in AI-assisted decision making. Based on our review of previous literature, we highlight three desirable properties that ideal AI explanations should satisfy \u2014 improve people\u2019s understanding of the AI model, help people recognize the model uncertainty, and support people\u2019s calibrated trust in the model. Through three randomized controlled experiments, we evaluate whether four types of common model-agnostic explainable AI methods satisfy these properties on two types of AI models of varying levels of complexity, and in two kinds of decision making contexts where people perceive themselves as having different levels of domain expertise. Our results demonstrate that many AI explanations do not satisfy any of the desirable properties when used on decision making tasks that people have little domain expertise in. On decision making tasks that people are more knowledgeable, the feature contribution explanation is shown to satisfy more desiderata of AI explanations, even when the AI model is inherently complex. We conclude by discussing the implications of our study for improving the design of XAI methods to better support human decision making, and for advancing more rigorous empirical evaluation of XAI methods.\n    [8] Title: \"Help Me Help the AI\": Understanding How Explainability Can Support Human-AI Interaction. Abstract: Despite the proliferation of explainable AI (XAI) methods, little is understood about end-users\u2019 explainability needs and behaviors around XAI explanations. To address this gap and contribute to understanding how explainability can support human-AI interaction, we conducted a mixed-methods study with 20 end-users of a real-world AI application, the Merlin bird identification app, and inquired about their XAI needs, uses, and perceptions. We found that participants desire practically useful information that can improve their collaboration with the AI, more so than technical system details. Relatedly, participants intended to use XAI explanations for various purposes beyond understanding the AI\u2019s outputs: calibrating trust, improving their task skills, changing their behavior to supply better inputs to the AI, and giving constructive feedback to developers. Finally, among existing XAI approaches, participants preferred part-based explanations that resemble human reasoning and explanations. We discuss the implications of our findings and provide recommendations for future XAI design.\n    [9] Title: Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making. Abstract: Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.\n\n\n    Idea: Implement **<b style=\"background-color:#FFFACD;\">recursively expandable abstracts</b>** in a **<b style=\"background-color:#F0FFF0;\">system designed to reduce UAV loss rates</b>** by providing operators with dynamically expandable mission briefings and real-time updates. This system would allow operators to access critical information progressively, reducing cognitive overload and improving decision-making during missions. A **<b style=\"background-color:#E6E6FA;\">notional design approach</b>** could be used to outline the system's architecture and determine the most effective ways to integrate expandable abstracts into the UAV control interface. This could lead to more informed and timely actions, ultimately reducing UAV loss rates.\n    Class: novel\n    Review: The idea is novel because it uniquely combines recursively expandable abstracts with UAV mission systems, a combination not explored in related works like Qlarify[0] and PaperPoles[2].\n    Related Papers:\n    [0] Title: Qlarify: Recursively Expandable Abstracts for Directed Information Retrieval over Scientific Papers. Abstract: Navigating the vast scientific literature often starts with browsing a paper's abstract. However, when a reader seeks additional information, not present in the abstract, they face a costly cognitive chasm during their dive into the full text. To bridge this gap, we introduce recursively expandable abstracts, a novel interaction paradigm that dynamically expands abstracts by progressively incorporating additional information from the papers' full text. This lightweight interaction allows scholars to specify their information needs by quickly brushing over the abstract or selecting AI-suggested expandable entities. Relevant information is synthesized using a retrieval-augmented generation approach, presented as a fluid, threaded expansion of the abstract, and made efficiently verifiable via attribution to relevant source-passages in the paper. Through a series of user studies, we demonstrate the utility of recursively expandable abstracts and identify future opportunities to support low-effort and just-in-time exploration of long-form information contexts through LLM-powered interactions.\n    [1] Title: P R. Abstract: E n l a a n t ig \u00fc e d a d los h o m b re s d e m a r a d o p t a b a n m e d id a s u n i l a t e r a l e s de s e g u r id a d t e n d i e n t e s a p ro t e g e r sus p ro p io s buq u e s . D ic h a s m e d id a s l l e g a ro n a c o n v e r t irs e e n d o c tr in a g e n e ra l qu e lo s c a p i t a n e s y p a tro n e s e m p l e a b a n p a ra s a lv a g u a rd i a r sus v id a s y la s de sus tr ip u l a c io n e s . P e ro com o n o f u e ra s u f ic i e n t e l a a p l ic a c i\u00f3 n d e e s to , a buqu e s c o n s tru id o s , f u e n e c e s a r io e x t e n d e r e l c o n tro l a l a f o rm a d e cons tru c c i\u00f3 n y a l e m p l e o d e m a t e r i a l e s qu e en caso d e n a u fr a g io p u d i e r a n u t i l i z ars e com o e l e m e n to s d e s a lv a m e n to . D e b id o a l a s p \u00e9 rd id a s d e c a rg a p o r h u n d im i e n t o d e b u q u e s d e tr a n s p o r t e a l L e j a n o O r i e n t e , e n lo s co m i e n z o s de l a e ra c o m e rc i a l y d e spu \u00e9 s a l n u e vo M u n d o d u r a n t e l a c o lo n i a , l l e g \u00f3 a e s ta b le c ers e e l s e g uro m a r \u00ed t i m o q u e fu e re g l a m e n t a d o p o r los g o b i e rn o s de In g l a t e rr a y E sp a \u00f1 a , c o n o c id a s e n ese e n tonc e s co m o l a s p o t e n c i a s m a r \u00ed t im a s que s urc a b a n l a s ru t a s d e l m u n d o . E n E sp a \u00f1 a se e n c a rg \u00f3 a lo s cnnsv iT t e . d e F r a g a t a J O S E G U IL L E R M O P A E Z\n    [2] Title: PaperPoles: Facilitating adaptive visual exploration of scientific publications by citation links. Abstract: Finding relevant publications is a common task. Typically, a researcher browses through a list of publications and traces additional relevant publications. When relevant publications are identified, the list may be expanded by the citation links of the relevant publications. The information needs of researchers may change as they go through such iterative processes. The exploration process quickly becomes cumbersome as the list expands. Most existing academic search systems tend to be limited in terms of the extent to which searchers can adapt their search as they proceed. In this article, we introduce an adaptive visual exploration system named PaperPoles to support exploration of scientific publications in a context\u2010aware environment. Searchers can express their information needs by intuitively formulating positive and negative queries. The search results are grouped and displayed in a cluster view, which shows aspects and relevance patterns of the results to support navigation and exploration. We conducted an experiment to compare PaperPoles with a list\u2010based interface in performing two academic search tasks with different complexity. The results show that PaperPoles can improve the accuracy of searching for the simple and complex tasks. It can also reduce the completion time of searching and improve exploration effectiveness in the complex task. PaperPoles demonstrates a potentially effective workflow for adaptive visual search of complex information.\n    [3] Title: FORMADOCT: Developing an information search strategy: Introduction. Abstract: Simple but efficient actions for a documentalist to assess the latest developments on a topic\n    [4] Title: As We May Interact: Challenges and Opportunities for Next-Generation Human-Information Interaction. Abstract: Long before the advent of personal computing, Vannevar Bush envisioned the Memex as a solution to address information over-load by enhancing the management and refinding of information through associative trails. While other hypertext pioneers like Douglas Engelbart and Ted Nelson introduced advanced hypertext concepts to create more flexible document structures and augment the human intellect, some of their original ideas are still absent in our daily interaction with documents and information systems. Today, many digital document formats mimic paper documents without fully leveraging the opportunities offered by digital media and documents are often organised in hierarchical file structures. In this keynote, we explore how cross-media technologies, such as the resource-selector-link (RSL) hypermedia metamodel, can be used to organise and interact with information across digital and physical spaces. While emerging wearable mixed reality (MR) headsets offer new possibilities to augment the human intellect, we discuss how hypermedia research, in combination with other technologies, could play a major role in providing the necessary linked data and hypertext infrastructure for this augmentation process. We outline the challenges and opportunities for next-generation multimodal human-information interaction enabled by flexible cross-media information spaces and document structures in combination with upcoming mixed and virtual reality solutions.\n    [5] Title: Proof-of-Concept Study of Image-Generating AIs As a Tool for Graphical Abstracts. Abstract: Graphical abstracts are a visual representation of the abstract, to gain visibility and succinctly display the study findings. To design graphical abstract for scientific articles through image-generating AIs, we carried out a proof-of-concept study using abstracts as prompts. With prompt engineering, partial abstract parts and full abstract yielded similar images. We observe that AI's still do not reach the ability to generate a coherent image with abstract discourse, but lead to a big creative jump to build one. This bias resulted from keywords AIs attention and a non-homogeneous guideline to picture a graphical abstract over merely workflow illustrations. Relevance\u2014 To write prompts to generate graphical abstracts allowing us to use image-generating AIs as we currently work with other AIs as translators, search engines or even chats. Also to challenge current graphical abstract beyond a workflow diagram towards an abstract but illustrative picture of the article content.\n    [6] Title: Paper Forager: Supporting the Rapid Exploration of Research Document Collections. Abstract: We present Paper Forager, a web-based system which allows users to rapidly explore large collections of research documents. Our sample corpus uses 5,055 papers published at the ACM CHI and UIST conferences. Paper Forager provides a visually based browsing experience, allowing users to identify papers of interest based on their graphical appearance, in addition to providing traditional faceted search techniques. A cloud-based architecture stores the papers as multi-resolution images, giving users immediate access to reading individual pages of a paper, thus reducing the transaction cost between finding, scanning, and reading papers of interest. Initial user feedback sessions elicited positive subjective feedback, while a 24-month external deployment generated in-the-wild usage data which we analyze. Users of the system indicated that they would be enthusiastic to continue having access to the Paper Forager system in the future.\n    [7] Title: Concurrent Brainstorming & Hypothesis Satisfying: An Iterative Framework for Enhanced Retrieval-Augmented Generation (R2CBR3H-SR). Abstract: Addressing the complexity of comprehensive information retrieval, this study introduces an innovative, iterative retrieval-augmented generation system. Our approach uniquely integrates a vector-space driven re-ranking mechanism with concurrent brainstorming to expedite the retrieval of highly relevant documents, thereby streamlining the generation of potential queries. This sets the stage for our novel hybrid process, which synergistically combines hypothesis formulation with satisfying decision-making strategy to determine content adequacy, leveraging a chain of thought-based prompting technique. This unified hypothesize-satisfied phase intelligently distills information to ascertain whether user queries have been satisfactorily addressed. Upon reaching this criterion, the system refines its output into a concise representation, maximizing conceptual density with minimal verbosity. The iterative nature of the workflow enhances process efficiency and accuracy. Crucially, the concurrency within the brainstorming phase significantly accelerates recursive operations, facilitating rapid convergence to solution satisfaction. Compared to conventional methods, our system demonstrates a marked improvement in computational time and cost-effectiveness. This research advances the state-of-the-art in intelligent retrieval systems, setting a new benchmark for resource-efficient information extraction and abstraction in knowledge-intensive applications.\n    [8] Title: Towards a RAG-based summarization for the Electron Ion Collider. Abstract: \n The complexity and sheer volume of information \u2014 encompassing documents, papers, data, and other resources \u2014 from large-scale experiments demand significant time and effort to navigate, making the task of accessing and utilizing these varied forms of information daunting, particularly for new collaborators and early-career scientists.\nTo tackle this issue, a Retrieval Augmented Generation (RAG)-based Summarization AI for EIC (RAGS4EIC) is under development. This AI-Agent not only condenses information but also effectively references relevant responses, offering substantial advantages for collaborators. Our project involves a two-step approach: first, querying a comprehensive vector database containing all pertinent experiment information; second, utilizing a Large Language Model (LLM) to generate concise summaries enriched with citations based on user queries and retrieved data. We describe the evaluation methods that use RAG assessments (RAGAs) scoring mechanisms to assess the effectiveness of responses. Furthermore, we describe the concept of prompt template based instruction-tuning which provides flexibility and accuracy in summarization. Importantly, the implementation relies on LangChain\u00a0[1], which serves as the foundation of our entire workflow. This integration ensures efficiency and scalability, facilitating smooth deployment and accessibility for various user groups within the Electron Ion Collider (EIC) community. This innovative AI-driven framework not only simplifies the understanding of vast datasets but also encourages collaborative participation, thereby empowering researchers. As a demonstration, a web application has been developed to explain each stage of the RAG Agent development in detail. The application can be accessed at https://rags4eic-ai4eic.streamlit.app.[A tagged version of the source code can be found in https://github.com/ai4eic/EIC-RAG-Project/releases/tag/AI4EIC2023_PROCEEDING.]\n    [9] Title: WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation Integrating Web Search and Knowledge Graphs. Abstract: Large Language Models (LLMs) have greatly contributed to the development of adaptive intelligent agents and are positioned as an important way to achieve Artificial General Intelligence (AGI). However, LLMs are prone to produce factually incorrect information and often produce\"phantom\"content that undermines their reliability, which poses a serious challenge for their deployment in real-world scenarios. Enhancing LLMs by combining external databases and information retrieval mechanisms is an effective path. To address the above challenges, we propose a new approach called WeKnow-RAG, which integrates Web search and Knowledge Graphs into a\"Retrieval-Augmented Generation (RAG)\"system. First, the accuracy and reliability of LLM responses are improved by combining the structured representation of Knowledge Graphs with the flexibility of dense vector retrieval. WeKnow-RAG then utilizes domain-specific knowledge graphs to satisfy a variety of queries and domains, thereby improving performance on factual information and complex reasoning tasks by employing multi-stage web page retrieval techniques using both sparse and dense retrieval methods. Our approach effectively balances the efficiency and accuracy of information retrieval, thus improving the overall retrieval process. Finally, we also integrate a self-assessment mechanism for the LLM to evaluate the trustworthiness of the answers it generates. Our approach proves its outstanding effectiveness in a wide range of offline experiments and online submissions.\n\n\n    Idea: Conduct a systematic literature review utilizing the PRISMA protocol to analyze the impact of integrating Computational Thinking (CT) into science education with a focus on gamification elements. This study could identify how CT principles can be gamified to enhance student engagement, motivation, and learning outcomes in science subjects. The evaluation would involve analyzing literature to categorize the benefits and challenges of this integration across different educational levels and contexts.\n    Class: novel\n    Review: The idea is novel because, while gamification in education has been extensively reviewed (e.g., Gamification Techniques[0], Gamification in Education[1]), the specific focus on integrating Computational Thinking (CT) into science education with gamification elements is not explored in the related papers.\n    Related Papers:\n    [0] Title: The Role of Gamification Techniques in Promoting Student Learning: A Review and Synthesis. Abstract: Aim/Purpose: This study reviewed previous research on the role of gamification techniques in promoting students\u2019 learning.\n\nBackground: The role of gamification in promoting students\u2019 learning has been investigated empirically by many scholars. To date, mixed results about the effectiveness of gamification have been reported, and researchers frequently argue that the inappropriateness of certain techniques may have contributed to these mixed findings.\n\nMethodology: The Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) protocol was used to assess the criteria required for this review. A total of 40 studies were identified and included in the systematic review. The selected studies were used to assess the association between certain gamification techniques and students\u2019 learning in this study.\n\nFindings: The results showed that gamification techniques differently affect students\u2019 learning. In addition, it is important for students to be instructed about the application of gamification approach before they engage in a gamified learning task. The key challenges relating to the use of gamification techniques were also discussed.\n\nRecommendations for Practitioners: This review can help educational decision makers and practitioners to stimulate certain learning outcomes of the students with the help of specific gamification techniques. \n\n\n    [1] Title: Using Gamification in Education: A Systematic Literature Review. Abstract: Gamification systems have the potential to foster students\u2019 engagement and enhance their learning performance. Although the literature to date has provided relevant contributions in explaining how gamification design can be effective, several studies seek to address the constructs used to measure the effects of gamification. To analyze how gamification outcomes should be properly measured, this paper systematically reviews the literature and provides a map of the most frequent scales used to measure gamification outcomes in the educational context. As research findings, we identify motivation, engagement, self-efficacy, and flow/cognitive absorption as the primary constructs addressed to experiential outcomes. Additionally, there are research opportunities to develop a better understanding of the effects of extrinsic motivation rewards on experiential outcomes and problem-solving transfer posited as instrumental outcome.\n    [2] Title: Gamification Effects on Motivation and Learning: Application to Primary and College Students. Abstract: This manuscript reports the effects of gamification elements on primary and college students\u2019 motivation and learning. This mixed methods research reports two years of data collection of primary and college students\u2019 reflections, preand post-test results, and survey results as well as observations made by the author. Business communication classes and mathematics classes experienced gamification by introducing game elements and frameworks in the classrooms while maintaining the integrity of the learning outcomes. Students expressed increased motivation and engagement at both the primary and college level as well as improved learning.\n    [3] Title: Gamification and learning performance : a systematic review of the literature. Abstract: Over the last 10 years, research on gamification, the use of game elements in non-game contexts, has increased in the field of education, due to its potential to enhance learning performance. Yet, the majority of available research rather focuses on the evaluation of motivation and engagement as key dependent variables. Hence, the purpose of this study is to review available studies on gamification, with an exclusive focus on learning performance as the key dependent variable. Through a systematic search and selection process, building on \"Web of Science\" articles and by considering studies between 2000 and 2016 related to gamification and learning, 582 articles were identified. Further inclusion and exclusion criteria, regarding setting (education), study focus (empirical), journal access (full access) and dependent variables (learning performance), resulted in a review of 23 articles meeting the criteria. The analysis of these articles showed how gamification could be linked to a direct increase in learning performance of students. Nevertheless, some studies also reflect weaker statistical differences between being involved or not in a gamified environment. The review analysis results are especially helpful to define a future agenda for gamification research, addressing the following gaps in the literature. First, include mediating and moderating variables to find more empirical research that can prove an indirect effect of gamification on learning performance. Second, carry out additional research that empirically underpins the direct linkage between gamification and learning performance. Third, include specific individual gamification elements to be able to determine explicit differential effects of these elements on learning performance. Fourth, conduct research in a broader range of knowledge fields to develop empirical evidence in the context of other knowledge domains next to computer sciences. Finally, consider involving larger sample and setting up longer experimental interventions, to avoid novelty effects and risks of lack of generalization.\n    [4] Title: The effects of gamification on students\u2019 academic achievement: a meta-analysis study. Abstract: ABSTRACT Whether gamification is an organized structure that contributes to student achievement, a simple pontification process or total nonsense is a matter of debate. In such, this study was conducted to provide a scientific answer while exhibiting the gamification effect on student achievement with the meta-analysis method, which is based on experimental research that investigates the effect of gamification on student achievement between 2010 and 2016. As a result of the completed meta-analysis, a common effect size value that was estimated by compiling experimental studies was found to be 0.557. This value shows that gamification has a moderately positive (Cohen, 1988) effect on student achievement. It is exhibited that there is no publication bias regarding determining the effect\u2019s validity. Calculated 0.557 Hedges\u2019 g value\u2019s omega square equivalent is .072. This value shows that gamification has added 7.2% positive value to academic achievement according to 45 experimental results involving 3,487 students from different countries.\n    [5] Title: A systematic mapping on gamification applied to education. Abstract: Gamification is a term that refers to the use of game elements in non-game contexts with the goal of engaging people in a variety of tasks. There is a growing interest in gamification as well as its applications and implications in the field of Education since it provides an alternative to engage and motivate students during the process of learning. Despite this increasing interest, to the best of our knowledge, there are no studies that cover and classify the types of research being published and the most investigated topics in the area. As a first step towards bridging this gap, we carried out a systematic mapping to synthesize an overview of the area. We went through 357 papers on gamification. Among them, 48 were related to education and only 26 met the criteria for inclusion and exclusion of articles defined in this study. These 26 papers were selected and categorized according to their contribution. As a result, we provide an overview of the area. Such an overview suggests that most studies focus on investigating how gamification can be used to motivate students, improve their skills, and maximize learning.\n    [6] Title: Assessing the effects of gami fi cation in the classroom : A longitudinal study on intrinsic motivation , social comparison , satisfaction , effort , and academic performance. Abstract: Gamification, the application of game elements to non-game settings, continues to grow in popularity as a method to increase student engagement in the classroom. We tested students across two courses, measuring their motivation, social comparison, effort, satisfaction, learner empowerment, and academic performance at four points during a 16-week semester. One course received a gamified curriculum, featuring a leaderboard and badges, whereas the other course received the same curriculum without the gamified elements. Our results found that students in the gamified course showed less motivation, satisfaction, and empowerment over time than those in the non-gamified class. The effect of course type on students' final exam scores was mediated by students' levels of intrinsic motivation, with students in the gamified course showing less motivation and lower final exam scores than the non-gamified class. This suggests that some care should be taken when applying certain gamification mechanics to educational settings. \u00a9 2014 Elsevier Ltd. All rights reserved.\n    [7] Title: A systematic literature review of game-based learning and gamification research in Asia. Abstract: subject areas of science and social studies. The effects of games on learning are measured in both cognitive and affective domains through quantitative measures, often involving a control group. Overall, the efficacy of games and gamification is positive .\n    [8] Title: Gamification and Implications for Second Language Education: A Meta Analysis. Abstract: Gamification is a fairly new concept that involves using game elements in non-game contexts. It has been shown that gamification can increase motivation and learning, but there have been conflicting results, with some studies reporting opposite findings. Because of these motivational results and benefits that have been reported, many researchers have attempted to use gamification in educational settings. Again, these studies have shown mixed results. However, as a large number of studies have shown benefits from using gamification in educational settings, it is important to know exactly what aspects of gamification are beneficial so that it can be properly used in second language education. The present study is a meta analysis of gamification of education research that set forth to determine what aspects of gamification are important in educational settings, and how this information can be used to successfully use gamification in second language education. Overall, it was found that gamification typically had a positive effect. Additionally, several moderator variables were of importance, including the length of instruction, inclusion of competitive aspects, and usage of time on task elements.\n    [9] Title: The effect of gamification on motivation and engagement. Abstract: Gamification is the application of game features, mainly video game elements, into non-game context for the purpose of promoting motivation and engagement in learning. The application of gamification in a pedagogical context provides some remedy for many students who find themselves alienated by traditional methods of instruction. The use of gamification could provide a partial solution to the decline in learners\u2019 motivation and engagement the schooling system is facing today. Specifically, the college environment could benefit a lot from gamifying not only their graduate recruitment strategies, but also the college course content and curricula. This critical analysis of literature on gamification is intended to be part of a sequence on the effect of gamification on motivation and engagement. A proposed methodology in the study of gamification effect on motivation and engagement in addition to an empirical study on three college courses are being finalized to complete this trilogy. The paper aims to discuss these issues.,Themes covered in the literature review include: conceptualizing gamification, advantages of gamification over game-based learning, theoretical connections to gamification, motivation and engagement, connecting gamification to motivation and engagement, emotions and fun in gamification, player types and gamification features, gamification in action, and implementation guidelines.,The literature on the effect of gamification on motivation and gamification is still limited on multiple levels. There is a gap between theory and practice in the study of gamification. There is limited literature on the implementation guidelines of the gamified designs.,This critical analysis of literature is followed by connecting it to future research by the same author as part of a sequence on the effect of gamification on motivation and engagement. The second project, will be proposing a methodology for any successful design to provide a holistic understanding of the topic of gamification. Finally, an empirical study on the effect of gamification on students\u2019 motivation and engagement in three college courses will be submitted to complete the trilogy.,This paper is a literature review, so there is a strong connection to literature on this topic. However, the synthesis of the themes and ideas are original. The literature review is extensive and covers the different aspects of the topic of gamification and its relationship to motivation and engagement.\n\n\n    Idea: Develop a system that uses **faceted author representation** to **enhance disaster response research** by identifying and connecting researchers with complementary expertise in real-time during disaster events. The system would create dynamic profiles of researchers based on their publication history, real-time data contributions, and inferred personas. These profiles would be used to form interdisciplinary teams rapidly. **Examination of teaming mechanisms** would be conducted to evaluate the effectiveness of these dynamically formed teams in improving response times and decision-making in disaster scenarios.\n    Class: novel\n    Review: The idea is novel because it introduces faceted author representation with a focus on forming teams for disaster response with complementary collaborators, which is not covered in existing works like Bridger[0][1] and ComLittee[2].\t\n    Related Papers:\n    [0] Title: Bursting Scientific Filter Bubbles: Boosting Innovation via Novel Author Discovery. Abstract: Isolated silos of scientific research and the growing challenge of information overload limit awareness across the literature and hinder innovation. Algorithmic curation and recommendation, which often prioritize relevance, can further reinforce these informational \u201cfilter bubbles.\u201d In response, we describe Bridger, a system for facilitating discovery of scholars and their work. We construct a faceted representation of authors with information gleaned from their papers and inferred author personas, and use it to develop an approach that locates commonalities and contrasts between scientists to balance relevance and novelty. In studies with computer science researchers, this approach helps users discover authors considered useful for generating novel research directions. We also demonstrate an approach for displaying information about authors, boosting the ability to understand the work of new, unfamiliar scholars. Our analysis reveals that Bridger connects authors who have different citation profiles and publish in different venues, raising the prospect of bridging diverse scientific communities.\n    [1] Title: Bridger: Toward Bursting Scientific Filter Bubbles and Boosting Innovation via Novel Author Discovery. Abstract: Scientific silos can hinder innovation. These information \u201cfilter bubbles\u201d and the growing challenge of information overload limit awareness across the literature, making it difficult to keep track of even narrow areas of interest, let alone discover new ones. Al-gorithmic curation and recommendation, which often prioritize relevance, can further reinforce these bubbles. In response, we describe Bridger, a system for facilitating discovery of scholars and their work, to explore design tradeoffs among relevant and novel recommendations. We construct a faceted representation of authors using information extracted from their papers and inferred personas. We explore approaches both for recommending new content and for displaying it in a manner that helps researchers to understand the work of authors who they are unfamiliar with. In studies with computer science researchers, our approach substantially improves users\u2019 abilities to do so. We develop an approach that locates commonalities and contrasts between scientists\u2014retrieving partially similar authors, rather than aiming for strict similarity. We find this approach helps users discover authors useful for generating novel research ideas of relevance to their work, at a higher rate than a state-of-art neural model. Our analysis reveals that Bridger connects authors who have different citation profiles, publish in different venues, and are more distant in social co-authorship networks, raising the prospect of bridging diverse communities and facilitating discovery.\n    [2] Title: ComLittee: Literature Discovery with Personal Elected Author Committees. Abstract: In order to help scholars understand and follow a research topic, significant research has been devoted to creating systems that help scholars discover relevant papers and authors. Recent approaches have shown the usefulness of highlighting relevant authors while scholars engage in paper discovery. However, these systems do not capture and utilize users\u2019 evolving knowledge of authors. We reflect on the design space and introduce ComLittee, a literature discovery system that supports author-centric exploration. In contrast to paper-centric interaction in prior systems, ComLittee\u2019s author-centric interaction supports curating research threads from individual authors, finding new authors and papers using combined signals from a paper recommender and the curated authors\u2019 authorship graphs, and understanding them in the context of those signals. In a within-subjects experiment that compares to a paper-centric discovery system with author-highlighting, we demonstrate how ComLittee improves author and paper discovery.\n    [3] Title: AMiner: Mining Deep Knowledge from Big Scholar Data. Abstract: AMiner is the second generation of the ArnetMiner system. We focus on developing author-centric analytic and mining tools for gaining a deep understanding of the large and heterogeneous networks formed by authors, papers, venues, and knowledge concepts. One fundamental goal is how to extract and integrate semantics from different sources. We have developed algorithms to automatically extract researchers' profiles from the Web and re- solve the name ambiguity problem, and connect different professional networks. We also developed methodologies to incorporate knowledge from the Wikipedia and other sources into the system to bridge the gap between network science and the web mining research. In this talk, I will focus on answering two fundamental questions for author-centric network analysis: who is who? and who are similar to each other? The system has been in operation since 2006 and has collected more than 100,000,000 author profiles, 100,000,000 publication papers, and 7,800,000 knowledge concepts. It has been widely used for collaboration recommendation, similarity analysis, and community evolution.\n    [4] Title: Online The Open University \u2019 s repository of research publications and other research outputs Understanding research dynamics. Abstract: Rexplore leverages novel solutions in data mining, semantic technologies and visual analytics, and provides an innovative environment for exploring and making sense of scholarly data. Rexplore allows users: 1) to detect and make sense of important trends in research; 2) to identify a variety of interesting relations between researchers, beyond the standard co-authorship relations provided by most other systems; 3) to perform fine-grained expert search with respect to detailed multi-dimensional parameters; 4) to detect and characterize the dynamics of interesting communities of researchers, identified on the basis of shared research interests and scientific trajectories; 5) to analyse research performance at different levels of abstraction, including individual researchers, organizations, countries, and research communities.\n    [5] Title: Recommending collaborators using social features and MeSH terms. Abstract: Unlike expertise location systems which users query actively when looking for an expert, expert recommender systems suggest individuals without the context of a specific problem. An interesting research question is whether expert recommender systems should consider a users' social context when recommending potential research collaborators. One may argue that it might be easier for scientists to collaborate with colleagues in their social network, because initiating collaboration with socially unconnected researchers is burdensome and fraught with risk, despite potentially relevant expertise. However, many scientists also initiate collaborations outside of their social network when they seek to work with individuals possessing relevant expertise or acknowledged experts. In this paper, we studied how well content-based, social and hybrid recommendation algorithms predicted co-author relationships among a random sample of 17,525 biomedical scientists. To generate recommendations, we used authors' research expertise inferred from publication metadata and their professional social networks derived from their co-authorship history. We used 80% of our data set (articles published before 2007) as our training set, and the remaining data as our test set (articles published in 2007 or later). Our results show that a hybrid algorithm combining expertise and social network information outperformed all other algorithms with regards to Top 10 and Top 20 recommendations. For the Top 2 and Top 5 recommendations, social network-based information alone generated the most useful recommendations. Our study provides evidence that integrating social network information in expert recommendations may outperform a purely expertise-based approach.\n    [6] Title: Similarity-based Complex Publication Network Analytics for Recommending Potential Collaborations. Abstract: As communities of researchers continue to become quite large and to grow incessantly, collaboration among researchers can be conducive to greater research pro- ductivity. Nevertheless, it is difficult for a researcher to find suitable collaborators from all researchers around the world. In this paper, we have used bibliographic DBLP data to extract information of a researcher and to discover the relationship between the co-authors and between authors and conferences. We evaluated some of the similarity measures and developed an innovative random walk model to find potential co-authors for a given researcher. These measures were then used to design a best model to rec- ommend co-authors. We have also applied an HITS algorithm and proposed a ranking algorithm to rank researchers and conferences with the intent of recommending authors or conferences.\n    [7] Title: VeTo-web: A Recommendation Tool for the Expansion of Sets of Scholars. Abstract: Expanding a set of known experts with new ones that share similar expertise is a problem that emerges in various real-life applications. We demonstrate VeTo-web, an open source, publicly available tool that deals with this problem in the context of searching for academic experts. VeTo-web exploits analysis techniques for scholarly knowledge graphs to identify scholars that share similar research activities with a given expert group and offers a Web-based user interface to assist its users in expanding a set of academic experts with additional scholars with similar expertise.\n    [8] Title: Discovering Patterns of Collaboration for Recommendation. Abstract: Collaboration between research scientists, particularly those with diverse backgrounds, is a driver of scientific innovation. However, finding the right collaborator is often an unscientific process that is subject to chance. This paper explores recommending collaborators based on repeating patterns of previous successful collaboration experiences, what we term prototypical collaborations. We investigate a method for discovering such prototypes to use them as a basis to guide the recommendation of new collaborations. To this end, we also examine two methods for matching collaboration seekers to these prototypical collaborations. Our initial studies reveal that though promising, improving collaborations through recommendation is a complex goal.\n    [9] Title: Exploiting Topic Modelling for the Identification of Untapped Scientific Collaborations. Abstract: Finding potential collaborators has become a challenge due to the growing number of scientists in organizations such as universities, research institutes, or companies. Collaboration Recommendation Systems (CRSs) have been developed to help researchers identify possible collaboration partners, but they often rely on citation graphs or paper abstracts which may not be readily available in organizational databases or online sources. However, scientific publication titles provide consistent bibliometric data that can provide insights into research areas. TOMOSCO is a topic modelling framework that uses transformer-based methods to extract research area information from small amounts of text, such as publication titles or brief project descriptions. TOMOSCO can classify, cluster, and match research topics across different disciplines, uncovering relationships among scientists and suggesting potential interdisciplinary collaborations. In experiments, TOMOSCO was able to identify existing collaborations with over 90% accuracy based solely on publication titles and propose new collaborations based on previously unseen publications and project descriptions.\n\n\n    Idea: Develop a system that **contextualizes text descriptions** of recommended papers based on **local network similarity measurements** of their citation networks **to enhance link prediction in citation networks**. This system would not only present the papers but also explain their relevance through the lens of citation patterns, adding a layer of interpretability. The effectiveness of this system can be assessed through a **user study** where researchers evaluate the relevance and usefulness of recommendations in their specific fields, comparing it against a baseline that does not incorporate citation network insights.\n    Class: novel\n    Review: The idea is novel because it focuses on utilizing citations for contextualized text descriptions for paper recommendation, unlike related papers shown.\n    Related Papers:\n    [0] Title: The Role of Document Embedding in Research Paper Recommender Systems: To Breakdown or to Bolster Disciplinary Borders?. Abstract: In the extensive recommender systems literature, novelty and diversity have been identified as key properties of useful recommendations. However, these properties have received limited attention in the specific sub-field of research paper recommender systems. In this work, we argue for the importance of offering novel and diverse research paper recommendations to scientists. This approach aims to reduce siloed reading, break down filter bubbles, and promote interdisciplinary research. We propose a novel framework for evaluating the novelty and diversity of research paper recommendations that leverages methods from network analysis and natural language processing. Using this framework, we show that the choice of representational method within a larger research paper recommendation system can have a measurable impact on the nature of downstream recommendations, specifically on their novelty and diversity. We introduce a novel paper embedding method, which we demonstrate offers more innovative and diverse recommendations without sacrificing precision, compared to other state-of-the-art baselines.\n    [1] Title: On Measuring the Contextual Relevance of Research Paper Recommendation Systems. Abstract: The contextual information present in scholarly \npapers plays a vital role in the implementation of \nresearch paper recommendation systems. However, \nthe most critical concern is how to measure the \ncontextual relevance of scholarly papers for better \nrecommendations? In this paper, we present the \nmost common approaches used to measure the \ncontextual relevance of research paper \nrecommendation systems. Based on the research \noutcome, content-link, citation relation, social \nnetwork analyses, and their combinations are the \nmost widely used. The paper also outlined the \nstrengths and weaknesses of each approach.\n    [2] Title: A Local Community and Resource Allocation Perspective on Link Prediction: Method and Visualization. Abstract: Link prediction is a challenging and complex research area in network studies, which plays an important role not only in recommendation systems in fields such as social media and e-commerce, but also in network security monitoring and biological relationship research. Over the past two decades of link prediction development, many branches in the field of computer science have used their own methods to solve link prediction problems, but analysis methods based on local similarity have stood out due to their lower time complexity, higher prediction accuracy, and interpretability. In this work, a link prediction method based on local community and resource allocation theory is proposed and extended within 3-hop paths, and is also extensively experimented. Compared to most methods based on local similarity, this method has a higher prediction accuracy. In addition, we propose an interactive visualization system, where users can select different indices and datasets to get link prediction recommendations and explore the analysis process.\n    [3] Title: Academic Article Recommendation Using Multiple Perspectives. Abstract: We argue that Content-based filtering (CBF) and Graph-based methods (GB) complement one another in Academic Search recommendations. The scientific literature can be viewed as a conversation between authors and the audience. CBF uses abstracts to infer authors' positions, and GB uses citations to infer responses from the audience. In this paper, we describe nine differences between CBF and GB, as well as synergistic opportunities for hybrid combinations. Two embeddings will be used to illustrate these opportunities: (1) Specter, a CBF method based on BERT-like deepnet encodings of abstracts, and (2) ProNE, a GB method based on spectral clustering of more than 200M papers and 2B citations from Semantic Scholar.\n    [4] Title: Learning-to-Rank in Research Paper CBF recommendation: Leveraging Irrelevant Papers. Abstract: Suggesting relevant literature to researchers has become an active area of study, typically relying on content-based \ufb01l-tering (CBF) over the rich textual features available. Given the high dimensionality and the sparsity of the training samples inherent to this domain, the focus has so far been on heuristic-based methods. In this paper, we argue for the model-based approach and propose a learning-to-rank method that leverages publicly available publications\u2019 meta-data to produce an e\ufb00ective prediction model. The proposed method is systematically evaluated on a scholarly paper recommendation dataset and compared against state-of-the-art model-based approaches as well as current, domain-speci\ufb01c heuristic methods. The results show that our approach clearly outperforms state-of-the-art research paper recommendations utilizing only publicly available meta-data.\n    [5] Title: Leveraging metadata to recommend keywords for academic papers. Abstract: Users of research databases, such as CiteSeerX, Google Scholar, and Microsoft Academic, often search for papers using a set of keywords. Unfortunately, many authors avoid listing sufficient keywords for their papers. As such, these applications may need to automatically associate good descriptive keywords with papers. When the full text of the paper is available this problem has been thoroughly studied. In many cases, however, due to copyright limitations, research databases do not have access to the full text. On the other hand, such databases typically maintain metadata, such as the title and abstract and the citation network of each paper. In this paper we study the problem of predicting which keywords are appropriate for a research paper, using different methods based on the citation network and available metadata. Our main goal is in providing search engines with the ability to extract keywords from the available metadata. However, our system can also be used for other applications, such as for recommending keywords for the authors of new papers. We create a data set of research papers, and their citation network, keywords, and other metadata, containing over 470K papers with and more than 2 million keywords. We compare our methods with predicting keywords using the title and abstract, in offline experiments and in a user study, concluding that the citation network provides much better predictions.\n    [6] Title: Combining collaborative and content-based filtering to recommend research papers. Abstract: The number of research papers available today is growing at a staggering rate, generating a huge amount of information that people cannot keep up with. According to a tendency indicated by the United States\u2019 National Science Foundation, more than 10 million new papers will be published in the next 20 years. Because most of these papers will be available on the Web, this research focus on exploring issues on recommending research papers to users, in order to directly lead users to papers of their interest. Recommender systems are used to recommend items to users among a huge stream of available items, according to users\u2019 interests. This research focuses on the two most prevalent techniques to date, namely Content-Based Filtering and Collaborative Filtering. The first explores the text of the paper itself, recommending items similar in content to the ones the user has rated in the past. The second explores the citation web existing among papers. As these two techniques have complementary advantages, we explored hybrid approaches to recommending research papers. We created standalone and hybrid versions of algorithms and evaluated them through both offline experiments on a database of 102,295 papers, and an online experiment with 110 users. Our results show that the two techniques can be successfully combined to recommend papers. The coverage is also increased at the level of 100% in the hybrid algorithms. In addition, we found that different algorithms are more suitable for recommending different kinds of papers. Finally, we verified that users\u2019 research experience influences the way users perceive recommendations. In parallel, we found that there are no significant differences in recommending papers for users from different countries. However, our results showed that users\u2019 interacting with a research paper Recommender Systems are much happier when the interface is presented in the user\u2019s native language, regardless the language that the papers are written. Therefore, an interface should be tailored to the user\u2019s mother language.\n    [7] Title: A Scalable Hybrid Research Paper Recommender System for Microsoft Academic. Abstract: We present the design and methodology for the large scale hybrid paper recommender system used by Microsoft Academic. The system provides recommendations for approximately 160 million English research papers and patents. Our approach handles incomplete citation information while also alleviating the cold-start problem that often affects other recommender systems. We use the Microsoft Academic Graph (MAG), titles, and available abstracts of research papers to build a recommendation list for all documents, thereby combining co-citation and content based approaches. Tuning system parameters also allows for blending and prioritization of each approach which, in turn, allows us to balance paper novelty versus authority in recommendation results. We evaluate the generated recommendations via a user study of 40 participants, with over 2400 recommendation pairs graded and discuss the quality of the results using P@10 and nDCG scores. We see that there is a strong correlation between participant scores and the similarity rankings produced by our system but that additional focus needs to be put towards improving recommender precision, particularly for content based recommendations. The results of the user survey and associated analysis scripts are made available via GitHub and the recommendations produced by our system are available as part of the MAG on Azure to facilitate further research and light up novel research paper recommendation applications.\n    [8] Title: \"Towards higher relevance and serendipity in scholarly paper recommendation\" by Kazunari Sugiyama and Min-Yen Kan with Martin Vesely as coordinator. Abstract: Finding relevant scholarly papers is an important task for researchers. Such a literature search involves identifying drawbacks in existing works and proposing new approaches that address them. However, the growing number of scientific published papers results in information overload even for simple searches, such that researchers have difficulty in finding papers relevant to their interests. Recommendation systems can help address this problem to find relevant papers efficiently. In this article, we summarize our work on scholarly paper recommendation from both relevance and serendipitous perspectives. Experimental results on a publicly-available scholarly paper recommendation dataset show that our proposed approaches provides promising recommendations for researchers, outperforming the state-of-the-art with statistical significance.\n    [9] Title: Rec4LRW - Scientific Paper Recommender System for Literature Review and Writing. Abstract: In this paper, we introduce Rec4LRW, a recommender system (RS) for assisting researchers in finding research papers for their literature review and writing purposes. This system focuses on three researcher tasks \u2013 (1) Building a reading list of research papers, (2) Finding similar papers based on a set of papers, and (3) Shortlisting papers from the final reading list for inclusion in manuscript based on article type. A set of intermediate criteria are proposed to capture the relations between a research paper and its bibliography. The recommendation techniques for the three tasks in Rec4LRW are specifically devised on top of the intermediate criteria. The Rec4LRW workflow along with the screen designs for the three tasks is provided in this paper. The recommendation techniques in the system will be evaluated with state-of-the-art approaches along with user-based evaluation in subsequent studies.\n             "